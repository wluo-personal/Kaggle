{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "loaded train\n"
=======
      "loaded train\n",
      "loaded test\n"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     ]
    }
   ],
   "source": [
    "train = pd.read_feather('Feathers/ss_alltrain_typechanged.ftr')\n",
    "print('loaded train')\n",
<<<<<<< HEAD
    "train.set_index('index', inplace=True)\n",
    "del train.index.name"
=======
    "test = pd.read_feather('Feathers/ss_test_via_alltrain_typechanged.ftr')\n",
    "print('loaded test')"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
=======
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.set_index('index', inplace=True)\n",
    "del train.index.name\n",
    "test.set_index('index', inplace=True)\n",
    "del test.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>app_day_hour_count</th>\n",
       "      <th>app_device_os_mean</th>\n",
       "      <th>attributed_timediff</th>\n",
       "      <th>channel</th>\n",
       "      <th>device</th>\n",
       "      <th>hour</th>\n",
       "      <th>ip_app_day_hour_count</th>\n",
       "      <th>ip_app_device_countfromfuture</th>\n",
       "      <th>ip_app_device_countfrompast</th>\n",
       "      <th>...</th>\n",
       "      <th>ip_device_os_lasttimediff</th>\n",
       "      <th>ip_device_os_mean</th>\n",
       "      <th>ip_device_os_time2nextclick</th>\n",
       "      <th>ip_device_os_time2previousclick</th>\n",
       "      <th>ip_os_day_hour_count</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>matrixFact_user_ip_item_appdeviceos</th>\n",
       "      <th>matrixFact_user_ipchannel_item_appdeviceos</th>\n",
       "      <th>matrixFact_user_iposdeviceapp_item_app</th>\n",
       "      <th>os</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.355859</td>\n",
       "      <td>-0.083193</td>\n",
       "      <td>0.482835</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.231806</td>\n",
       "      <td>0.354194</td>\n",
       "      <td>-0.21365</td>\n",
       "      <td>...</td>\n",
       "      <td>2.718168</td>\n",
       "      <td>0.027336</td>\n",
       "      <td>1.292117</td>\n",
       "      <td>-0.207378</td>\n",
       "      <td>-0.221148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.269234</td>\n",
       "      <td>-0.426699</td>\n",
       "      <td>-1.119404</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.355859</td>\n",
       "      <td>-0.084415</td>\n",
       "      <td>0.482835</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.231806</td>\n",
       "      <td>0.262257</td>\n",
       "      <td>-0.21365</td>\n",
       "      <td>...</td>\n",
       "      <td>2.711555</td>\n",
       "      <td>0.086107</td>\n",
       "      <td>1.272907</td>\n",
       "      <td>-0.207378</td>\n",
       "      <td>-0.221148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.984914</td>\n",
       "      <td>-0.612455</td>\n",
       "      <td>-0.528700</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  app_day_hour_count  app_device_os_mean  attributed_timediff  channel  \\\n",
       "0    3           -1.355859           -0.083193             0.482835      133   \n",
       "1    3           -1.355859           -0.084415             0.482835      133   \n",
       "\n",
       "   device  hour  ip_app_day_hour_count  ip_app_device_countfromfuture  \\\n",
       "0       1    14              -0.231806                       0.354194   \n",
       "1       1    14              -0.231806                       0.262257   \n",
       "\n",
       "   ip_app_device_countfrompast ...  ip_device_os_lasttimediff  \\\n",
       "0                     -0.21365 ...                   2.718168   \n",
       "1                     -0.21365 ...                   2.711555   \n",
       "\n",
       "   ip_device_os_mean  ip_device_os_time2nextclick  \\\n",
       "0           0.027336                     1.292117   \n",
       "1           0.086107                     1.272907   \n",
       "\n",
       "   ip_device_os_time2previousclick  ip_os_day_hour_count  is_attributed  \\\n",
       "0                        -0.207378             -0.221148            0.0   \n",
       "1                        -0.207378             -0.221148            0.0   \n",
       "\n",
       "   matrixFact_user_ip_item_appdeviceos  \\\n",
       "0                            -0.269234   \n",
       "1                            -0.984914   \n",
       "\n",
       "   matrixFact_user_ipchannel_item_appdeviceos  \\\n",
       "0                                   -0.426699   \n",
       "1                                   -0.612455   \n",
       "\n",
       "   matrixFact_user_iposdeviceapp_item_app  os  \n",
       "0                               -1.119404  13  \n",
       "1                               -0.528700  19  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 4,
=======
     "execution_count": 5,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded test\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_feather('Feathers/ss_test_via_alltrain_typechanged.ftr')\n",
    "print('loaded test')\n",
    "test.set_index('index', inplace=True)\n",
    "del test.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
=======
   "execution_count": 6,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>app_day_hour_count</th>\n",
       "      <th>app_device_os_mean</th>\n",
       "      <th>attributed_timediff</th>\n",
       "      <th>channel</th>\n",
       "      <th>device</th>\n",
       "      <th>hour</th>\n",
       "      <th>ip_app_day_hour_count</th>\n",
       "      <th>ip_app_device_countfromfuture</th>\n",
       "      <th>ip_app_device_countfrompast</th>\n",
       "      <th>...</th>\n",
       "      <th>ip_device_os_lasttimediff</th>\n",
       "      <th>ip_device_os_mean</th>\n",
       "      <th>ip_device_os_time2nextclick</th>\n",
       "      <th>ip_device_os_time2previousclick</th>\n",
       "      <th>ip_os_day_hour_count</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>matrixFact_user_ip_item_appdeviceos</th>\n",
       "      <th>matrixFact_user_ipchannel_item_appdeviceos</th>\n",
       "      <th>matrixFact_user_iposdeviceapp_item_app</th>\n",
       "      <th>os</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.790197</td>\n",
       "      <td>-0.0755</td>\n",
       "      <td>0.091585</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.215741</td>\n",
       "      <td>-0.197744</td>\n",
       "      <td>-0.192336</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.132750</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>-0.205401</td>\n",
       "      <td>-0.205966</td>\n",
       "      <td>-0.217264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.521780</td>\n",
       "      <td>0.467860</td>\n",
       "      <td>1.140992</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.790197</td>\n",
       "      <td>-0.0755</td>\n",
       "      <td>-0.343579</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.119355</td>\n",
       "      <td>-0.060634</td>\n",
       "      <td>-0.101353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553337</td>\n",
       "      <td>0.201479</td>\n",
       "      <td>-0.204271</td>\n",
       "      <td>-0.205401</td>\n",
       "      <td>-0.190076</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.239275</td>\n",
       "      <td>0.255219</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   app  app_day_hour_count  app_device_os_mean  attributed_timediff  channel  \\\n",
       "0    9            0.790197             -0.0755             0.091585       17   \n",
       "1    9            0.790197             -0.0755            -0.343579      177   \n",
       "\n",
       "   device  hour  ip_app_day_hour_count  ip_app_device_countfromfuture  \\\n",
       "0       1     4              -0.215741                      -0.197744   \n",
       "1       1     4              -0.119355                      -0.060634   \n",
       "\n",
       "   ip_app_device_countfrompast ...  ip_device_os_lasttimediff  \\\n",
       "0                    -0.192336 ...                  -1.132750   \n",
       "1                    -0.101353 ...                   0.553337   \n",
       "\n",
       "   ip_device_os_mean  ip_device_os_time2nextclick  \\\n",
       "0           0.008702                    -0.205401   \n",
       "1           0.201479                    -0.204271   \n",
       "\n",
       "   ip_device_os_time2previousclick  ip_os_day_hour_count  is_attributed  \\\n",
       "0                        -0.205966             -0.217264            NaN   \n",
       "1                        -0.205401             -0.190076            NaN   \n",
       "\n",
       "   matrixFact_user_ip_item_appdeviceos  \\\n",
       "0                             0.521780   \n",
       "1                             0.239275   \n",
       "\n",
       "   matrixFact_user_ipchannel_item_appdeviceos  \\\n",
       "0                                    0.467860   \n",
       "1                                    0.255219   \n",
       "\n",
       "   matrixFact_user_iposdeviceapp_item_app  os  \n",
       "0                                1.140992   3  \n",
       "1                                0.778947   3  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 68,
=======
     "execution_count": 6,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ori_path = '/home/kai/data/kaggle/talkingdata/data/'\n",
    "\n",
    "test_ori = pd.read_csv(ori_path+\"test.csv\", usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 7,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "['attributed_timediff', 'matrixFact_user_ipchannel_item_appdeviceos', 'ip_os_day_hour_count', 'ip_app_device_os_time2previousclick', 'ip_device_os_count', 'matrixFact_user_ip_item_appdeviceos', 'ip_device_os_mean', 'ip_app_day_hour_count', 'ip_app_device_os_lasttimediff', 'ip_app_device_os_time2nextclick', 'ip_app_device_os_countfromfuture', 'ip_app_device_countfrompast', 'ip_app_device_os_countfrompast', 'ip_device_os_countfromfuture', 'ip_app_device_os_firsttimediff', 'app_day_hour_count', 'ip_app_device_time2previousclick', 'ip_app_os_day_hour_count', 'ip_app_device_mean', 'ip_app_device_firsttimediff', 'ip_device_os_time2previousclick', 'ip_app_device_os_count', 'ip_device_os_firsttimediff', 'app_device_os_mean', 'matrixFact_user_iposdeviceapp_item_app', 'ip_app_device_lasttimediff', 'ip_app_device_countfromfuture', 'ip_device_os_countfrompast', 'ip_app_device_os_mean', 'ip_day_hour_count', 'ip_app_device_time2nextclick', 'ip_device_os_lasttimediff', 'ip_device_os_time2nextclick'] 33\n"
=======
      "['app_day_hour_count', 'ip_app_device_time2nextclick', 'matrixFact_user_ip_item_appdeviceos', 'ip_app_day_hour_count', 'ip_device_os_time2nextclick', 'attributed_timediff', 'app_device_os_mean', 'ip_app_device_os_lasttimediff', 'ip_app_device_firsttimediff', 'ip_app_device_os_countfromfuture', 'ip_app_device_os_countfrompast', 'ip_app_device_os_time2previousclick', 'matrixFact_user_ipchannel_item_appdeviceos', 'ip_app_os_day_hour_count', 'ip_device_os_countfrompast', 'ip_app_device_countfromfuture', 'ip_device_os_time2previousclick', 'ip_app_device_mean', 'ip_device_os_firsttimediff', 'ip_app_device_os_firsttimediff', 'ip_app_device_countfrompast', 'ip_app_device_os_mean', 'ip_device_os_mean', 'ip_app_device_os_time2nextclick', 'ip_os_day_hour_count', 'ip_app_device_time2previousclick', 'ip_device_os_count', 'ip_device_os_lasttimediff', 'ip_day_hour_count', 'matrixFact_user_iposdeviceapp_item_app', 'ip_device_os_countfromfuture', 'ip_app_device_os_count', 'ip_app_device_lasttimediff'] 33\n"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     ]
    }
   ],
   "source": [
    "embedding_features = ['app','channel','device','os','hour']\n",
    "\n",
    "numeric_features = list(set(set(train.columns) - set(embedding_features)) - set(['is_attributed']))\n",
    "print(numeric_features, len(numeric_features))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 12,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keras_data(dataset):\n",
    "    X = dict((col, np.array(dataset[col])) for col in embedding_features)\n",
    "    X_num =  dict(('num_'+col, np.array(dataset[col])) for col in numeric_features)\n",
    "    X.update(X_num)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 8,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 8,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "((184903890, 39), 1.0)"
=======
       "((184903890, 39), (18790469, 39))"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "train.shape, train['ip_device_os_lasttimediff'].std()"
=======
    "train.shape, test.shape"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 69,
=======
   "execution_count": 10,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "((18790469, 39), 0.6089447)"
      ]
     },
     "execution_count": 69,
=======
       "(1.0, 0.6089447)"
      ]
     },
     "execution_count": 10,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "test.shape, test['ip_device_os_lasttimediff'].std()"
=======
    "train['ip_device_os_lasttimediff'].std(), test['ip_device_os_lasttimediff'].std()"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 11,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape, Flatten\n",
    "from keras.layers.merge import concatenate, dot, add, multiply\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import SpatialDropout1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 13,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "0"
      ]
     },
     "execution_count": 11,
=======
       "176"
      ]
     },
     "execution_count": 13,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 14,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hour = pd.read_csv('/home/kai/data/kaggle/talkingdata/data/hourdistri.csv', index_col='Unnamed: 0')\n",
    "index = {}\n",
<<<<<<< HEAD
    "for day in ['day7', 'day8','day9']:\n",
=======
    "for day in ['day7']:#, 'day8','day9']:\n",
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
    "    index[day] = list(range(df_hour.loc[day,'4start'], df_hour.loc[day,'6end0sec'])) + \\\n",
    "    list(range(df_hour.loc[day,'9start'], df_hour.loc[day,'11end0sec'])) + \\\n",
    "    list(range(df_hour.loc[day,'13start'], df_hour.loc[day,'15end0sec'])) "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 22,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forval = train.index.isin(index['day7'])\n",
    "\n",
    "assert np.sum(forval) == 19534560\n",
    "assert len(forval) == 184903890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day8start = int(len(train)*1/3)\n",
    "day8start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day9start = int(len(train)*2/3)\n",
    "day9start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "del train_df, y_train, val_df, y_val; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
=======
   "execution_count": 208,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "27.208416391164064"
      ]
     },
     "execution_count": 62,
=======
       "32.208416391164064"
      ]
     },
     "execution_count": 208,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(train)/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-32768, 32767)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int16).min, np.iinfo(np.int16).max"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
=======
   "execution_count": 28,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "165369330"
      ]
     },
     "execution_count": 63,
=======
       "(165369330, 165369330)"
      ]
     },
     "execution_count": 28,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "gc.collect()\n",
    "train_df = get_keras_data(train[~forval])#[day8start:])#train.iloc[index['day8']+index['day9']])#:(day9len+day8len)])\n",
    "y_train = train[~forval]['is_attributed'].values\n",
    "\n",
    "assert len(y_train) == len(train_df['app'])\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gc.collect()\n",
    "train_df = get_keras_data(train.iloc[index['day7']+index['day8']])#:(day9len+day8len)])\n",
    "y_train = train.iloc[index['day7']+index['day8']]['is_attributed'].values\n",
    "\n",
    "assert len(y_train) == len(train_df['app'])\n",
    "len(y_train)"
=======
    "train_df = get_keras_data(train[~forval])#[day8start:])#train.iloc[index['day8']+index['day9']])#:(day9len+day8len)])\n",
    "y_train = train[~forval]['is_attributed'].values\n",
    "\n",
    "len(y_train), len(train_df['app'])"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 64,
=======
   "execution_count": 27,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "19534560"
      ]
     },
     "execution_count": 64,
=======
       "(19534560, 19534560)"
      ]
     },
     "execution_count": 27,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = get_keras_data(train[forval])#[(day9len+day8len):(day9len+day8len+day7len)])\n",
    "y_val = train[forval]['is_attributed'].values\n",
    "\n",
<<<<<<< HEAD
    "assert len(y_val) == len(val_df['app'])\n",
    "len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gc.collect()\n",
    "val_df = get_keras_data(train.iloc[index['day9']])\n",
    "y_val = train.iloc[index['day9']]['is_attributed'].values\n",
    "\n",
    "assert len(y_val) == len(val_df['app'])\n",
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = get_keras_data(test)\n",
    "len(test_df['app'])"
=======
    "len(y_val), len(val_df['app'])"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 62,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FunctionalDense(n, x, batchnorm=False, act='relu', lw1=0.0, dropout=0, name=''):\n",
    "    if lw1 == 0.0:\n",
    "        x = Dense(n, name=name+'_dense')(x)\n",
    "    else:\n",
    "        x = Dense(n, kernel_regularizer=l1(lw1), name=name+'_dense')(x)\n",
    "    \n",
    "    if batchnorm:\n",
    "        x = BatchNormalization(name=name+'_batchnorm')(x)\n",
    "        \n",
    "    if act in {'relu', 'tanh', 'sigmoid'}:\n",
    "        x = Activation(act, name=name+'_activation')(x)\n",
    "    elif act =='prelu':\n",
    "        x = PReLU(name=name+'_activation')(x)\n",
    "    elif act == 'leakyrelu':\n",
    "        x = LeakyReLU(name=name+'_activation')(x)\n",
    "    elif act == 'elu':\n",
    "        x = ELU(name=name+'_activation')(x)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        x = Dropout(dropout, name=name+'_dropout')(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_size=50000):\n",
    "        super(Callback, self).__init__()\n",
    "        print('RocAuc evaluating batch size is: {}'.format(batch_size))\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            print('\\non epoch end, start predicting validation set...')\n",
    "            y_pred = self.model.predict(self.X_val, batch_size=self.batch_size,verbose=1)\n",
    "            print('start calculating ROC-AUC...')\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "            \n",
    "            \n",
    "import keras\n",
    "class RocAucMetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data=(), predict_batch_size=100000, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size=predict_batch_size\n",
    "        self.include_on_batch=include_on_batch\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if(self.include_on_batch):\n",
    "            logs['roc_auc_val']=float('-inf')\n",
    "            if(self.validation_data):\n",
    "                logs['roc_auc_val']=roc_auc_score(self.y_val,\n",
    "                                              self.model.predict(self.X_val,\n",
    "                                                                 batch_size=self.predict_batch_size))\n",
    "                \n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val']=float('-inf')\n",
    "#         y_val = self.validation_data[-3]\n",
    "#         val_df = self.validation_data[:-3]\n",
    "        if(self.validation_data):\n",
    "            logs['roc_auc_val']=roc_auc_score(self.y_val,\n",
    "                                              self.model.predict(self.X_val,\n",
    "                                                                 batch_size=self.predict_batch_size))\n",
    "#             logs['roc_auc_val']=roc_auc_score(y_val,#self.validation_data[1], \n",
    "#                                               self.model.predict(val_df,#self.validation_data[0],\n",
<<<<<<< HEAD
    "#                                                                  batch_size=self.predict_batch_size))\n",
    "\n",
    "def get_time():\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "    \n",
    "    return est.strftime('%Y-%m-%d %H:%M:%S')"
=======
    "#                                                                  batch_size=self.predict_batch_size))"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 31,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app': 729, 'channel': 201, 'device': 3798, 'hour': 23, 'os': 855}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 20,
=======
     "execution_count": 31,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dict = dict(train[embedding_features].max())\n",
    "max_dict"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "templ = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "templ2 = itertools.combinations(templ, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "3\n",
      "(1, 3)\n",
      "4\n",
      "(2, 3)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for pair in templ2:\n",
    "    print(pair)\n",
    "    print(pair[0]+pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.combinations object at 0x7ff74cb97318>\n"
     ]
    }
   ],
   "source": [
    "print(templ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(emb_n=50, lr=0.0015, conv_n=100, dense_n=300, drop_out=0.3, act='relu', batchnorm=False, mode=1):\n",
=======
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(emb_n=50, lr=0.0015, conv_n=100, dense_n=300, drop_out=0.3, act='relu', batchnorm=False):\n",
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
    "    #embids = embedding_features#['app', 'channel', 'device', 'os', 'hour', 'day', 'wday', 'qty', 'ip_app_count', 'ip_app_os_count']\n",
    "    # get the max of each code type\n",
    "    embmaxs = dict((col, max_dict[col]+1) for col in embedding_features)\n",
    "\n",
    "    # Build the inputs, embeddings and concatenate them all for each column\n",
    "    emb_inputs = dict((col, Input(shape=[1], name = col))  for col in embedding_features)\n",
    "    emb_model  = dict((col, Embedding(embmaxs[col], emb_n, trainable=True)(emb_inputs[col])) for col in embedding_features)\n",
    "\n",
<<<<<<< HEAD
    "    emb_flatten = []\n",
    "    for emb_ in emb_model.values():\n",
    "        emb_flatten.append(Flatten()(emb_))\n",
    "        \n",
    "    import itertools\n",
    "    emb_flatten_combine2 = itertools.combinations(emb_flatten, 2)\n",
    "    emb_dot = []\n",
    "    for pair in emb_flatten_combine2:\n",
    "        emb_dot.append(dot([pair[0],pair[1]], axes=1))\n",
    "        \n",
    "    \n",
    "#     fe = concatenate([(emb_) for emb_ in emb_model.values()])\n",
    "#     s_dout = SpatialDropout1D(drop_out)(fe)\n",
    "\n",
    "#     conv = Conv1D(conv_n, kernel_size=4, strides=1, padding='same')(s_dout)\n",
    "    \n",
=======
    "    fe = concatenate([(emb_) for emb_ in emb_model.values()])\n",
    "    s_dout = SpatialDropout1D(drop_out)(fe)\n",
    "\n",
    "    conv = Conv1D(conv_n, kernel_size=4, strides=1, padding='same')(s_dout)\n",
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
    "\n",
    "    num_inputs = dict((col, Input(shape=[1], name = 'num_'+col))  for col in numeric_features)\n",
    "\n",
    "    num_fe = concatenate([(num_) for num_ in num_inputs.values()])\n",
<<<<<<< HEAD
    "    \n",
    "    num_fe_preds0 = FunctionalDense(200, num_fe, batchnorm=batchnorm, act='relu', name='num_fe_preds0')\n",
    "    num_fe_preds1 = FunctionalDense(dense_n, num_fe_preds0, batchnorm=batchnorm, act='relu', name='num_fe_preds1')\n",
    "\n",
    "#     joint_embeddings = concatenate([Flatten()(s_dout), Flatten()(conv), (num_fe)])\n",
    "    joint_embeddings = concatenate([concatenate(emb_dot), (num_fe_preds1)])\n",
    "\n",
    "    if mode==1:\n",
    "        preds0 = FunctionalDense(dense_n, joint_embeddings, batchnorm=batchnorm, act=act, name='preds_0')\n",
    "        preds1 = FunctionalDense(dense_n, concatenate([joint_embeddings, preds0]), batchnorm=batchnorm, act=act, name='preds_1')\n",
    "        preds2 = FunctionalDense(dense_n, concatenate([joint_embeddings, preds0, preds1]), batchnorm=batchnorm, act=act, name='preds_2')\n",
    "\n",
    "        preds = concatenate([joint_embeddings, preds0, preds1, preds2], name='prediction_aggr')\n",
    "        preds = Dropout(drop_out, name='prediction_dropout')(preds)#preds\n",
    "\n",
    "#     elif mode==2:\n",
    "# #         preds = Dropout(drop_out)(Dense(dense_n*2,activation='relu')(joint_embeddings))\n",
    "# #         preds = Dropout(drop_out)(Dense(dense_n*2,activation='relu')(preds))\n",
    "#         preds = FunctionalDense(dense_n*2, joint_embeddings, batchnorm=batchnorm, act=act, dropout=drop_out, name='preds_0')\n",
    "#         preds = FunctionalDense(dense_n*2, preds, batchnorm=batchnorm, act=act, dropout=drop_out, name='preds_1')\n",
=======
    "\n",
    "    joint_embeddings = concatenate([Flatten()(s_dout), Flatten()(conv), (num_fe)])\n",
    "\n",
    "    preds0 = FunctionalDense(dense_n, joint_embeddings, batchnorm=batchnorm, act=act, name='preds_0')\n",
    "    preds1 = FunctionalDense(dense_n, concatenate([joint_embeddings, preds0]), batchnorm=batchnorm, act=act, name='preds_1')\n",
    "    preds2 = FunctionalDense(dense_n, concatenate([joint_embeddings, preds0, preds1]), batchnorm=batchnorm, act=act, name='preds_2')\n",
    "\n",
    "    preds = concatenate([joint_embeddings, preds0, preds1, preds2], name='prediction_aggr')\n",
    "    preds = Dropout(drop_out, name='prediction_dropout')(preds)#preds\n",
    "\n",
    "    #x = Dropout(0.2)(Dense(dense_n,activation='relu')(concat))\n",
    "    #x = Dropout(0.2)(Dense(dense_n,activation='relu')(x))\n",
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
    "\n",
    "    outp = Dense(1,activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[inp for inp in emb_inputs.values()] + [inp for inp in num_inputs.values()], outputs=outp)\n",
    "    #model = Model(inputs=[inp for inp in emb_inputs.values()], outputs=outp)\n",
    "\n",
<<<<<<< HEAD
=======
    "    batch_size = 200000\n",
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
    "    optimizer = RMSprop(lr=lr) # Adam\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model(emb_n, lr, conv_n, dense_n, drop_out, act, batchnorm, mode) # mode 2 untested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emb_n: 34, lr: 1.016685e-03, lr_decay: 7.097341e-01, conv_n: 50, dense_n: 475, drop_out: 0.300000, act: prelu, batchnorm: True, pos_weight: 123, mode: 1"
=======
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_n: 55, lr: 5.372772e-03, lr_decay: 6.588207e-01, conv_n: 81, dense_n: 480, drop_out: 0.200000, act: elu, batchnorm: True, pos_weight: 219\n",
      "Model ID: 5416260\n",
      "Train on 10000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      " 9800000/10000000 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.9287\n",
      "Epoch 00001: roc_auc_val improved from -inf to 0.97562, saving model to NN_MODELS/NN_data165369330_batchsize200000_ep1_5416260.hdf5\n",
      "10000000/10000000 [==============================] - 60s 6us/step - loss: 0.6117 - acc: 0.9295 - val_loss: 0.7205 - val_acc: 0.6351 - roc_auc_val: 0.9756\n",
      "emb_n: 53, lr: 6.208583e-03, lr_decay: 7.411320e-01, conv_n: 135, dense_n: 113, drop_out: 0.400000, act: tanh, batchnorm: False, pos_weight: 210\n",
      "Model ID: 5416371\n",
      "Train on 10000000 samples, validate on 1000000 samples\n",
      "Epoch 1/1\n",
      " 9800000/10000000 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.9535\n",
      "Epoch 00001: roc_auc_val improved from -inf to 0.95098, saving model to NN_MODELS/NN_data165369330_batchsize200000_ep1_5416371.hdf5\n",
      "10000000/10000000 [==============================] - 44s 4us/step - loss: 0.3305 - acc: 0.9539 - val_loss: 1.1781 - val_acc: 0.4067 - roc_auc_val: 0.9510\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):    \n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    emb_n = np.random.randint(30, 60)\n",
    "    lr = 7.5e-3 * (0.1 ** (np.random.rand() * 2 - 1.0))\n",
    "    conv_n = np.random.randint(50, 150)\n",
    "    dense_n = np.random.randint(100, 500)\n",
    "    drop_out = np.random.randint(2,5)/10\n",
    "    act = np.random.choice(['relu', 'tanh', 'prelu', 'leakyrelu', 'elu'])\n",
    "    batchnorm = np.random.choice([True, False])\n",
    "    \n",
    "    lr_decay = 0.65 + np.random.rand() * 0.3\n",
    "    pos_weight = np.random.randint(70, 400)\n",
    "    \n",
    "    print('emb_n: %d, lr: %e, lr_decay: %e, conv_n: %d, dense_n: %d, drop_out: %f, act: %s, batchnorm: %s, pos_weight: %d'%\\\n",
    "         (emb_n, lr, lr_decay, conv_n, dense_n, drop_out, act, batchnorm, pos_weight))\n",
    "    \n",
    "    model = get_model(emb_n, lr, conv_n, dense_n, drop_out, act, batchnorm)\n",
    "\n",
    "    class_weight = {0:1,1:pos_weight}\n",
    "    \n",
    "    epochs = 30\n",
    "    patience = 5\n",
    "    import time\n",
    "    tempid = str(int(time.time()))[3:]\n",
    "    print('Model ID: ' + tempid) \n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "    pred_batch_size = 100000\n",
    "    model_save_path = 'NN_MODELS/NN_data{}_batchsize{}_ep{}_{}.hdf5'.format(len(y_train), batch_size, epochs, tempid)\n",
    "\n",
    "    cb = [\n",
    "        RocAucMetricCallback(validation_data=(val_df, y_val)), # include it before EarlyStopping!\n",
    "        EarlyStopping(monitor='roc_auc_val',mode='max',patience=patience, verbose=2),\n",
    "        ModelCheckpoint(model_save_path, monitor='roc_auc_val', verbose=1, save_best_only=True, mode='max'),\n",
    "        LearningRateScheduler(lambda x: lr*(lr_decay**x))\n",
    "    #     RocAucEvaluation(validation_data=(val_df, y_val), interval=1, batch_size=pred_batch_size)\n",
    "    ]\n",
    "\n",
    "    model.fit(train_df, y_train, batch_size=batch_size, epochs=epochs, \n",
    "              validation_data=(val_df, y_val), \n",
    "    #           validation_split = 0.5,\n",
    "              class_weight=class_weight,\n",
    "              shuffle=True, verbose=1, callbacks=cb)#[rocauc, earlystopping, checkpoint])\n",
    "    \n",
    "    hist = model.history\n",
    "    bst_epoch = np.argmin(hist.history['val_loss'])\n",
    "    trn_loss = hist.history['loss'][bst_epoch]\n",
    "    trn_acc = hist.history['acc'][bst_epoch]\n",
    "    val_loss = hist.history['val_loss'][bst_epoch]\n",
    "    val_acc = hist.history['val_acc'][bst_epoch]    \n",
    "    val_auc = model.history.history['roc_auc_val'][bst_epoch]\n",
    "    \n",
    "    if str(val_auc)[2] != '5': # 5 means auc is 0.5, which indicates an error\n",
    "        test_pred = model.predict(test_df, batch_size=pred_batch_size, verbose=2) # 2 means no log\n",
    "        np.save('NN_SUBS/{}_{}'.format(str(val_auc)[2:7],tempid), test_pred)\n",
    "\n",
    "    res = '%s,%d,%e,%e,%d,%d,%f,%s,%s,%d,%d,%d,%d,%d,%d,%d,%.5f,%.5f,%.5f,%.5f,%.5f\\n'%\\\n",
    "    (tempid, emb_n, lr, lr_decay, conv_n, dense_n, drop_out, act, batchnorm, pos_weight, \\\n",
    "     -1,-1,-1,-1,-1,bst_epoch+1, trn_loss, trn_acc, val_loss, val_acc, val_auc)\n",
    "       \n",
    "    f = open('./nn_record.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_record = pd.read_csv('nn_record.csv', \n",
    "                        names=['tempid', 'emb_n', 'lr', 'lr_decay', 'conv_n', 'dense_n', \\\n",
    "                               'drop_out', 'act', 'batchnorm', 'pos_weight', 'NA1','NA2','NA3', \\\n",
    "                               'NA4','NA5', 'bst_epoch', 'trn_loss', 'trn_acc', 'val_loss', 'val_acc', 'val_auc'])"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_n: 76, lr: 7.509015e-03, lr_decay: 8.238572e-01, conv_n: 126, dense_n: 39, drop_out: 0.300000, act: relu, batchnorm: False, pos_weight: 201, mode: 1\n",
      "Model ID: 6001468 / Seed: 8242668\n",
      "Train on 165369330 samples, validate on 19534560 samples\n",
      "Epoch 1/30\n",
      " 16400000/165369330 [=>............................] - ETA: 8:32 - loss: 0.3849 - acc: 0.9660"
     ]
    }
   ],
   "source": [
    "mode_list = [1,1,2] * 50\n",
    "for i in range(10):\n",
    "    try:\n",
    "        seed = int(time.time()* 1000000) % 45234634\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        emb_n = np.random.randint(30, 80)\n",
    "        lr = 7.5e-3 * (0.1 ** (np.random.rand() * 2 - 1.0))\n",
    "        conv_n = np.random.randint(50, 150)\n",
    "        dense_n = np.random.randint(30, 70)#(100, 500)\n",
    "        drop_out = np.random.randint(1,4)/10\n",
    "        act = np.random.choice(['relu', 'tanh', 'prelu', 'leakyrelu', 'elu'])\n",
    "        batchnorm = np.random.choice([True, False])\n",
    "        mode = 1#mode_list[i]\n",
    "\n",
    "        lr_decay = 0.65 + np.random.rand() * 0.3\n",
    "        pos_weight = np.random.randint(70, 400)\n",
    "\n",
    "        print('emb_n: %d, lr: %e, lr_decay: %e, conv_n: %d, dense_n: %d, drop_out: %f, act: %s, batchnorm: %s, pos_weight: %d, mode: %d'%\\\n",
    "             (emb_n, lr, lr_decay, conv_n, dense_n, drop_out, act, batchnorm, pos_weight, mode))\n",
    "\n",
    "        model = get_model(emb_n, lr, conv_n, dense_n, drop_out, act, batchnorm, mode) # mode 2 untested\n",
    "\n",
    "        class_weight = {0:1,1:pos_weight}\n",
    "\n",
    "        epochs = 30\n",
    "        patience = 2\n",
    "        import time\n",
    "        tempid = str(int(time.time()))[3:]\n",
    "        print('Model ID: {} / Seed: {}'.format(tempid, seed))\n",
    "        batch_size = 200000\n",
    "        pred_batch_size = 100000\n",
    "        model_save_path = 'NN_MODELS/NN_data{}_batchsize{}_ep{}_{}.hdf5'.format(len(y_train), batch_size, epochs, tempid)\n",
    "\n",
    "        cb = [\n",
    "            RocAucMetricCallback(validation_data=(val_df, y_val)), # include it before EarlyStopping!\n",
    "            EarlyStopping(monitor='roc_auc_val',mode='max',patience=patience, verbose=2),\n",
    "            ModelCheckpoint(model_save_path, monitor='roc_auc_val', verbose=1, save_best_only=True, mode='max'),\n",
    "            LearningRateScheduler(lambda x: lr*(lr_decay**x))\n",
    "        #     RocAucEvaluation(validation_data=(val_df, y_val), interval=1, batch_size=pred_batch_size)\n",
    "        ]\n",
    "\n",
    "        model.fit(train_df, y_train, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(val_df, y_val), \n",
    "        #           validation_split = 0.5,\n",
    "                  class_weight=class_weight,\n",
    "                  shuffle=True, verbose=1, callbacks=cb)#[rocauc, earlystopping, checkpoint])\n",
    "\n",
    "        hist = model.history\n",
    "        bst_epoch = np.argmax(hist.history['roc_auc_val'])\n",
    "        trn_loss = hist.history['loss'][bst_epoch]\n",
    "        trn_acc = hist.history['acc'][bst_epoch]\n",
    "        val_loss = hist.history['val_loss'][bst_epoch]\n",
    "        val_acc = hist.history['val_acc'][bst_epoch]    \n",
    "        val_auc = hist.history['roc_auc_val'][bst_epoch]\n",
    "\n",
    "\n",
    "        res = '%s,%d,%e,%e,%d,%d,%f,%s,%s,%d,%d,%d,%d,%s,%d,%d,%.5f,%.5f,%.5f,%.5f,%.5f\\n'%\\\n",
    "        (tempid, emb_n, lr, lr_decay, conv_n, dense_n, drop_out, act, batchnorm, pos_weight, \\\n",
    "         -1,-1,seed,get_time(),mode,bst_epoch+1, trn_loss, trn_acc, val_loss, val_acc, val_auc)\n",
    "\n",
    "        f = open('./nn_record.csv', 'a')\n",
    "        f.write(res)\n",
    "        f.close()\n",
    "        \n",
    "        if str(val_auc)[2] != '5': # 5 means auc is 0.5, which indicates an error\n",
    "            model.load_weights(model_save_path)\n",
    "            test_pred = model.predict(test_df, batch_size=pred_batch_size, verbose=2) # 2 means no log\n",
    "            np.save('NN_SUBS/{}_{}'.format(str(val_auc)[2:7],tempid), test_pred)\n",
    "            print('load and predict DONE')\n",
    "        \n",
    "    except Exception as e:\n",
    "        if 'ResourceExhaustedError' in str(type(e)): # can't catch this error directly...tu \n",
    "            print('Oops! ResourceExhaustedError. Continue next round')\n",
    "            continue\n",
    "        else:\n",
    "            print(e)\n",
    "            break"
=======
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tempid</th>\n",
       "      <th>emb_n</th>\n",
       "      <th>lr</th>\n",
       "      <th>lr_decay</th>\n",
       "      <th>conv_n</th>\n",
       "      <th>dense_n</th>\n",
       "      <th>drop_out</th>\n",
       "      <th>act</th>\n",
       "      <th>batchnorm</th>\n",
       "      <th>pos_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>NA2</th>\n",
       "      <th>NA3</th>\n",
       "      <th>NA4</th>\n",
       "      <th>NA5</th>\n",
       "      <th>bst_epoch</th>\n",
       "      <th>trn_loss</th>\n",
       "      <th>trn_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5416260</td>\n",
       "      <td>55</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>0.658821</td>\n",
       "      <td>81</td>\n",
       "      <td>480</td>\n",
       "      <td>0.2</td>\n",
       "      <td>elu</td>\n",
       "      <td>True</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61171</td>\n",
       "      <td>0.92952</td>\n",
       "      <td>0.72047</td>\n",
       "      <td>0.63508</td>\n",
       "      <td>0.97562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5416371</td>\n",
       "      <td>53</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.741132</td>\n",
       "      <td>135</td>\n",
       "      <td>113</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>False</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33049</td>\n",
       "      <td>0.95393</td>\n",
       "      <td>1.17805</td>\n",
       "      <td>0.40672</td>\n",
       "      <td>0.95098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    tempid  emb_n        lr  lr_decay  conv_n  dense_n  drop_out   act  \\\n",
       "0  5416260     55  0.005373  0.658821      81      480       0.2   elu   \n",
       "1  5416371     53  0.006209  0.741132     135      113       0.4  tanh   \n",
       "\n",
       "   batchnorm  pos_weight   ...     NA2  NA3  NA4  NA5  bst_epoch  trn_loss  \\\n",
       "0       True         219   ...      -1   -1   -1   -1          1   0.61171   \n",
       "1      False         210   ...      -1   -1   -1   -1          1   0.33049   \n",
       "\n",
       "   trn_acc  val_loss  val_acc  val_auc  \n",
       "0  0.92952   0.72047  0.63508  0.97562  \n",
       "1  0.95393   1.17805  0.40672  0.95098  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_record.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.load_weights('NN_MODELS/NN_data165369330_batchsize200000_ep15_5394316.hdf5')"
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "emb_n: 38, lr: 2.781791e-02, lr_decay: 8.828531e-01, conv_n: 92, dense_n: 282, drop_out: 0.300000, act: relu, batchnorm: True, pos_weight: 254, mode: 1\n",
    "Model ID: 5983912 / Seed: 45234634\n",
    "Train on 39981303 samples, validate on 20898422 samples\n",
    "Epoch 1/5\n",
    "39800000/39981303 [============================>.] - ETA: 0s - loss: 0.6824 - acc: 0.9523\n",
    "Epoch 00001: roc_auc_val improved from -inf to 0.96514, saving model to NN_MODELS/NN_data39981303_batchsize200000_ep5_5983912.hdf5\n",
    "39981303/39981303 [==============================] - 257s 6us/step - loss: 0.6809 - acc: 0.9524 - val_loss: 0.2534 - val_acc: 0.9637 - roc_auc_val: 0.9651\n",
    "Epoch 2/5\n",
    "39800000/39981303 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.9663\n",
    "Epoch 00002: roc_auc_val improved from 0.96514 to 0.96870, saving model to NN_MODELS/NN_data39981303_batchsize200000_ep5_5983912.hdf5\n",
    "39981303/39981303 [==============================] - 254s 6us/step - loss: 0.3943 - acc: 0.9664 - val_loss: 0.2487 - val_acc: 0.9622 - roc_auc_val: 0.9687\n",
    "Epoch 3/5\n",
    "39800000/39981303 [============================>.] - ETA: 0s - loss: 0.3599 - acc: 0.9708\n",
    "Epoch 00003: roc_auc_val improved from 0.96870 to 0.96940, saving model to NN_MODELS/NN_data39981303_batchsize200000_ep5_5983912.hdf5\n",
    "39981303/39981303 [==============================] - 255s 6us/step - loss: 0.3599 - acc: 0.9709 - val_loss: 0.2450 - val_acc: 0.9666 - roc_auc_val: 0.9694\n",
    "Epoch 4/5\n",
    "39800000/39981303 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.9723\n",
    "Epoch 00004: roc_auc_val improved from 0.96940 to 0.96991, saving model to NN_MODELS/NN_data39981303_batchsize200000_ep5_5983912.hdf5\n",
    "39981303/39981303 [==============================] - 253s 6us/step - loss: 0.3404 - acc: 0.9723 - val_loss: 0.2268 - val_acc: 0.9681 - roc_auc_val: 0.9699\n",
    "Epoch 5/5\n",
    "39800000/39981303 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.9733\n",
    "Epoch 00005: roc_auc_val did not improve\n",
    "39981303/39981303 [==============================] - 255s 6us/step - loss: 0.3332 - acc: 0.9733 - val_loss: 0.2349 - val_acc: 0.9645 - roc_auc_val: 0.9693\n",
    "name 'test_df' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
=======
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model.load_weights('NN_batchsize200000_ep8_5308912.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 18790469)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = get_keras_data(test)\n",
    "\n",
    "len(test_df), len(test_df['app'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting....\n",
      "18790469/18790469 [==============================] - 39s 2us/step\n"
     ]
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_ori['click_id'].astype('int')\n",
    "\n",
    "print(\"predicting....\")\n",
    "sub['is_attributed'] = model.predict(test_df, batch_size=pred_batch_size, verbose=1)\n",
    "\n",
    "flag = 'AllDataExceptDay7eh'\n",
    "sub.to_csv('NN_SUBS/nn_{}_{}.csv.gz'.format(flag, tempid), index=False, float_format='%.9f', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
<<<<<<< HEAD
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
=======
>>>>>>> b2457b082fe748781adafe76607becab16758a0f
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
