{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T00:19:55.767390Z",
     "start_time": "2018-06-02T00:19:46.645450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# this script makes it possible to describe a model in one argument.\n",
    "\n",
    "lgb_target = 'model=LGBM_feat=lgbmBest_categoricalThreVal=10000_validation=subm_params=-,gbdt,0.45,0.04,188,7,auc,20,5,0,20,76,binary,0,0,32.0,1.0,200000,1,0'\n",
    "keras_target = 'BatchNormalization=on_sameNDenseAsEmb=off_model=keras_feat=kerasBest_validation=team_params=-,20000,1000,1,0.2,100,2,0.001,0.0001,0.001,100,2,3'\n",
    "target = lgb_target#''\n",
    "\n",
    "def get_opt(name,default=None):\n",
    "    global target\n",
    "    if target == '':\n",
    "        target = get_target()\n",
    "    if target == '':\n",
    "        return default\n",
    "    flds = target.replace('__','\u0001').split('_')\n",
    "    for fld in flds:\n",
    "        if fld == '':\n",
    "            continue\n",
    "        key, val = fld.split('=')\n",
    "        if key == name:\n",
    "            if isinstance(default, int):\n",
    "                val = int(val)\n",
    "            elif isinstance(default, float):\n",
    "                val = float(val)\n",
    "            else:\n",
    "                val = val.replace('\u0001','_')\n",
    "            return val\n",
    "    return default\n",
    "\n",
    "def get_target():\n",
    "    global target\n",
    "    if target != '':\n",
    "        return target\n",
    "    if len(sys.argv) > 1:\n",
    "        target = sys.argv[1]\n",
    "    else:\n",
    "        target = default_target\n",
    "    return target\n",
    "\n",
    "def reset_target():\n",
    "    global target\n",
    "    target = ''\n",
    "    a = get_target()\n",
    "    return get_target()\n",
    "\n",
    "def set_target(tgt):\n",
    "    global target\n",
    "    target = tgt\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "import os\n",
    "# from lib_util import get_target,get_opt\n",
    "import lightgbm as lgb\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Dropout, concatenate, BatchNormalization, SpatialDropout1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import gc\n",
    "\n",
    "def get_params(params_str):\n",
    "    if get_opt('model') == 'keras':\n",
    "        names = ['batch_size', 'dense_cate', 'dense_nume_n_layers', 'drop', 'emb_cate', 'epochs_for_lr', 'lr', 'lr_fin', 'lr_init', 'max_epochs', 'n_layers', 'patience']\n",
    "    elif 'LGBM' in get_opt('model'):\n",
    "        names = ['boosting_type','colsample_bytree','learning_rate','max_bin','max_depth','metric','min_child_samples','min_child_weight','min_split_gain','nthread','num_leaves','objective','reg_alpha','reg_lambda','scale_pos_weight','subsample','subsample_for_bin','subsample_freq','verbose']\n",
    "    else:\n",
    "        print(\"no valid target\")\n",
    "        sys.exit(1)\n",
    "    pvals = params_str.split(',')\n",
    "    del pvals[0]\n",
    "    if len(pvals) != len(names):\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!ERR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print('params: count is not fit',len(pvals), len(names))\n",
    "        print('params_str:',params_str)\n",
    "        print('names:',names)\n",
    "        print('param_values:',pvals)\n",
    "        sys.exit()\n",
    "    params = dict(zip(names, pvals))\n",
    "    return params\n",
    "\n",
    "def LGBM(X_tr,X_va,X_te,predictors,cat_feats,seed=2018):\n",
    "    params_str = get_opt('params')\n",
    "    if params_str != None:\n",
    "        params = get_params(params_str)\n",
    "        return LGBM_helper(X_tr,X_va,X_te,predictors,cat_feats,params,seed=2018)\n",
    "\n",
    "def Keras(X_tr,X_va,X_te,predictors,cat_feats,seed=2018):\n",
    "    params_str = get_opt('params')\n",
    "    if params_str != None:\n",
    "        params = get_params(params_str)\n",
    "        return Keras0_helper(X_tr,X_va,X_te,predictors,cat_feats,params,seed=2018)\n",
    "\n",
    "def LGBM_helper(_X_tr,_X_va,_X_te,predictors,cat_feats,params,seed=2018):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    X_tr = _X_tr[predictors]\n",
    "    X_va = _X_va[predictors]\n",
    "    X_te = _X_te[predictors]\n",
    "    y_tr = _X_tr['is_attributed']\n",
    "    y_va = _X_va['is_attributed']\n",
    "    y_te = _X_te['is_attributed']\n",
    "    params['feature_fraction_seed'] = seed\n",
    "    params['bagging_seed'] = seed\n",
    "    params['drop_seed'] = seed\n",
    "    params['data_random_seed'] = seed\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['subsample_for_bin'] = int(params['subsample_for_bin'])\n",
    "    params['max_depth'] = int(np.log2(params['num_leaves'])+1.2)\n",
    "    params['max_bin'] = int(params['max_bin'])\n",
    "    print('*'*50)\n",
    "    for k,v in sorted(params.items()):\n",
    "        print(k,':',v)\n",
    "    columns = X_tr.columns\n",
    "\n",
    "    print('start for lgvalid')\n",
    "    lgvalid = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_feats)\n",
    "    _X_va.drop(predictors,axis=1)\n",
    "    del _X_va, X_va, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    print('start for lgtrain')\n",
    "    lgtrain = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_feats)\n",
    "    _X_te.drop(predictors,axis=1)\n",
    "    del _X_tr, X_tr, y_tr\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "    if get_opt('trainCheck','-') == 'on':\n",
    "         valid_names=['train','valid']\n",
    "         valid_sets=[lgtrain, lgvalid]\n",
    "    else:\n",
    "         valid_names=['valid']\n",
    "         valid_sets=[lgvalid]\n",
    "    if get_opt('testCheck','-') == 'on':\n",
    "         valid_names.append('test')\n",
    "         lgtest = lgb.Dataset(X_te, label=y_te, categorical_feature=cat_feats)\n",
    "         valid_sets.append(lgtest)\n",
    "\n",
    "    print('start training')\n",
    "    bst = lgb.train(params,\n",
    "                     lgtrain,\n",
    "                     valid_sets=valid_sets,\n",
    "                     valid_names=valid_names,\n",
    "                     evals_result=evals_results,\n",
    "                     num_boost_round=2000,\n",
    "                     early_stopping_rounds=100,\n",
    "                     verbose_eval=10,\n",
    "                     )\n",
    "\n",
    "    importance = bst.feature_importance()\n",
    "    print('importance (count)')\n",
    "    tuples = sorted(zip(columns, importance), key=lambda x: x[1],reverse=True)\n",
    "    for col, val in tuples:\n",
    "        print(val,\"\\t\",col)\n",
    "\n",
    "    importance = bst.feature_importance(importance_type='gain')\n",
    "    print('importance (gain)')\n",
    "    tuples = sorted(zip(columns, importance), key=lambda x: x[1],reverse=True)\n",
    "    for col, val in tuples:\n",
    "        print(val,\"\\t\",col)\n",
    "\n",
    "    n_estimators = bst.best_iteration\n",
    "    metric = params['metric']\n",
    "    auc = evals_results['valid'][metric][n_estimators-1]\n",
    "    _X_te['pred'] = bst.predict(X_te)\n",
    "\n",
    "    return auc\n",
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(self,training_data=False,validation_data=False, testing_data=False, min_delta=0, patience=0, model_file=None, verbose=0):\n",
    "        super(EarlyStopping, self).__init__()\n",
    "        self.best_epoch = 0\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.monitor_op = np.greater\n",
    "        if training_data:\n",
    "            self.x_tr = training_data[0]\n",
    "            self.y_tr = training_data[1]\n",
    "        else:\n",
    "            self.x_tr = False\n",
    "            self.y_tr = False\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        if testing_data:\n",
    "            self.x_te = testing_data[0]\n",
    "            self.y_te = testing_data[1]\n",
    "        else:\n",
    "            self.x_te = False\n",
    "            self.y_te = False\n",
    "        self.model_file = model_file\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.wait = 0\n",
    "        self.best_epoch = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = -np.Inf\n",
    "    def on_train_end(self, logs={}):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch ',self.best_epoch,': EarlyStopping')\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if self.x_tr:\n",
    "            y_pred = self.model.predict(self.x_tr,batch_size=100000)\n",
    "            roc_tr = roc_auc_score(self.y_tr, y_pred)\n",
    "        else:\n",
    "            roc_tr = 0\n",
    "\n",
    "        y_hat_val=self.model.predict(self.x_val,batch_size=100000)\n",
    "        roc_val = roc_auc_score(self.y_val, y_hat_val)\n",
    "\n",
    "        if self.x_te:\n",
    "            y_hat_te=self.model.predict(self.x_te,batch_size=100000)\n",
    "            roc_te = roc_auc_score(self.y_te, y_hat_te)\n",
    "        else:\n",
    "            roc_te = 0\n",
    "        print('roc-auc: %s - roc-auc_val: %s - roc-auc_test: %s' % (str(round(roc_tr,6)),str(round(roc_val,6)), str(round(roc_te,6))),end=100*' '+'\\n')\n",
    "\n",
    "        if self.model_file:\n",
    "            print(\"saving\",self.model_file+'.'+str(epoch))\n",
    "            self.model.save_weights(self.model_file+'.'+str(epoch))\n",
    "        if(self.x_val):\n",
    "            if get_opt('testCheck','-') == 'on': \n",
    "                current = roc_te\n",
    "            else:\n",
    "                current = roc_val\n",
    "            if self.monitor_op(current - self.min_delta, self.best):\n",
    "                self.best = current\n",
    "                self.best_epoch = epoch\n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stopped_epoch = epoch\n",
    "                    self.model.stop_training = True\n",
    "\n",
    "def Keras0_helper(_X_tr,_X_va,_X_te,predictors,cat_feats,params,seed=2018):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    X_tr = _X_tr[predictors]\n",
    "    X_va = _X_va[predictors]\n",
    "    X_te = _X_te[predictors]\n",
    "    y_tr = _X_tr['is_attributed']\n",
    "    y_va = _X_va['is_attributed']\n",
    "    y_te = _X_te['is_attributed']\n",
    "    print('*************params**************')\n",
    "    for f in sorted(params): print(f+\":\",params[f])\n",
    "    batch_size = int(params['batch_size'])\n",
    "    epochs_for_lr = float(params['epochs_for_lr'])\n",
    "    max_epochs = int(params['max_epochs'])\n",
    "    emb_cate = int(params['emb_cate'])\n",
    "    dense_cate = int(params['dense_cate'])\n",
    "    dense_nume_n_layers = int(params['dense_nume_n_layers'])\n",
    "    drop = float(params['drop'])\n",
    "    lr= float(params['lr'])\n",
    "    lr_init = float(params['lr_init'])\n",
    "    lr_fin = float(params['lr_fin'])\n",
    "    n_layers = int(params['n_layers'])\n",
    "    patience = int(params['patience'])\n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    test_dict = {}\n",
    "    input_list = []\n",
    "    emb_list = []\n",
    "    numerical_feats = []\n",
    "    tot_emb_n = 0\n",
    "    for col in X_tr:\n",
    "        if col not in cat_feats:\n",
    "            numerical_feats.append(col)\n",
    "    if len(cat_feats) > 0:\n",
    "        for col in cat_feats:\n",
    "            train_dict[col] = np.array(X_tr[col])\n",
    "            valid_dict[col] = np.array(X_va[col])\n",
    "            test_dict[col] = np.array(X_te[col])\n",
    "            inpt = Input(shape=[1], name = col)\n",
    "            input_list.append(inpt)\n",
    "            max_val = np.max([X_tr[col].max(), X_va[col].max(), X_te[col].max()])+1\n",
    "            emb_n = np.min([emb_cate, max_val])\n",
    "            if get_opt('fixEmb','on') == 'on':\n",
    "                emb_n = emb_cate\n",
    "            tot_emb_n += emb_n\n",
    "            if emb_n == 1:\n",
    "                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Warinig!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! emb_1 = 1\")\n",
    "                return 0\n",
    "            print('Embedding size:',max_val, emb_cate, X_tr[col].max(), X_va[col].max(), X_te[col].max(), emb_n,col)\n",
    "            embd = Embedding(max_val, emb_n)(inpt)\n",
    "            emb_list.append(embd)\n",
    "        if len(emb_list) == 1:\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Warinig!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! emb_list = 1\")\n",
    "            return 0\n",
    "        fe = concatenate(emb_list)\n",
    "        s_dout = SpatialDropout1D(drop)(fe)\n",
    "        x1 = Flatten()(s_dout)\n",
    "\n",
    "    if get_opt('sameNDenseAsEmb','-') == 'on':\n",
    "        dense_cate = tot_emb_n\n",
    "    if len(numerical_feats) > 0:\n",
    "        train_dict['numerical'] = X_tr[numerical_feats].values\n",
    "        valid_dict['numerical'] = X_va[numerical_feats].values\n",
    "        test_dict['numerical'] = X_te[numerical_feats].values\n",
    "        inpt = Input((len(numerical_feats),),name='numerical')\n",
    "        input_list.append(inpt)\n",
    "        x2 = inpt\n",
    "        for n in range(dense_nume_n_layers):\n",
    "            x2 = Dense(dense_cate,activation='relu',kernel_initializer=RandomUniform(seed=seed))(x2)\n",
    "            if get_opt('numeDropout','on') != 'off':\n",
    "                x2 = Dropout(drop)(x2)\n",
    "            if get_opt('NumeBatchNormalization','on') != 'off':\n",
    "                x2 = BatchNormalization()(x2)\n",
    "\n",
    "    if len(numerical_feats) > 0 and len(cat_feats) > 0:\n",
    "        x = concatenate([x1, x2])\n",
    "    elif len(numerical_feats) > 0:\n",
    "        x =  x2\n",
    "    elif len(cat_feats) > 0:\n",
    "        x =  x1\n",
    "    else:\n",
    "        return 0 # for small data test\n",
    "\n",
    "    for n in range(n_layers):\n",
    "        x = Dense(dense_cate,activation='relu',kernel_initializer=RandomUniform(seed=seed))(x)\n",
    "        if get_opt('lastDropout','on') != 'off':\n",
    "            x = Dropout(drop)(x)\n",
    "        if get_opt('BatchNormalization','off') == 'on' or get_opt('LastBatchNormalization','off') == 'on':\n",
    "            x = BatchNormalization()(x)\n",
    "    outp = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform(seed=seed))(x)\n",
    "    model = Model(inputs=input_list, outputs=outp)\n",
    "    if get_opt('optimizer','expo') == 'adam':\n",
    "        optimizer = Adam(lr=lr)\n",
    "    elif get_opt('optimizer','expo') == 'nadam':\n",
    "        optimizer = Nadam(lr=lr)\n",
    "    else:\n",
    "        exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "        steps = int(len(X_tr) / batch_size) * epochs_for_lr\n",
    "        lr_init, lr_fin = 0.001, 0.0001\n",
    "        lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "        optimizer = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "    model.summary()\n",
    "    #from keras.utils import plot_model\n",
    "    #plot_model(model, to_file='model.png')\n",
    "\n",
    "    model_file = '../work/weights.'+str(os.getpid())+'.hdf5'\n",
    "    if get_opt('trainCheck','-') == 'on': \n",
    "        training_data=(train_dict, y_tr)\n",
    "    else:\n",
    "        training_data=False\n",
    "    if get_opt('testCheck','-') == 'on':\n",
    "        testing_data=(test_dict, y_te)\n",
    "    else:\n",
    "        testing_data=False\n",
    "    aucEarlyStopping = EarlyStopping(\n",
    "        training_data=training_data,\n",
    "        validation_data=(valid_dict,y_va),\n",
    "        testing_data=testing_data,\n",
    "        patience=patience,\n",
    "        model_file=model_file,\n",
    "        verbose=1)\n",
    "    model.fit(train_dict,\n",
    "        y_tr,\n",
    "        validation_data=[valid_dict, y_va],\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        callbacks=[aucEarlyStopping])\n",
    "    best_epoch = aucEarlyStopping.best_epoch\n",
    "    print('loading',model_file+'.'+str(best_epoch))\n",
    "    model.load_weights(model_file+'.'+str(best_epoch))\n",
    "    _X_te['pred'] = model.predict(test_dict, batch_size=batch_size, verbose=1)[:,0]\n",
    "    _X_va['pred'] = model.predict(valid_dict, batch_size=batch_size, verbose=1)[:,0]\n",
    "    if get_opt('avgEpoch',0) > 0:\n",
    "        added = 1\n",
    "        for i in range(min(get_opt('avgEpoch',0),patience)):\n",
    "            best_epoch = aucEarlyStopping.best_epoch + (i+1)\n",
    "            if best_epoch >= max_epochs:\n",
    "                continue\n",
    "            print('loading',model_file+'.'+str(best_epoch))\n",
    "            model.load_weights(model_file+'.'+str(best_epoch))\n",
    "            _X_te['pred'] += model.predict(test_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            _X_va['pred'] += model.predict(valid_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            added += 0.5\n",
    "            best_epoch = aucEarlyStopping.best_epoch - (i+1)\n",
    "            if best_epoch < 0:\n",
    "                continue\n",
    "            print('loading',model_file+'.'+str(best_epoch))\n",
    "            model.load_weights(model_file+'.'+str(best_epoch))\n",
    "            _X_te['pred'] += model.predict(test_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            _X_va['pred'] += model.predict(valid_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            added += 0.5\n",
    "        _X_te['pred'] /= added\n",
    "        _X_va['pred'] /= added\n",
    "\n",
    "    os.system('rm -f '+model_file+'.*')\n",
    "    auc = roc_auc_score(y_va, _X_va.pred)\n",
    "    return auc\n",
    "\n",
    "def Predict(X_tr,X_va,X_te,predictors,cat_feats,seed=2018):\n",
    "    model = get_opt('model')\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    if 'LGBM' in model:\n",
    "        return LGBM(X_tr,X_va,X_te,predictors,cat_feats,seed=2018)\n",
    "    elif 'keras' in model:\n",
    "        return Keras(X_tr,X_va,X_te,predictors,cat_feats,seed=2018)\n",
    "    else:\n",
    "        print(\"no valid model\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "#-*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.special import logit\n",
    "# from lib_util import get_target,get_opt,set_target,reset_target\n",
    "import shutil\n",
    "import pdb\n",
    "\n",
    "target=get_target()\n",
    "nrows=get_opt('nrows',-1)\n",
    "if nrows == -1:\n",
    "    nrows=None\n",
    "path = '../input/'\n",
    "work = '../work/'\n",
    "csv_dir='../csv/'\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        }\n",
    "\n",
    "# wrapper of pd.read_csv with cache\n",
    "def read_csv(csv_file,df_len=None,nrows=None,usecols=None,dtype=None):\n",
    "    nrows = 1000000\n",
    "    pkl_file = csv_file[:-4] + '.pkl'\n",
    "    if os.path.isfile(pkl_file) and nrows == None:\n",
    "        with open(pkl_file, 'rb') as pk:\n",
    "            print(\"loading\",pkl_file)\n",
    "            df = pickle.load(pk)\n",
    "        if df_len != None and len(df) != df_len:\n",
    "            print('ERROR!!!!!!!!!!!!!!!!!!!!!!!',pkl_file,'is broken',len(df),df_len)\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"loading\",csv_file)\n",
    "        df = pd.read_csv(csv_file, nrows=nrows)\n",
    "        if 'next' in csv_file:\n",
    "            df = np.absolute(df)\n",
    "        for ptn in df:\n",
    "            if dtype:\n",
    "                df = df.astype(dtype)\n",
    "            else:\n",
    "                df[ptn] = df[ptn].astype(get_type_with_fld_check(df,ptn))\n",
    "        if nrows == None and (df_len == None or len(df) == df_len):\n",
    "            print(\"saving cache file\",pkl_file)\n",
    "            with open(pkl_file+str(os.getpid()), 'wb') as pk:\n",
    "                pickle.dump(df,pk,protocol=4)\n",
    "            shutil.move(pkl_file+str(os.getpid()), pkl_file)\n",
    "        if nrows == None and df_len and len(df) != df_len:\n",
    "            print('ERROR!!!!!!!!!!!!!!!!!!!!!!!',csv_file,'is broken')\n",
    "            sys.exit(1)\n",
    "    if usecols != None:\n",
    "        df = df[usecols]\n",
    "    if nrows != None:\n",
    "        df = df[:nrows]\n",
    "    gc.collect()\n",
    "    if df_len != None and len(df) != df_len:\n",
    "        print('ERROR!!!!!!!!!!!!!!!!!!!!!!!',csv_file,'line is not same',df_len,len(df))\n",
    "        sys.exit(1)\n",
    "    return df\n",
    "\n",
    "def get_type_with_fld_check(df,ptn):\n",
    "    max_val = df[ptn].max()\n",
    "    if  'cumratio' in ptn or 'mean_' in ptn or 'Ratio' in ptn or 'CVR' in ptn or 'WOE' in ptn:\n",
    "        dtype = 'float16'\n",
    "    else:\n",
    "        if max_val < 256:\n",
    "            dtype = 'uint8'\n",
    "        elif max_val < 65536:\n",
    "            dtype = 'uint16'\n",
    "        else:\n",
    "            dtype = 'uint32'\n",
    "    return dtype\n",
    "\n",
    "def get_type(df,ptn):\n",
    "    max_val = df[ptn].max()\n",
    "    if max_val < 256:\n",
    "        dtype = 'uint8'\n",
    "    elif max_val < 65536:\n",
    "        dtype = 'uint16'\n",
    "    else:\n",
    "        dtype = 'uint32'\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def read_data_ph1():\n",
    "    keep_patterns = []\n",
    "    feat_opt = get_opt('feat','none')\n",
    "\n",
    "    if 'lgbmBest' == feat_opt:\n",
    "        numerical_patterns = ['WOEBnd_ip_nextClickLeakDayFlt', 'WOEBnd_app_nextClickLeakDayFlt', 'WOEBnd_device_nextClickLeakDayFlt', 'WOEBnd_os_nextClickLeakDayFlt', 'WOEBnd_channel_nextClickLeakDayFlt', 'WOEBnd_ip_app_nextClickLeakDayFlt', 'WOEBnd_ip_device_nextClickLeakDayFlt', 'WOEBnd_ip_os_nextClickLeakDayFlt', 'WOEBnd_ip_channel_nextClickLeakDayFlt', 'WOEBnd_app_device_nextClickLeakDayFlt', 'WOEBnd_app_os_nextClickLeakDayFlt', 'WOEBnd_app_channel_nextClickLeakDayFlt', 'WOEBnd_device_os_nextClickLeakDayFlt', 'WOEBnd_device_channel_nextClickLeakDayFlt', 'WOEBnd_os_channel_nextClickLeakDayFlt', 'WOEBnd_ip', 'WOEBnd_app', 'WOEBnd_device', 'WOEBnd_os', 'WOEBnd_channel', 'WOEBnd_ip_app', 'WOEBnd_ip_device', 'WOEBnd_ip_os', 'WOEBnd_ip_channel', 'WOEBnd_app_device', 'WOEBnd_app_os', 'WOEBnd_app_channel', 'WOEBnd_ip_app_device', 'WOEBnd_ip_app_os', 'WOEBnd_ip_app_channel', 'WOEBnd_ip_device_os', 'WOEBnd_ip_device_channel', 'WOEBnd_ip_os_channel', 'WOEBnd_app_device_os', 'WOEBnd_app_device_channel', 'WOEBnd_app_os_channel', 'WOEBnd_ip_app_device_os', 'WOEBnd_ip_app_device_channel', 'WOEBnd_ip_app_os_channel', 'WOEBnd_ip_device_os_channel', 'WOEBnd_app_device_os_channel', 'countRatio_ip_machine', 'countRatio_ip_channel', 'countRatio_machine_ip', 'countRatio_app_channel', 'countRatio_channel_app', 'uniqueCount_day_ip_os', 'uniqueCount_day_ip_device', 'uniqueCountRatio_day_ip_channel', 'uniqueCount_day_ip_machine', 'uniqueCount_day_ip_app',  'uniqueCount_machine_app', 'uniqueCount_machine_channel', 'uniqueCount_machine_ip', 'nextClickLeakDay', 'nextNextClickLeakDay', 'dayhourminute10count_ip_device_os', 'dayhourminute10count_ip_channel', 'dayhourminute10count_app_os_channel', 'cumratio_ip_day', 'cumcount_ip_day', 'count_ip_os', 'count_ip_device_os_day_hourminute10', 'count_ip_app_os_channel_day', 'count_ip_app_os_channel', 'count_ip_app_device_os_day_hour', 'count_ip_app_device_day', 'count_ip_app_device_channel_day', 'count_ip', 'count_device_os_day_hourminute10', 'count_app_os_channel_day_hour', 'count_app_device_day_hour', 'count_app_device_channel_day_hour', 'recumcount_app_device_os_day', 'var_ip_device_hour', 'count_app_day_hourminute']\n",
    "        cat_patterns = ['cat_os', 'cat_hour', 'cat_device', 'cat_dayhourcount_ip', 'cat_com1_ip', 'cat_channel', 'cat_app']\n",
    "    elif 'kerasBest' == feat_opt:\n",
    "        numerical_patterns = ['uniqueCountRatio_day_ip_machine', 'uniqueCountRatio_day_ip_app', 'uniqueCountRatio_day_ip_channel', 'uniqueCount_day_ip_machine', 'uniqueCount_day_ip_app', 'uniqueCount_day_ip_channel', 'uniqueCount_machine_app', 'uniqueCount_machine_channel', 'uniqueCount_machine_ip', 'nextClickLeakDay', 'dayhourcount_ip', 'count_ip', 'count_ip_app_device_os_day_hour', 'count_app_channel', 'cumcount_ip_app_device_os_day_hour', 'count_device_os_day_hourminute10', 'count_app_device_day_hour', 'dayhourminute10count_ip']\n",
    "        cat_patterns = ['cat_nextClickLeakDay', 'cat_nextNextClickLeakDay', 'cat_app', 'cat_device', 'cat_os', 'cat_count_ip', 'cat_count_app_channel', 'cat_hour', 'cat_dayhourcount_ip']\n",
    "    else:\n",
    "        print('ERR: no valid feat !!!!!!!!!!!!!!!!')\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"start reading feature for\",feat_opt)\n",
    "\n",
    "    # all cache\n",
    "    tgt = 'model=' + get_opt('model','none')\n",
    "    tgt += '_nrows=' + get_opt('nrows','0') \n",
    "    tgt += '_feat=' + get_opt('feat','0') \n",
    "    tgt += '_categoricalThreVal=' + get_opt('categoricalThreVal','1000') \n",
    "    tgt += '_offlineADD=' + get_opt('offlineADD','off') \n",
    "    tgt += '_sample=' + get_opt('sample','0.0') \n",
    "    tgt += '_noTestSample=' + get_opt('noTestSample','off') \n",
    "    tgt += '_noLogDev=' + get_opt('noLogDev','off') \n",
    "    tgt += '_smallTest=' + get_opt('smallTest','off') \n",
    "    tgt += '_ver=3'\n",
    "#     tr_pkl_file = '../work/train_' + tgt + '.pkl'\n",
    "#     te_pkl_file = '../work/test_supplement_' + tgt + '.pkl'\n",
    "#     if os.path.isfile(tr_pkl_file) == True and os.path.isfile(te_pkl_file) == True:\n",
    "#         with open(tr_pkl_file, 'rb') as pk:\n",
    "#             print(\"loading\",tr_pkl_file)\n",
    "#             train_df = pickle.load(pk)\n",
    "#         with open(te_pkl_file, 'rb') as pk:\n",
    "#             print(\"loading\",te_pkl_file)\n",
    "#             test_df = pickle.load(pk)\n",
    "#         gc.collect()\n",
    "#         return train_df, test_df, numerical_patterns, cat_patterns\n",
    "\n",
    "    # reading base data\n",
    "    train_df = read_csv(work+\"train_base.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel','day','hour','is_attributed'],nrows=nrows)\n",
    "    test_df = read_csv(work+\"test_supplement_base.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel','day','hour'],nrows=nrows)\n",
    "    test_df['is_attributed'] = 0\n",
    "    \n",
    "    #reading categorical data\n",
    "    n = 0\n",
    "    for ptn in cat_patterns:\n",
    "        n+=1\n",
    "        print('start categorical convert for',ptn,n,'/',len(cat_patterns))\n",
    "        if ptn in train_df.columns:\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! warning cat ptn is in train_df.columns !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(ptn,train_df.columns)\n",
    "        org_ptn = ptn[4:]\n",
    "        if org_ptn in train_df.columns:\n",
    "            _train_df = train_df[[org_ptn]]\n",
    "            _test_df = test_df[[org_ptn]]\n",
    "        else:\n",
    "            _train_df = read_csv(work + 'train_' + org_ptn + '.csv', nrows=nrows, df_len=len(train_df))\n",
    "            _test_df = read_csv(work + 'test_supplement_' + org_ptn + '.csv', nrows=nrows, df_len=len(test_df))\n",
    "            \n",
    "            \n",
    "        _train_df = _train_df.rename(columns={org_ptn: ptn})\n",
    "        _test_df = _test_df.rename(columns={org_ptn: ptn})\n",
    "        \n",
    "        len_train = len(_train_df)\n",
    "        _df = _train_df.append(_test_df)\n",
    "        thre_val = get_opt('categoricalThreVal',1000)\n",
    "        max_val = _df[ptn].max()\n",
    "        if 'cat_device' == ptn and get_opt('noLogDev','-') == 'on':\n",
    "            _df[ptn] = LabelEncoder().fit_transform(_df[ptn])\n",
    "        elif thre_val > 0 and max_val > thre_val:\n",
    "            if 'cumratio' in ptn:\n",
    "                fixed_vals = (10000*df[ptn]).astype('uint16')\n",
    "            else:\n",
    "#                 pdb.set_trace()\n",
    "                fixed_vals = (np.log2(_df[ptn]+1)*thre_val/100).astype('uint16')\n",
    "            _df[ptn] = LabelEncoder().fit_transform(fixed_vals)\n",
    "            print('logged for',ptn,max_val,fixed_vals.max(), _df[ptn].max())\n",
    "        else:\n",
    "#             pass\n",
    "            _df[ptn] = LabelEncoder().fit_transform(_df[ptn])\n",
    "        _df[ptn] = _df[ptn].astype(get_type(_df,ptn))\n",
    "\n",
    "        train_df[ptn] = _df[:len_train]\n",
    "        test_df[ptn] = _df[len_train:]\n",
    "        gc.collect()\n",
    "             \n",
    "    # reading numerical data\n",
    "    n = 0\n",
    "    for ptn in numerical_patterns:\n",
    "        n+=1\n",
    "        print('start for',ptn,n,'/',len(numerical_patterns))\n",
    "        if ptn in train_df.columns: continue\n",
    "        train_df[ptn] = read_csv(work + 'train_' + ptn + '.csv', nrows=nrows, df_len=len(train_df))\n",
    "        test_df[ptn] = read_csv(work + 'test_supplement_' + ptn + '.csv', nrows=nrows, df_len=len(test_df))\n",
    "\n",
    "    # numerical data conversion\n",
    "    if get_opt('model','-') == 'keras':\n",
    "        for ptn in numerical_patterns:\n",
    "            print('start for numerical convert',ptn)\n",
    "            all_df = train_df[[ptn]].append(test_df[[ptn]])\n",
    "            if 'cumratio' in ptn or 'CVRTgt' in ptn or 'WOETgt' in ptn:\n",
    "                pass\n",
    "            else:\n",
    "                all_df = np.log2(all_df+1)\n",
    "            all_df = StandardScaler().fit_transform(all_df).astype('float16')\n",
    "            train_df[ptn] = all_df[:len(train_df)]\n",
    "            test_df[ptn] = all_df[len(train_df):]\n",
    "\n",
    "#     # saving cache\n",
    "#     print(\"saving\",tr_pkl_file)\n",
    "#     with open(tr_pkl_file+str(os.getpid()), 'wb') as pk:\n",
    "#         pickle.dump(train_df,pk,protocol=4)\n",
    "#     shutil.move(tr_pkl_file+str(os.getpid()), tr_pkl_file)\n",
    "#     print(\"saving\",te_pkl_file)\n",
    "#     with open(te_pkl_file+str(os.getpid()), 'wb') as pk:\n",
    "#         pickle.dump(test_df,pk,protocol=4)\n",
    "#     shutil.move(te_pkl_file+str(os.getpid()), te_pkl_file)\n",
    "#     print('saved cache file')\n",
    "\n",
    "    gc.collect()\n",
    "    return train_df, test_df, numerical_patterns, cat_patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training / prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T00:19:55.774224Z",
     "start_time": "2018-06-02T00:19:55.771954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start for model=LGBM_feat=lgbmBest_categoricalThreVal=10000_validation=subm_params=-,gbdt,0.45,0.04,188,7,auc,20,5,0,20,76,binary,0,0,32.0,1.0,200000,1,0\n"
     ]
    }
   ],
   "source": [
    "target=get_target()\n",
    "print('start for',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T00:20:24.210369Z",
     "start_time": "2018-06-02T00:19:55.778386Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading feature for lgbmBest\n",
      "loading ../work/train_base.csv\n",
      "loading ../work/test_supplement_base.csv\n",
      "start categorical convert for cat_os 1 / 7\n",
      "start categorical convert for cat_hour 2 / 7\n",
      "start categorical convert for cat_device 3 / 7\n",
      "start categorical convert for cat_dayhourcount_ip 4 / 7\n",
      "loading ../work/train_dayhourcount_ip.csv\n",
      "loading ../work/test_supplement_dayhourcount_ip.csv\n",
      "start categorical convert for cat_com1_ip 5 / 7\n",
      "loading ../work/train_com1_ip.csv\n",
      "loading ../work/test_supplement_com1_ip.csv\n",
      "logged for cat_com1_ip 87804 1642 2\n",
      "start categorical convert for cat_channel 6 / 7\n",
      "start categorical convert for cat_app 7 / 7\n",
      "start for WOEBnd_ip_nextClickLeakDayFlt 1 / 76\n",
      "loading ../work/train_WOEBnd_ip_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_app_nextClickLeakDayFlt 2 / 76\n",
      "loading ../work/train_WOEBnd_app_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_device_nextClickLeakDayFlt 3 / 76\n",
      "loading ../work/train_WOEBnd_device_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_device_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_os_nextClickLeakDayFlt 4 / 76\n",
      "loading ../work/train_WOEBnd_os_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_os_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_channel_nextClickLeakDayFlt 5 / 76\n",
      "loading ../work/train_WOEBnd_channel_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_channel_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_ip_app_nextClickLeakDayFlt 6 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_ip_device_nextClickLeakDayFlt 7 / 76\n",
      "loading ../work/train_WOEBnd_ip_device_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_device_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_ip_os_nextClickLeakDayFlt 8 / 76\n",
      "loading ../work/train_WOEBnd_ip_os_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_os_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_ip_channel_nextClickLeakDayFlt 9 / 76\n",
      "loading ../work/train_WOEBnd_ip_channel_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_channel_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_app_device_nextClickLeakDayFlt 10 / 76\n",
      "loading ../work/train_WOEBnd_app_device_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_device_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_app_os_nextClickLeakDayFlt 11 / 76\n",
      "loading ../work/train_WOEBnd_app_os_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_os_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_app_channel_nextClickLeakDayFlt 12 / 76\n",
      "loading ../work/train_WOEBnd_app_channel_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_channel_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_device_os_nextClickLeakDayFlt 13 / 76\n",
      "loading ../work/train_WOEBnd_device_os_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_device_os_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_device_channel_nextClickLeakDayFlt 14 / 76\n",
      "loading ../work/train_WOEBnd_device_channel_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_device_channel_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_os_channel_nextClickLeakDayFlt 15 / 76\n",
      "loading ../work/train_WOEBnd_os_channel_nextClickLeakDayFlt.csv\n",
      "loading ../work/test_supplement_WOEBnd_os_channel_nextClickLeakDayFlt.csv\n",
      "start for WOEBnd_ip 16 / 76\n",
      "loading ../work/train_WOEBnd_ip.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip.csv\n",
      "start for WOEBnd_app 17 / 76\n",
      "loading ../work/train_WOEBnd_app.csv\n",
      "loading ../work/test_supplement_WOEBnd_app.csv\n",
      "start for WOEBnd_device 18 / 76\n",
      "loading ../work/train_WOEBnd_device.csv\n",
      "loading ../work/test_supplement_WOEBnd_device.csv\n",
      "start for WOEBnd_os 19 / 76\n",
      "loading ../work/train_WOEBnd_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_os.csv\n",
      "start for WOEBnd_channel 20 / 76\n",
      "loading ../work/train_WOEBnd_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_channel.csv\n",
      "start for WOEBnd_ip_app 21 / 76\n",
      "loading ../work/train_WOEBnd_ip_app.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app.csv\n",
      "start for WOEBnd_ip_device 22 / 76\n",
      "loading ../work/train_WOEBnd_ip_device.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_device.csv\n",
      "start for WOEBnd_ip_os 23 / 76\n",
      "loading ../work/train_WOEBnd_ip_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_os.csv\n",
      "start for WOEBnd_ip_channel 24 / 76\n",
      "loading ../work/train_WOEBnd_ip_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_channel.csv\n",
      "start for WOEBnd_app_device 25 / 76\n",
      "loading ../work/train_WOEBnd_app_device.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_device.csv\n",
      "start for WOEBnd_app_os 26 / 76\n",
      "loading ../work/train_WOEBnd_app_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_os.csv\n",
      "start for WOEBnd_app_channel 27 / 76\n",
      "loading ../work/train_WOEBnd_app_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_channel.csv\n",
      "start for WOEBnd_ip_app_device 28 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_device.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_device.csv\n",
      "start for WOEBnd_ip_app_os 29 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_os.csv\n",
      "start for WOEBnd_ip_app_channel 30 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_channel.csv\n",
      "start for WOEBnd_ip_device_os 31 / 76\n",
      "loading ../work/train_WOEBnd_ip_device_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_device_os.csv\n",
      "start for WOEBnd_ip_device_channel 32 / 76\n",
      "loading ../work/train_WOEBnd_ip_device_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_device_channel.csv\n",
      "start for WOEBnd_ip_os_channel 33 / 76\n",
      "loading ../work/train_WOEBnd_ip_os_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_os_channel.csv\n",
      "start for WOEBnd_app_device_os 34 / 76\n",
      "loading ../work/train_WOEBnd_app_device_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_device_os.csv\n",
      "start for WOEBnd_app_device_channel 35 / 76\n",
      "loading ../work/train_WOEBnd_app_device_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_device_channel.csv\n",
      "start for WOEBnd_app_os_channel 36 / 76\n",
      "loading ../work/train_WOEBnd_app_os_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_os_channel.csv\n",
      "start for WOEBnd_ip_app_device_os 37 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_device_os.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_device_os.csv\n",
      "start for WOEBnd_ip_app_device_channel 38 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_device_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_device_channel.csv\n",
      "start for WOEBnd_ip_app_os_channel 39 / 76\n",
      "loading ../work/train_WOEBnd_ip_app_os_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_app_os_channel.csv\n",
      "start for WOEBnd_ip_device_os_channel 40 / 76\n",
      "loading ../work/train_WOEBnd_ip_device_os_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_ip_device_os_channel.csv\n",
      "start for WOEBnd_app_device_os_channel 41 / 76\n",
      "loading ../work/train_WOEBnd_app_device_os_channel.csv\n",
      "loading ../work/test_supplement_WOEBnd_app_device_os_channel.csv\n",
      "start for countRatio_ip_machine 42 / 76\n",
      "loading ../work/train_countRatio_ip_machine.csv\n",
      "loading ../work/test_supplement_countRatio_ip_machine.csv\n",
      "start for countRatio_ip_channel 43 / 76\n",
      "loading ../work/train_countRatio_ip_channel.csv\n",
      "loading ../work/test_supplement_countRatio_ip_channel.csv\n",
      "start for countRatio_machine_ip 44 / 76\n",
      "loading ../work/train_countRatio_machine_ip.csv\n",
      "loading ../work/test_supplement_countRatio_machine_ip.csv\n",
      "start for countRatio_app_channel 45 / 76\n",
      "loading ../work/train_countRatio_app_channel.csv\n",
      "loading ../work/test_supplement_countRatio_app_channel.csv\n",
      "start for countRatio_channel_app 46 / 76\n",
      "loading ../work/train_countRatio_channel_app.csv\n",
      "loading ../work/test_supplement_countRatio_channel_app.csv\n",
      "start for uniqueCount_day_ip_os 47 / 76\n",
      "loading ../work/train_uniqueCount_day_ip_os.csv\n",
      "loading ../work/test_supplement_uniqueCount_day_ip_os.csv\n",
      "start for uniqueCount_day_ip_device 48 / 76\n",
      "loading ../work/train_uniqueCount_day_ip_device.csv\n",
      "loading ../work/test_supplement_uniqueCount_day_ip_device.csv\n",
      "start for uniqueCountRatio_day_ip_channel 49 / 76\n",
      "loading ../work/train_uniqueCountRatio_day_ip_channel.csv\n",
      "loading ../work/test_supplement_uniqueCountRatio_day_ip_channel.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start for uniqueCount_day_ip_machine 50 / 76\n",
      "loading ../work/train_uniqueCount_day_ip_machine.csv\n",
      "loading ../work/test_supplement_uniqueCount_day_ip_machine.csv\n",
      "start for uniqueCount_day_ip_app 51 / 76\n",
      "loading ../work/train_uniqueCount_day_ip_app.csv\n",
      "loading ../work/test_supplement_uniqueCount_day_ip_app.csv\n",
      "start for uniqueCount_machine_app 52 / 76\n",
      "loading ../work/train_uniqueCount_machine_app.csv\n",
      "loading ../work/test_supplement_uniqueCount_machine_app.csv\n",
      "start for uniqueCount_machine_channel 53 / 76\n",
      "loading ../work/train_uniqueCount_machine_channel.csv\n",
      "loading ../work/test_supplement_uniqueCount_machine_channel.csv\n",
      "start for uniqueCount_machine_ip 54 / 76\n",
      "loading ../work/train_uniqueCount_machine_ip.csv\n",
      "loading ../work/test_supplement_uniqueCount_machine_ip.csv\n",
      "start for nextClickLeakDay 55 / 76\n",
      "loading ../work/train_nextClickLeakDay.csv\n",
      "loading ../work/test_supplement_nextClickLeakDay.csv\n",
      "start for nextNextClickLeakDay 56 / 76\n",
      "loading ../work/train_nextNextClickLeakDay.csv\n",
      "loading ../work/test_supplement_nextNextClickLeakDay.csv\n",
      "start for dayhourminute10count_ip_device_os 57 / 76\n",
      "loading ../work/train_dayhourminute10count_ip_device_os.csv\n",
      "loading ../work/test_supplement_dayhourminute10count_ip_device_os.csv\n",
      "start for dayhourminute10count_ip_channel 58 / 76\n",
      "loading ../work/train_dayhourminute10count_ip_channel.csv\n",
      "loading ../work/test_supplement_dayhourminute10count_ip_channel.csv\n",
      "start for dayhourminute10count_app_os_channel 59 / 76\n",
      "loading ../work/train_dayhourminute10count_app_os_channel.csv\n",
      "loading ../work/test_supplement_dayhourminute10count_app_os_channel.csv\n",
      "start for cumratio_ip_day 60 / 76\n",
      "loading ../work/train_cumratio_ip_day.csv\n",
      "loading ../work/test_supplement_cumratio_ip_day.csv\n",
      "start for cumcount_ip_day 61 / 76\n",
      "loading ../work/train_cumcount_ip_day.csv\n",
      "loading ../work/test_supplement_cumcount_ip_day.csv\n",
      "start for count_ip_os 62 / 76\n",
      "loading ../work/train_count_ip_os.csv\n",
      "loading ../work/test_supplement_count_ip_os.csv\n",
      "start for count_ip_device_os_day_hourminute10 63 / 76\n",
      "loading ../work/train_count_ip_device_os_day_hourminute10.csv\n",
      "loading ../work/test_supplement_count_ip_device_os_day_hourminute10.csv\n",
      "start for count_ip_app_os_channel_day 64 / 76\n",
      "loading ../work/train_count_ip_app_os_channel_day.csv\n",
      "loading ../work/test_supplement_count_ip_app_os_channel_day.csv\n",
      "start for count_ip_app_os_channel 65 / 76\n",
      "loading ../work/train_count_ip_app_os_channel.csv\n",
      "loading ../work/test_supplement_count_ip_app_os_channel.csv\n",
      "start for count_ip_app_device_os_day_hour 66 / 76\n",
      "loading ../work/train_count_ip_app_device_os_day_hour.csv\n",
      "loading ../work/test_supplement_count_ip_app_device_os_day_hour.csv\n",
      "start for count_ip_app_device_day 67 / 76\n",
      "loading ../work/train_count_ip_app_device_day.csv\n",
      "loading ../work/test_supplement_count_ip_app_device_day.csv\n",
      "start for count_ip_app_device_channel_day 68 / 76\n",
      "loading ../work/train_count_ip_app_device_channel_day.csv\n",
      "loading ../work/test_supplement_count_ip_app_device_channel_day.csv\n",
      "start for count_ip 69 / 76\n",
      "loading ../work/train_count_ip.csv\n",
      "loading ../work/test_supplement_count_ip.csv\n",
      "start for count_device_os_day_hourminute10 70 / 76\n",
      "loading ../work/train_count_device_os_day_hourminute10.csv\n",
      "loading ../work/test_supplement_count_device_os_day_hourminute10.csv\n",
      "start for count_app_os_channel_day_hour 71 / 76\n",
      "loading ../work/train_count_app_os_channel_day_hour.csv\n",
      "loading ../work/test_supplement_count_app_os_channel_day_hour.csv\n",
      "start for count_app_device_day_hour 72 / 76\n",
      "loading ../work/train_count_app_device_day_hour.csv\n",
      "loading ../work/test_supplement_count_app_device_day_hour.csv\n",
      "start for count_app_device_channel_day_hour 73 / 76\n",
      "loading ../work/train_count_app_device_channel_day_hour.csv\n",
      "loading ../work/test_supplement_count_app_device_channel_day_hour.csv\n",
      "start for recumcount_app_device_os_day 74 / 76\n",
      "loading ../work/train_recumcount_app_device_os_day.csv\n",
      "loading ../work/test_supplement_recumcount_app_device_os_day.csv\n",
      "start for var_ip_device_hour 75 / 76\n",
      "loading ../work/train_var_ip_device_hour.csv\n",
      "loading ../work/test_supplement_var_ip_device_hour.csv\n",
      "start for count_app_day_hourminute 76 / 76\n",
      "loading ../work/train_count_app_day_hourminute.csv\n",
      "loading ../work/test_supplement_count_app_day_hourminute.csv\n",
      "CPU times: user 27.5 s, sys: 1.09 s, total: 28.6 s\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pdb.set_trace()\n",
    "train_df, test_df, numerical_patterns, cat_patterns = read_data_ph1()\n",
    "predictors = numerical_patterns + cat_patterns\n",
    "categorical = cat_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T20:54:41.793905Z",
     "start_time": "2018-06-01T20:54:41.778347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors), len(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T19:38:50.226280Z",
     "start_time": "2018-06-01T19:35:35.440730Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# is_val = (train_df['day'] == 9) & ((train_df['hour'] == 13) |(train_df['hour'] == 17) |(train_df['hour'] == 21))\n",
    "# val_df = train_df[is_val]\n",
    "# train_df = train_df[~is_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T00:20:35.970226Z",
     "start_time": "2018-06-02T00:20:35.820714Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df = train_df[-250000:]\n",
    "train_df = train_df[:-250000]\n",
    "val_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-02T01:11:35.042425Z",
     "start_time": "2018-06-02T01:11:20.791328Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc = Predict(train_df,val_df,test_df,predictors,categorical,seed=get_opt('seed',2018))\n",
    "print('validation auc:',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T14:15:53.920685Z",
     "start_time": "2018-05-31T14:14:47.200531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ../input/mapping.pkl\n",
      "loading ../input/sample_submission.pkl\n",
      "writing to ../csv/pred_test_model=LGBM_feat=lgbmBest_categoricalThreVal=10000_validation=subm_params=-,gbdt,0.45,0.04,188,7,auc,20,5,0,20,76,binary,0,0,32.0,1.0,200000,1,0.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_df = test_df[['pred']].rename(columns={'pred': 'is_attributed'})\n",
    "mapping = read_csv('../input/mapping.csv')\n",
    "click_id = read_csv('../input/sample_submission.csv',usecols=['click_id'])\n",
    "test_df = test_df.reset_index().merge(mapping, left_on='index', right_on='old_click_id', how='left')\n",
    "test_df = click_id.merge(test_df,on='click_id',how='left')\n",
    "outfile = '../csv/pred_test_'+target+'.csv'\n",
    "print('writing to',outfile)\n",
    "test_df[['click_id','is_attributed']].to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T14:13:59.564351Z",
     "start_time": "2018-05-31T14:13:59.539800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852901220779562"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T19:42:10.586877Z",
     "start_time": "2018-06-01T19:42:10.574747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10115468 entries, 148740843 to 178434048\n",
      "Data columns (total 91 columns):\n",
      "ip                                           uint32\n",
      "app                                          uint16\n",
      "device                                       uint16\n",
      "os                                           uint16\n",
      "channel                                      uint16\n",
      "day                                          int64\n",
      "hour                                         int64\n",
      "is_attributed                                uint8\n",
      "WOEBnd_ip_nextClickLeakDayFlt                float16\n",
      "WOEBnd_app_nextClickLeakDayFlt               float16\n",
      "WOEBnd_device_nextClickLeakDayFlt            float16\n",
      "WOEBnd_os_nextClickLeakDayFlt                float16\n",
      "WOEBnd_channel_nextClickLeakDayFlt           float16\n",
      "WOEBnd_ip_app_nextClickLeakDayFlt            float16\n",
      "WOEBnd_ip_device_nextClickLeakDayFlt         float16\n",
      "WOEBnd_ip_os_nextClickLeakDayFlt             float16\n",
      "WOEBnd_ip_channel_nextClickLeakDayFlt        float16\n",
      "WOEBnd_app_device_nextClickLeakDayFlt        float16\n",
      "WOEBnd_app_os_nextClickLeakDayFlt            float16\n",
      "WOEBnd_app_channel_nextClickLeakDayFlt       float16\n",
      "WOEBnd_device_os_nextClickLeakDayFlt         float16\n",
      "WOEBnd_device_channel_nextClickLeakDayFlt    float16\n",
      "WOEBnd_os_channel_nextClickLeakDayFlt        float16\n",
      "WOEBnd_ip                                    float16\n",
      "WOEBnd_app                                   float16\n",
      "WOEBnd_device                                float16\n",
      "WOEBnd_os                                    float16\n",
      "WOEBnd_channel                               float16\n",
      "WOEBnd_ip_app                                float16\n",
      "WOEBnd_ip_device                             float16\n",
      "WOEBnd_ip_os                                 float16\n",
      "WOEBnd_ip_channel                            float16\n",
      "WOEBnd_app_device                            float16\n",
      "WOEBnd_app_os                                float16\n",
      "WOEBnd_app_channel                           float16\n",
      "WOEBnd_ip_app_device                         float16\n",
      "WOEBnd_ip_app_os                             float16\n",
      "WOEBnd_ip_app_channel                        float16\n",
      "WOEBnd_ip_device_os                          float16\n",
      "WOEBnd_ip_device_channel                     float16\n",
      "WOEBnd_ip_os_channel                         float16\n",
      "WOEBnd_app_device_os                         float16\n",
      "WOEBnd_app_device_channel                    float16\n",
      "WOEBnd_app_os_channel                        float16\n",
      "WOEBnd_ip_app_device_os                      float16\n",
      "WOEBnd_ip_app_device_channel                 float16\n",
      "WOEBnd_ip_app_os_channel                     float16\n",
      "WOEBnd_ip_device_os_channel                  float16\n",
      "WOEBnd_app_device_os_channel                 float16\n",
      "countRatio_ip_machine                        float16\n",
      "countRatio_ip_channel                        float16\n",
      "countRatio_machine_ip                        float16\n",
      "countRatio_app_channel                       float16\n",
      "countRatio_channel_app                       float16\n",
      "uniqueCount_day_ip_os                        uint8\n",
      "uniqueCount_day_ip_device                    uint16\n",
      "uniqueCountRatio_day_ip_channel              float16\n",
      "uniqueCount_day_ip_machine                   uint16\n",
      "uniqueCount_day_ip_app                       uint8\n",
      "uniqueCount_machine_app                      uint16\n",
      "uniqueCount_machine_channel                  uint8\n",
      "uniqueCount_machine_ip                       uint32\n",
      "nextClickLeakDay                             uint32\n",
      "nextNextClickLeakDay                         uint32\n",
      "dayhourminute10count_ip_device_os            uint16\n",
      "dayhourminute10count_ip_channel              uint16\n",
      "dayhourminute10count_app_os_channel          uint16\n",
      "cumratio_ip_day                              float16\n",
      "cumcount_ip_day                              uint32\n",
      "count_ip_os                                  uint32\n",
      "count_ip_device_os_day_hourminute10          uint16\n",
      "count_ip_app_os_channel_day                  uint16\n",
      "count_ip_app_os_channel                      uint16\n",
      "count_ip_app_device_os_day_hour              uint16\n",
      "count_ip_app_device_day                      uint32\n",
      "count_ip_app_device_channel_day              uint16\n",
      "count_ip                                     uint32\n",
      "count_device_os_day_hourminute10             uint32\n",
      "count_app_os_channel_day_hour                uint32\n",
      "count_app_device_day_hour                    uint32\n",
      "count_app_device_channel_day_hour            uint32\n",
      "recumcount_app_device_os_day                 uint32\n",
      "var_ip_device_hour                           uint16\n",
      "count_app_day_hourminute                     uint16\n",
      "cat_os                                       uint16\n",
      "cat_hour                                     uint8\n",
      "cat_device                                   uint16\n",
      "cat_dayhourcount_ip                          uint8\n",
      "cat_com1_ip                                  uint8\n",
      "cat_channel                                  uint8\n",
      "cat_app                                      uint16\n",
      "dtypes: float16(48), int64(2), uint16(20), uint32(13), uint8(8)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "**************************************************\n",
    "bagging_seed : 2018\n",
    "boosting_type : gbdt\n",
    "colsample_bytree : 0.45\n",
    "data_random_seed : 2018\n",
    "drop_seed : 2018\n",
    "feature_fraction_seed : 2018\n",
    "learning_rate : 0.04\n",
    "max_bin : 188\n",
    "max_depth : 7\n",
    "metric : auc\n",
    "min_child_samples : 20\n",
    "min_child_weight : 5\n",
    "min_split_gain : 0\n",
    "nthread : 20\n",
    "num_leaves : 76\n",
    "objective : binary\n",
    "reg_alpha : 0\n",
    "reg_lambda : 0\n",
    "scale_pos_weight : 32.0\n",
    "subsample : 1.0\n",
    "subsample_for_bin : 200000\n",
    "subsample_freq : 1\n",
    "verbose : 0\n",
    "start for lgvalid\n",
    "start for lgtrain\n",
    "start training\n",
    "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
    "  warnings.warn('Using categorical_feature in Dataset.')\n",
    "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
    "  warnings.warn('categorical_feature in param dict is overrided.')\n",
    "Training until validation scores don't improve for 100 rounds.\n",
    "[10]\tvalid's auc: 0.976439\n",
    "[20]\tvalid's auc: 0.977661\n",
    "[30]\tvalid's auc: 0.978294\n",
    "[40]\tvalid's auc: 0.978978\n",
    "[50]\tvalid's auc: 0.979365\n",
    "[60]\tvalid's auc: 0.980071\n",
    "[70]\tvalid's auc: 0.980501\n",
    "[80]\tvalid's auc: 0.980976\n",
    "[90]\tvalid's auc: 0.981433\n",
    "[100]\tvalid's auc: 0.981916\n",
    "[110]\tvalid's auc: 0.982365\n",
    "[120]\tvalid's auc: 0.982697\n",
    "[140]\tvalid's auc: 0.983176\n",
    "[150]\tvalid's auc: 0.983445\n",
    "[160]\tvalid's auc: 0.983669\n",
    "[170]\tvalid's auc: 0.983835\n",
    "[180]\tvalid's auc: 0.983965\n",
    "[190]\tvalid's auc: 0.984083\n",
    "[200]\tvalid's auc: 0.984191\n",
    "[210]\tvalid's auc: 0.984321\n",
    "[220]\tvalid's auc: 0.984415\n",
    "[230]\tvalid's auc: 0.984487\n",
    "[240]\tvalid's auc: 0.984561\n",
    "[250]\tvalid's auc: 0.984613\n",
    "[260]\tvalid's auc: 0.984665\n",
    "[270]\tvalid's auc: 0.984715\n",
    "[280]\tvalid's auc: 0.984778\n",
    "[290]\tvalid's auc: 0.984809\n",
    "[300]\tvalid's auc: 0.984838\n",
    "[310]\tvalid's auc: 0.98488\n",
    "[320]\tvalid's auc: 0.984915\n",
    "[330]\tvalid's auc: 0.984946\n",
    "[340]\tvalid's auc: 0.984977\n",
    "[350]\tvalid's auc: 0.984989\n",
    "[360]\tvalid's auc: 0.985011\n",
    "[370]\tvalid's auc: 0.985032\n",
    "[380]\tvalid's auc: 0.985056\n",
    "[390]\tvalid's auc: 0.98507\n",
    "[400]\tvalid's auc: 0.985092\n",
    "[410]\tvalid's auc: 0.985107\n",
    "[420]\tvalid's auc: 0.985115\n",
    "[430]\tvalid's auc: 0.985124\n",
    "[440]\tvalid's auc: 0.985131\n",
    "[450]\tvalid's auc: 0.985134\n",
    "[460]\tvalid's auc: 0.985141\n",
    "[470]\tvalid's auc: 0.985141\n",
    "[480]\tvalid's auc: 0.985158\n",
    "[490]\tvalid's auc: 0.985164\n",
    "[500]\tvalid's auc: 0.985174\n",
    "[510]\tvalid's auc: 0.985183\n",
    "[520]\tvalid's auc: 0.985195\n",
    "[530]\tvalid's auc: 0.98521\n",
    "[540]\tvalid's auc: 0.985218\n",
    "[550]\tvalid's auc: 0.985223\n",
    "[560]\tvalid's auc: 0.985227\n",
    "[570]\tvalid's auc: 0.985232\n",
    "[580]\tvalid's auc: 0.985233\n",
    "[590]\tvalid's auc: 0.985242\n",
    "[600]\tvalid's auc: 0.985246\n",
    "[610]\tvalid's auc: 0.985255\n",
    "[620]\tvalid's auc: 0.985256\n",
    "[630]\tvalid's auc: 0.985261\n",
    "[640]\tvalid's auc: 0.985263\n",
    "[650]\tvalid's auc: 0.985263\n",
    "[660]\tvalid's auc: 0.985261\n",
    "[670]\tvalid's auc: 0.985261\n",
    "[680]\tvalid's auc: 0.985266\n",
    "[690]\tvalid's auc: 0.985266\n",
    "[700]\tvalid's auc: 0.985267\n",
    "[710]\tvalid's auc: 0.98527\n",
    "[720]\tvalid's auc: 0.985273\n",
    "[730]\tvalid's auc: 0.985276\n",
    "[740]\tvalid's auc: 0.985274\n",
    "[750]\tvalid's auc: 0.985276\n",
    "[760]\tvalid's auc: 0.985277\n",
    "[770]\tvalid's auc: 0.985275\n",
    "[780]\tvalid's auc: 0.985273\n",
    "[790]\tvalid's auc: 0.98528\n",
    "[800]\tvalid's auc: 0.98528\n",
    "[810]\tvalid's auc: 0.985281\n",
    "[820]\tvalid's auc: 0.985276\n",
    "[830]\tvalid's auc: 0.985275\n",
    "[840]\tvalid's auc: 0.985283\n",
    "[850]\tvalid's auc: 0.985283\n",
    "[860]\tvalid's auc: 0.985279\n",
    "[870]\tvalid's auc: 0.98528\n",
    "[880]\tvalid's auc: 0.985285\n",
    "[890]\tvalid's auc: 0.98529\n",
    "[900]\tvalid's auc: 0.985288\n",
    "[910]\tvalid's auc: 0.985285\n",
    "[920]\tvalid's auc: 0.985283\n",
    "[930]\tvalid's auc: 0.985285\n",
    "[940]\tvalid's auc: 0.985283\n",
    "[950]\tvalid's auc: 0.985273\n",
    "[960]\tvalid's auc: 0.985272\n",
    "[970]\tvalid's auc: 0.985273\n",
    "[980]\tvalid's auc: 0.985279\n",
    "Early stopping, best iteration is:\n",
    "[889]\tvalid's auc: 0.98529\n",
    "importance (count)\n",
    "7662 \t cat_channel\n",
    "6053 \t cat_dayhourcount_ip\n",
    "4186 \t cat_os\n",
    "4076 \t cat_hour\n",
    "4043 \t cat_app\n",
    "2305 \t nextClickLeakDay\n",
    "1156 \t cumratio_ip_day\n",
    "1138 \t recumcount_app_device_os_day\n",
    "1109 \t var_ip_device_hour\n",
    "1070 \t uniqueCount_day_ip_machine\n",
    "1012 \t count_ip_device_os_day_hourminute10\n",
    "1007 \t uniqueCount_day_ip_os\n",
    "979 \t uniqueCount_day_ip_app\n",
    "949 \t count_ip_os\n",
    "935 \t dayhourminute10count_ip_device_os\n",
    "891 \t count_ip\n",
    "890 \t nextNextClickLeakDay\n",
    "832 \t cumcount_ip_day\n",
    "761 \t count_ip_app_device_day\n",
    "746 \t count_app_day_hourminute\n",
    "717 \t WOEBnd_app_os_nextClickLeakDayFlt\n",
    "715 \t count_device_os_day_hourminute10\n",
    "689 \t uniqueCountRatio_day_ip_channel\n",
    "681 \t countRatio_ip_channel\n",
    "652 \t uniqueCount_day_ip_device\n",
    "648 \t countRatio_ip_machine\n",
    "626 \t WOEBnd_app_channel_nextClickLeakDayFlt\n",
    "618 \t count_app_device_channel_day_hour\n",
    "617 \t WOEBnd_ip\n",
    "609 \t WOEBnd_ip_device\n",
    "598 \t dayhourminute10count_ip_channel\n",
    "586 \t WOEBnd_os_channel_nextClickLeakDayFlt\n",
    "578 \t count_app_device_day_hour\n",
    "572 \t count_app_os_channel_day_hour\n",
    "564 \t countRatio_machine_ip\n",
    "558 \t WOEBnd_app_nextClickLeakDayFlt\n",
    "558 \t WOEBnd_ip_device_nextClickLeakDayFlt\n",
    "536 \t WOEBnd_ip_nextClickLeakDayFlt\n",
    "516 \t count_ip_app_device_os_day_hour\n",
    "501 \t WOEBnd_app_os_channel\n",
    "475 \t WOEBnd_app_channel\n",
    "475 \t WOEBnd_ip_app_device\n",
    "470 \t WOEBnd_app_os\n",
    "470 \t count_ip_app_device_channel_day\n",
    "458 \t WOEBnd_os_nextClickLeakDayFlt\n",
    "457 \t WOEBnd_app\n",
    "454 \t count_ip_app_os_channel\n",
    "452 \t WOEBnd_channel_nextClickLeakDayFlt\n",
    "440 \t WOEBnd_app_device_channel\n",
    "434 \t WOEBnd_app_device_os_channel\n",
    "416 \t WOEBnd_app_device_os\n",
    "415 \t WOEBnd_app_device_nextClickLeakDayFlt\n",
    "407 \t WOEBnd_channel\n",
    "402 \t WOEBnd_device_os_nextClickLeakDayFlt\n",
    "396 \t WOEBnd_ip_app\n",
    "385 \t count_ip_app_os_channel_day\n",
    "373 \t WOEBnd_ip_app_nextClickLeakDayFlt\n",
    "363 \t WOEBnd_ip_os_nextClickLeakDayFlt\n",
    "356 \t WOEBnd_app_device\n",
    "356 \t countRatio_app_channel\n",
    "332 \t WOEBnd_ip_os\n",
    "318 \t WOEBnd_ip_device_os\n",
    "313 \t dayhourminute10count_app_os_channel\n",
    "294 \t WOEBnd_device_channel_nextClickLeakDayFlt\n",
    "283 \t WOEBnd_os\n",
    "246 \t cat_device\n",
    "234 \t WOEBnd_device_nextClickLeakDayFlt\n",
    "227 \t countRatio_channel_app\n",
    "185 \t uniqueCount_machine_ip\n",
    "184 \t WOEBnd_ip_app_device_os\n",
    "171 \t WOEBnd_ip_app_channel\n",
    "170 \t uniqueCount_machine_app\n",
    "166 \t WOEBnd_ip_app_device_channel\n",
    "159 \t WOEBnd_ip_app_os\n",
    "156 \t uniqueCount_machine_channel\n",
    "147 \t WOEBnd_device\n",
    "144 \t WOEBnd_ip_channel_nextClickLeakDayFlt\n",
    "126 \t WOEBnd_ip_channel\n",
    "115 \t WOEBnd_ip_device_channel\n",
    "101 \t WOEBnd_ip_device_os_channel\n",
    "86 \t WOEBnd_ip_os_channel\n",
    "73 \t WOEBnd_ip_app_os_channel\n",
    "20 \t cat_com1_ip\n",
    "importance (gain)\n",
    "310909115.1367215 \t WOEBnd_app_channel_nextClickLeakDayFlt\n",
    "116189675.08218685 \t WOEBnd_app_channel\n",
    "74585079.33311379 \t WOEBnd_app_device_channel\n",
    "65346948.51699972 \t WOEBnd_app_os_nextClickLeakDayFlt\n",
    "52930113.69488859 \t WOEBnd_app_os_channel\n",
    "34738707.1083705 \t WOEBnd_app_nextClickLeakDayFlt\n",
    "13248923.494836092 \t WOEBnd_app_device_nextClickLeakDayFlt\n",
    "11820401.508634567 \t cat_channel\n",
    "10859121.123201381 \t uniqueCount_day_ip_app\n",
    "10806922.325395584 \t nextClickLeakDay\n",
    "8295971.22974968 \t cat_app\n",
    "8119549.615546957 \t WOEBnd_os_channel_nextClickLeakDayFlt\n",
    "7742286.443136215 \t cat_dayhourcount_ip\n",
    "6167366.172456443 \t count_ip\n",
    "5279370.588954449 \t cat_hour\n",
    "4964512.492777765 \t WOEBnd_app_device_os_channel\n",
    "4775688.893976212 \t cat_os\n",
    "3952056.84449701 \t uniqueCount_day_ip_machine\n",
    "3902632.14283338 \t uniqueCount_day_ip_os\n",
    "2870196.691186905 \t WOEBnd_app_device_os\n",
    "2601322.2771796584 \t WOEBnd_app_os\n",
    "2512163.12083447 \t count_ip_os\n",
    "2142401.274064959 \t WOEBnd_app\n",
    "2080720.4628853558 \t cumcount_ip_day\n",
    "2012502.033362314 \t dayhourminute10count_ip_device_os\n",
    "1969777.27425766 \t WOEBnd_ip_app_device\n",
    "1958352.8883017013 \t cumratio_ip_day\n",
    "1777085.3216042519 \t WOEBnd_ip_app_nextClickLeakDayFlt\n",
    "1757405.878881462 \t recumcount_app_device_os_day\n",
    "1720464.7791444212 \t uniqueCount_day_ip_device\n",
    "1592370.3896488547 \t count_ip_device_os_day_hourminute10\n",
    "1310657.6935391873 \t WOEBnd_device_os_nextClickLeakDayFlt\n",
    "1182321.2738022804 \t WOEBnd_device_channel_nextClickLeakDayFlt\n",
    "1153360.1602563858 \t nextNextClickLeakDay\n",
    "1121229.8367853165 \t WOEBnd_app_device\n",
    "1077885.5237140656 \t var_ip_device_hour\n",
    "1032646.6714598294 \t WOEBnd_channel_nextClickLeakDayFlt\n",
    "991875.3891591795 \t WOEBnd_ip_device_nextClickLeakDayFlt\n",
    "948495.0860916078 \t uniqueCountRatio_day_ip_channel\n",
    "906906.5093252361 \t countRatio_machine_ip\n",
    "864391.4197893143 \t WOEBnd_ip_app\n",
    "861099.666806221 \t uniqueCount_machine_ip\n",
    "833559.8858332038 \t count_device_os_day_hourminute10\n",
    "770556.3390529752 \t WOEBnd_os_nextClickLeakDayFlt\n",
    "724418.9525639177 \t countRatio_ip_machine\n",
    "634177.162967205 \t count_ip_app_device_day\n",
    "613470.6909191906 \t WOEBnd_ip_nextClickLeakDayFlt\n",
    "511534.1028394699 \t count_ip_app_device_channel_day\n",
    "507560.0583263412 \t countRatio_ip_channel\n",
    "500883.5614546463 \t WOEBnd_ip_os_nextClickLeakDayFlt\n",
    "487522.6605615616 \t WOEBnd_ip_device\n",
    "483755.38690439984 \t count_app_os_channel_day_hour\n",
    "467635.90353414044 \t count_app_device_channel_day_hour\n",
    "463408.1527768709 \t count_app_device_day_hour\n",
    "452831.9241409302 \t cat_device\n",
    "450700.4908838272 \t uniqueCount_machine_app\n",
    "441651.1567156911 \t WOEBnd_ip\n",
    "431864.92791201174 \t WOEBnd_device_nextClickLeakDayFlt\n",
    "406568.6708442494 \t dayhourminute10count_ip_channel\n",
    "374913.27003860474 \t WOEBnd_ip_app_device_os\n",
    "370983.0273208618 \t count_app_day_hourminute\n",
    "354809.19077014923 \t WOEBnd_ip_device_os\n",
    "354620.4433208108 \t count_ip_app_device_os_day_hour\n",
    "339179.5741372297 \t WOEBnd_channel\n",
    "321153.5785684213 \t countRatio_app_channel\n",
    "304542.94038391113 \t WOEBnd_ip_os\n",
    "287665.46991825104 \t count_ip_app_os_channel\n",
    "256743.59909152985 \t count_ip_app_os_channel_day\n",
    "252504.0149960518 \t uniqueCount_machine_channel\n",
    "252411.30738449097 \t WOEBnd_ip_app_os\n",
    "240897.25969219208 \t WOEBnd_ip_channel_nextClickLeakDayFlt\n",
    "239004.0766961202 \t dayhourminute10count_app_os_channel\n",
    "209945.51604175568 \t WOEBnd_ip_app_device_channel\n",
    "157689.6334280548 \t WOEBnd_os\n",
    "154800.7994966507 \t countRatio_channel_app\n",
    "149291.7297153473 \t WOEBnd_ip_app_channel\n",
    "92971.41224992275 \t WOEBnd_ip_channel\n",
    "92793.21971416473 \t WOEBnd_device\n",
    "80946.17392452061 \t WOEBnd_ip_device_channel\n",
    "34718.61844649911 \t WOEBnd_ip_device_os_channel\n",
    "24498.98580223322 \t WOEBnd_ip_os_channel\n",
    "20669.156454086304 \t WOEBnd_ip_app_os_channel\n",
    "5074.541618347168 \t cat_com1_ip\n",
    "validation auc: 0.9852901220779562"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
