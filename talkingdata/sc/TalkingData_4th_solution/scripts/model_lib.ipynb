{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utils methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:51:19.014273Z",
     "start_time": "2018-05-30T17:51:10.377071Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# this script makes it possible to describe a model in one argument.\n",
    "\n",
    "lgb_target = 'model=LGBM_feat=lgbmBest_categoricalThreVal=10000_validation=subm_params=-,gbdt,0.45,0.04,188,7,auc,20,5,0,70,76,binary,0,0,32.0,1.0,200000,1,0'\n",
    "keras_target = 'BatchNormalization=on_sameNDenseAsEmb=off_model=keras_feat=kerasBest_validation=team_params=-,20000,1000,1,0.2,100,2,0.001,0.0001,0.001,100,2,3'\n",
    "target = keras_target#''\n",
    "\n",
    "def get_opt(name,default=None):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    global target\n",
    "    if target == '':\n",
    "        target = get_target()\n",
    "    if target == '':\n",
    "        return default\n",
    "    flds = target.replace('__','\u0001').split('_')\n",
    "    for fld in flds:\n",
    "        if fld == '':\n",
    "            continue\n",
    "        key, val = fld.split('=')\n",
    "        if key == name:\n",
    "            if isinstance(default, int):\n",
    "                val = int(val)\n",
    "            elif isinstance(default, float):\n",
    "                val = float(val)\n",
    "            else:\n",
    "                val = val.replace('\u0001','_')\n",
    "            return val\n",
    "    return default\n",
    "\n",
    "def get_target():\n",
    "    global target\n",
    "    if target != '':\n",
    "        return target\n",
    "    if len(sys.argv) > 1:\n",
    "        target = sys.argv[1]\n",
    "    else:\n",
    "        target = default_target\n",
    "    return target\n",
    "\n",
    "def reset_target():\n",
    "    global target\n",
    "    target = ''\n",
    "    a = get_target()\n",
    "    return get_target()\n",
    "\n",
    "def set_target(tgt):\n",
    "    global target\n",
    "    target = tgt\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "import os\n",
    "# from lib_util import get_target,get_opt\n",
    "import lightgbm as lgb\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Dropout, concatenate, BatchNormalization, SpatialDropout1D\n",
    "from keras.callbacks import Callback\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import gc\n",
    "\n",
    "def get_params(params_str):\n",
    "    if get_opt('model') == 'keras':\n",
    "        names = ['batch_size', 'dense_cate', 'dense_nume_n_layers', 'drop', 'emb_cate', 'epochs_for_lr', 'lr', 'lr_fin', 'lr_init', 'max_epochs', 'n_layers', 'patience']\n",
    "    elif 'LGBM' in get_opt('model'):\n",
    "        names = ['boosting_type','colsample_bytree','learning_rate','max_bin','max_depth','metric','min_child_samples','min_child_weight','min_split_gain','nthread','num_leaves','objective','reg_alpha','reg_lambda','scale_pos_weight','subsample','subsample_for_bin','subsample_freq','verbose']\n",
    "    else:\n",
    "        print(\"no valid target\")\n",
    "        sys.exit(1)\n",
    "    pvals = params_str.split(',')\n",
    "    del pvals[0]\n",
    "    if len(pvals) != len(names):\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!ERR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print('params: count is not fit',len(pvals), len(names))\n",
    "        print('params_str:',params_str)\n",
    "        print('names:',names)\n",
    "        print('param_values:',pvals)\n",
    "        sys.exit()\n",
    "    params = dict(zip(names, pvals))\n",
    "    return params\n",
    "\n",
    "def LGBM(X_tr,X_va,X_te,predictors,cat_feats,seed=2018):\n",
    "    params_str = get_opt('params')\n",
    "    if params_str != None:\n",
    "        params = get_params(params_str)\n",
    "        return LGBM_helper(X_tr,X_va,X_te,predictors,cat_feats,params,seed=2018)\n",
    "\n",
    "def Keras(X_tr,X_va,X_te,predictors,cat_feats,seed=2018):\n",
    "    params_str = get_opt('params')\n",
    "    if params_str != None:\n",
    "        params = get_params(params_str)\n",
    "        return Keras0_helper(X_tr,X_va,X_te,predictors,cat_feats,params,seed=2018)\n",
    "\n",
    "def LGBM_helper(_X_tr,_X_va,_X_te,predictors,cat_feats,params,seed=2018):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    X_tr = _X_tr[predictors]\n",
    "    X_va = _X_va[predictors]\n",
    "    X_te = _X_te[predictors]\n",
    "    y_tr = _X_tr['is_attributed']\n",
    "    y_va = _X_va['is_attributed']\n",
    "    y_te = _X_te['is_attributed']\n",
    "    params['feature_fraction_seed'] = seed\n",
    "    params['bagging_seed'] = seed\n",
    "    params['drop_seed'] = seed\n",
    "    params['data_random_seed'] = seed\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['subsample_for_bin'] = int(params['subsample_for_bin'])\n",
    "    params['max_depth'] = int(np.log2(params['num_leaves'])+1.2)\n",
    "    params['max_bin'] = int(params['max_bin'])\n",
    "    print('*'*50)\n",
    "    for k,v in sorted(params.items()):\n",
    "        print(k,':',v)\n",
    "    columns = X_tr.columns\n",
    "\n",
    "    print('start for lgvalid')\n",
    "    lgvalid = lgb.Dataset(X_va, label=y_va, categorical_feature=cat_feats)\n",
    "    _X_va.drop(predictors,axis=1)\n",
    "    del _X_va, X_va, y_va\n",
    "    gc.collect()\n",
    "\n",
    "    print('start for lgtrain')\n",
    "    lgtrain = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_feats)\n",
    "    _X_te.drop(predictors,axis=1)\n",
    "    del _X_tr, X_tr, y_tr\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "    if get_opt('trainCheck','-') == 'on':\n",
    "         valid_names=['train','valid']\n",
    "         valid_sets=[lgtrain, lgvalid]\n",
    "    else:\n",
    "         valid_names=['valid']\n",
    "         valid_sets=[lgvalid]\n",
    "    if get_opt('testCheck','-') == 'on':\n",
    "         valid_names.append('test')\n",
    "         lgtest = lgb.Dataset(X_te, label=y_te, categorical_feature=cat_feats)\n",
    "         valid_sets.append(lgtest)\n",
    "\n",
    "    print('start training')\n",
    "    bst = lgb.train(params,\n",
    "                     lgtrain,\n",
    "                     valid_sets=valid_sets,\n",
    "                     valid_names=valid_names,\n",
    "                     evals_result=evals_results,\n",
    "                     num_boost_round=2000,\n",
    "                     early_stopping_rounds=100,\n",
    "                     verbose_eval=10,\n",
    "                     )\n",
    "\n",
    "    importance = bst.feature_importance()\n",
    "    print('importance (count)')\n",
    "    tuples = sorted(zip(columns, importance), key=lambda x: x[1],reverse=True)\n",
    "    for col, val in tuples:\n",
    "        print(val,\"\\t\",col)\n",
    "\n",
    "    importance = bst.feature_importance(importance_type='gain')\n",
    "    print('importance (gain)')\n",
    "    tuples = sorted(zip(columns, importance), key=lambda x: x[1],reverse=True)\n",
    "    for col, val in tuples:\n",
    "        print(val,\"\\t\",col)\n",
    "\n",
    "    n_estimators = bst.best_iteration\n",
    "    metric = params['metric']\n",
    "    auc = evals_results['valid'][metric][n_estimators-1]\n",
    "    _X_te['pred'] = bst.predict(X_te)\n",
    "\n",
    "    return auc\n",
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(self,training_data=False,validation_data=False, testing_data=False, min_delta=0, patience=0, model_file=None, verbose=0):\n",
    "        super(EarlyStopping, self).__init__()\n",
    "        self.best_epoch = 0\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.monitor_op = np.greater\n",
    "        if training_data:\n",
    "            self.x_tr = training_data[0]\n",
    "            self.y_tr = training_data[1]\n",
    "        else:\n",
    "            self.x_tr = False\n",
    "            self.y_tr = False\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        if testing_data:\n",
    "            self.x_te = testing_data[0]\n",
    "            self.y_te = testing_data[1]\n",
    "        else:\n",
    "            self.x_te = False\n",
    "            self.y_te = False\n",
    "        self.model_file = model_file\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.wait = 0\n",
    "        self.best_epoch = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = -np.Inf\n",
    "    def on_train_end(self, logs={}):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print('Epoch ',self.best_epoch,': EarlyStopping')\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        if self.x_tr:\n",
    "            y_pred = self.model.predict(self.x_tr,batch_size=100000)\n",
    "            roc_tr = roc_auc_score(self.y_tr, y_pred)\n",
    "        else:\n",
    "            roc_tr = 0\n",
    "\n",
    "        y_hat_val=self.model.predict(self.x_val,batch_size=100000)\n",
    "        roc_val = roc_auc_score(self.y_val, y_hat_val)\n",
    "\n",
    "        if self.x_te:\n",
    "            y_hat_te=self.model.predict(self.x_te,batch_size=100000)\n",
    "            roc_te = roc_auc_score(self.y_te, y_hat_te)\n",
    "        else:\n",
    "            roc_te = 0\n",
    "        print('roc-auc: %s - roc-auc_val: %s - roc-auc_test: %s' % (str(round(roc_tr,6)),str(round(roc_val,6)), str(round(roc_te,6))),end=100*' '+'\\n')\n",
    "\n",
    "        if self.model_file:\n",
    "            print(\"saving\",self.model_file+'.'+str(epoch))\n",
    "            self.model.save_weights(self.model_file+'.'+str(epoch))\n",
    "        if(self.x_val):\n",
    "            if get_opt('testCheck','-') == 'on': \n",
    "                current = roc_te\n",
    "            else:\n",
    "                current = roc_val\n",
    "            if self.monitor_op(current - self.min_delta, self.best):\n",
    "                self.best = current\n",
    "                self.best_epoch = epoch\n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stopped_epoch = epoch\n",
    "                    self.model.stop_training = True\n",
    "\n",
    "def Keras0_helper(_X_tr,_X_va,_X_te,predictors,cat_feats,params,seed=2018):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    X_tr = _X_tr[predictors]\n",
    "    X_va = _X_va[predictors]\n",
    "    X_te = _X_te[predictors]\n",
    "    y_tr = _X_tr['is_attributed']\n",
    "    y_va = _X_va['is_attributed']\n",
    "    y_te = _X_te['is_attributed']\n",
    "    print('*************params**************')\n",
    "    for f in sorted(params): print(f+\":\",params[f])\n",
    "    batch_size = int(params['batch_size'])\n",
    "    epochs_for_lr = float(params['epochs_for_lr'])\n",
    "    max_epochs = int(params['max_epochs'])\n",
    "    emb_cate = int(params['emb_cate'])\n",
    "    dense_cate = int(params['dense_cate'])\n",
    "    dense_nume_n_layers = int(params['dense_nume_n_layers'])\n",
    "    drop = float(params['drop'])\n",
    "    lr= float(params['lr'])\n",
    "    lr_init = float(params['lr_init'])\n",
    "    lr_fin = float(params['lr_fin'])\n",
    "    n_layers = int(params['n_layers'])\n",
    "    patience = int(params['patience'])\n",
    "    train_dict = {}\n",
    "    valid_dict = {}\n",
    "    test_dict = {}\n",
    "    input_list = []\n",
    "    emb_list = []\n",
    "    numerical_feats = []\n",
    "    tot_emb_n = 0\n",
    "    for col in X_tr:\n",
    "        if col not in cat_feats:\n",
    "            numerical_feats.append(col)\n",
    "    if len(cat_feats) > 0:\n",
    "        for col in cat_feats:\n",
    "            train_dict[col] = np.array(X_tr[col])\n",
    "            valid_dict[col] = np.array(X_va[col])\n",
    "            test_dict[col] = np.array(X_te[col])\n",
    "            inpt = Input(shape=[1], name = col)\n",
    "            input_list.append(inpt)\n",
    "            max_val = np.max([X_tr[col].max(), X_va[col].max(), X_te[col].max()])+1\n",
    "            emb_n = np.min([emb_cate, max_val])\n",
    "            if get_opt('fixEmb','on') == 'on':\n",
    "                emb_n = emb_cate\n",
    "            tot_emb_n += emb_n\n",
    "            if emb_n == 1:\n",
    "                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Warinig!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! emb_1 = 1\")\n",
    "                return 0\n",
    "            print('Embedding size:',max_val, emb_cate, X_tr[col].max(), X_va[col].max(), X_te[col].max(), emb_n,col)\n",
    "            embd = Embedding(max_val, emb_n)(inpt)\n",
    "            emb_list.append(embd)\n",
    "        if len(emb_list) == 1:\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Warinig!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! emb_list = 1\")\n",
    "            return 0\n",
    "        fe = concatenate(emb_list)\n",
    "        s_dout = SpatialDropout1D(drop)(fe)\n",
    "        x1 = Flatten()(s_dout)\n",
    "\n",
    "    if get_opt('sameNDenseAsEmb','-') == 'on':\n",
    "        dense_cate = tot_emb_n\n",
    "    if len(numerical_feats) > 0:\n",
    "        train_dict['numerical'] = X_tr[numerical_feats].values\n",
    "        valid_dict['numerical'] = X_va[numerical_feats].values\n",
    "        test_dict['numerical'] = X_te[numerical_feats].values\n",
    "        inpt = Input((len(numerical_feats),),name='numerical')\n",
    "        input_list.append(inpt)\n",
    "        x2 = inpt\n",
    "        for n in range(dense_nume_n_layers):\n",
    "            x2 = Dense(dense_cate,activation='relu',kernel_initializer=RandomUniform(seed=seed))(x2)\n",
    "            if get_opt('numeDropout','on') != 'off':\n",
    "                x2 = Dropout(drop)(x2)\n",
    "            if get_opt('NumeBatchNormalization','on') != 'off':\n",
    "                x2 = BatchNormalization()(x2)\n",
    "\n",
    "    if len(numerical_feats) > 0 and len(cat_feats) > 0:\n",
    "        x = concatenate([x1, x2])\n",
    "    elif len(numerical_feats) > 0:\n",
    "        x =  x2\n",
    "    elif len(cat_feats) > 0:\n",
    "        x =  x1\n",
    "    else:\n",
    "        return 0 # for small data test\n",
    "\n",
    "    for n in range(n_layers):\n",
    "        x = Dense(dense_cate,activation='relu',kernel_initializer=RandomUniform(seed=seed))(x)\n",
    "        if get_opt('lastDropout','on') != 'off':\n",
    "            x = Dropout(drop)(x)\n",
    "        if get_opt('BatchNormalization','off') == 'on' or get_opt('LastBatchNormalization','off') == 'on':\n",
    "            x = BatchNormalization()(x)\n",
    "    outp = Dense(1,activation='sigmoid',kernel_initializer=RandomUniform(seed=seed))(x)\n",
    "    model = Model(inputs=input_list, outputs=outp)\n",
    "    if get_opt('optimizer','expo') == 'adam':\n",
    "        optimizer = Adam(lr=lr)\n",
    "    elif get_opt('optimizer','expo') == 'nadam':\n",
    "        optimizer = Nadam(lr=lr)\n",
    "    else:\n",
    "        exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "        steps = int(len(X_tr) / batch_size) * epochs_for_lr\n",
    "        lr_init, lr_fin = 0.001, 0.0001\n",
    "        lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "        optimizer = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "    model.summary()\n",
    "    #from keras.utils import plot_model\n",
    "    #plot_model(model, to_file='model.png')\n",
    "\n",
    "    model_file = '../work/weights.'+str(os.getpid())+'.hdf5'\n",
    "    if get_opt('trainCheck','-') == 'on': \n",
    "        training_data=(train_dict, y_tr)\n",
    "    else:\n",
    "        training_data=False\n",
    "    if get_opt('testCheck','-') == 'on':\n",
    "        testing_data=(test_dict, y_te)\n",
    "    else:\n",
    "        testing_data=False\n",
    "    aucEarlyStopping = EarlyStopping(\n",
    "        training_data=training_data,\n",
    "        validation_data=(valid_dict,y_va),\n",
    "        testing_data=testing_data,\n",
    "        patience=patience,\n",
    "        model_file=model_file,\n",
    "        verbose=1)\n",
    "    model.fit(train_dict,\n",
    "        y_tr,\n",
    "        validation_data=[valid_dict, y_va],\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        callbacks=[aucEarlyStopping])\n",
    "    best_epoch = aucEarlyStopping.best_epoch\n",
    "    print('loading',model_file+'.'+str(best_epoch))\n",
    "    model.load_weights(model_file+'.'+str(best_epoch))\n",
    "    _X_te['pred'] = model.predict(test_dict, batch_size=batch_size, verbose=1)[:,0]\n",
    "    _X_va['pred'] = model.predict(valid_dict, batch_size=batch_size, verbose=1)[:,0]\n",
    "    if get_opt('avgEpoch',0) > 0:\n",
    "        added = 1\n",
    "        for i in range(min(get_opt('avgEpoch',0),patience)):\n",
    "            best_epoch = aucEarlyStopping.best_epoch + (i+1)\n",
    "            if best_epoch >= max_epochs:\n",
    "                continue\n",
    "            print('loading',model_file+'.'+str(best_epoch))\n",
    "            model.load_weights(model_file+'.'+str(best_epoch))\n",
    "            _X_te['pred'] += model.predict(test_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            _X_va['pred'] += model.predict(valid_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            added += 0.5\n",
    "            best_epoch = aucEarlyStopping.best_epoch - (i+1)\n",
    "            if best_epoch < 0:\n",
    "                continue\n",
    "            print('loading',model_file+'.'+str(best_epoch))\n",
    "            model.load_weights(model_file+'.'+str(best_epoch))\n",
    "            _X_te['pred'] += model.predict(test_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            _X_va['pred'] += model.predict(valid_dict, batch_size=batch_size, verbose=1)[:,0]*0.5\n",
    "            added += 0.5\n",
    "        _X_te['pred'] /= added\n",
    "        _X_va['pred'] /= added\n",
    "\n",
    "    os.system('rm -f '+model_file+'.*')\n",
    "    auc = roc_auc_score(y_va, _X_va.pred)\n",
    "    return auc\n",
    "\n",
    "def Predict(X_tr,X_va,X_te,predictors,cat_feats,seed=2018):\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    model = get_opt('model')\n",
    "    if 'LGBM' in model:\n",
    "        return LGBM(X_tr,X_va,X_te,predictors,cat_feats,seed=2018)\n",
    "    elif 'keras' in model:\n",
    "        return Keras(X_tr,X_va,X_te,predictors,cat_feats,seed=2018)\n",
    "    else:\n",
    "        print(\"no valid model\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "#-*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.special import logit\n",
    "# from lib_util import get_target,get_opt,set_target,reset_target\n",
    "import shutil\n",
    "import pdb\n",
    "\n",
    "target=get_target()\n",
    "nrows=get_opt('nrows',-1)\n",
    "if nrows == -1:\n",
    "    nrows=None\n",
    "path = '../input/'\n",
    "work = '../work/'\n",
    "csv_dir='../csv/'\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        }\n",
    "\n",
    "# wrapper of pd.read_csv with cache\n",
    "def read_csv(csv_file,df_len=None,nrows=None,usecols=None,dtype=None):\n",
    "    pkl_file = csv_file[:-4] + '.pkl'\n",
    "    if os.path.isfile(pkl_file) and nrows == None:\n",
    "        with open(pkl_file, 'rb') as pk:\n",
    "            print(\"loading\",pkl_file)\n",
    "            df = pickle.load(pk)\n",
    "        if df_len != None and len(df) != df_len:\n",
    "            print('ERROR!!!!!!!!!!!!!!!!!!!!!!!',pkl_file,'is broken',len(df),df_len)\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"loading\",csv_file)\n",
    "        df = pd.read_csv(csv_file, nrows=nrows)\n",
    "        if 'next' in csv_file:\n",
    "            df = np.absolute(df)\n",
    "        for ptn in df:\n",
    "            if dtype:\n",
    "                df = df.astype(dtype)\n",
    "            else:\n",
    "                df[ptn] = df[ptn].astype(get_type_with_fld_check(df,ptn))\n",
    "        if nrows == None and (df_len == None or len(df) == df_len):\n",
    "            print(\"saving cache file\",pkl_file)\n",
    "            with open(pkl_file+str(os.getpid()), 'wb') as pk:\n",
    "                pickle.dump(df,pk,protocol=4)\n",
    "            shutil.move(pkl_file+str(os.getpid()), pkl_file)\n",
    "        if nrows == None and df_len and len(df) != df_len:\n",
    "            print('ERROR!!!!!!!!!!!!!!!!!!!!!!!',csv_file,'is broken')\n",
    "            sys.exit(1)\n",
    "    if usecols != None:\n",
    "        df = df[usecols]\n",
    "    if nrows != None:\n",
    "        df = df[:nrows]\n",
    "    gc.collect()\n",
    "    if df_len != None and len(df) != df_len:\n",
    "        print('ERROR!!!!!!!!!!!!!!!!!!!!!!!',csv_file,'line is not same',df_len,len(df))\n",
    "        sys.exit(1)\n",
    "    return df\n",
    "\n",
    "def get_type_with_fld_check(df,ptn):\n",
    "    max_val = df[ptn].max()\n",
    "    if  'cumratio' in ptn or 'mean_' in ptn or 'Ratio' in ptn or 'CVR' in ptn or 'WOE' in ptn:\n",
    "        dtype = 'float16'\n",
    "    else:\n",
    "        if max_val < 256:\n",
    "            dtype = 'uint8'\n",
    "        elif max_val < 65536:\n",
    "            dtype = 'uint16'\n",
    "        else:\n",
    "            dtype = 'uint32'\n",
    "    return dtype\n",
    "\n",
    "def get_type(df,ptn):\n",
    "    max_val = df[ptn].max()\n",
    "    if max_val < 256:\n",
    "        dtype = 'uint8'\n",
    "    elif max_val < 65536:\n",
    "        dtype = 'uint16'\n",
    "    else:\n",
    "        dtype = 'uint32'\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def read_data_ph1():\n",
    "    keep_patterns = []\n",
    "    feat_opt = get_opt('feat','none')\n",
    "\n",
    "    if 'lgbmBest' == feat_opt:\n",
    "        numerical_patterns = ['WOEBnd_ip_nextClickLeakDayFlt', 'WOEBnd_app_nextClickLeakDayFlt', 'WOEBnd_device_nextClickLeakDayFlt', 'WOEBnd_os_nextClickLeakDayFlt', 'WOEBnd_channel_nextClickLeakDayFlt', 'WOEBnd_ip_app_nextClickLeakDayFlt', 'WOEBnd_ip_device_nextClickLeakDayFlt', 'WOEBnd_ip_os_nextClickLeakDayFlt', 'WOEBnd_ip_channel_nextClickLeakDayFlt', 'WOEBnd_app_device_nextClickLeakDayFlt', 'WOEBnd_app_os_nextClickLeakDayFlt', 'WOEBnd_app_channel_nextClickLeakDayFlt', 'WOEBnd_device_os_nextClickLeakDayFlt', 'WOEBnd_device_channel_nextClickLeakDayFlt', 'WOEBnd_os_channel_nextClickLeakDayFlt', 'WOEBnd_ip', 'WOEBnd_app', 'WOEBnd_device', 'WOEBnd_os', 'WOEBnd_channel', 'WOEBnd_ip_app', 'WOEBnd_ip_device', 'WOEBnd_ip_os', 'WOEBnd_ip_channel', 'WOEBnd_app_device', 'WOEBnd_app_os', 'WOEBnd_app_channel', 'WOEBnd_ip_app_device', 'WOEBnd_ip_app_os', 'WOEBnd_ip_app_channel', 'WOEBnd_ip_device_os', 'WOEBnd_ip_device_channel', 'WOEBnd_ip_os_channel', 'WOEBnd_app_device_os', 'WOEBnd_app_device_channel', 'WOEBnd_app_os_channel', 'WOEBnd_ip_app_device_os', 'WOEBnd_ip_app_device_channel', 'WOEBnd_ip_app_os_channel', 'WOEBnd_ip_device_os_channel', 'WOEBnd_app_device_os_channel', 'countRatio_ip_machine', 'countRatio_ip_channel', 'countRatio_machine_ip', 'countRatio_app_channel', 'countRatio_channel_app', 'uniqueCount_day_ip_os', 'uniqueCount_day_ip_device', 'uniqueCountRatio_day_ip_channel', 'uniqueCount_day_ip_machine', 'uniqueCount_day_ip_app',  'uniqueCount_machine_app', 'uniqueCount_machine_channel', 'uniqueCount_machine_ip', 'nextClickLeakDay', 'nextNextClickLeakDay', 'dayhourminute10count_ip_device_os', 'dayhourminute10count_ip_channel', 'dayhourminute10count_app_os_channel', 'cumratio_ip_day', 'cumcount_ip_day', 'count_ip_os', 'count_ip_device_os_day_hourminute10', 'count_ip_app_os_channel_day', 'count_ip_app_os_channel', 'count_ip_app_device_os_day_hour', 'count_ip_app_device_day', 'count_ip_app_device_channel_day', 'count_ip', 'count_device_os_day_hourminute10', 'count_app_os_channel_day_hour', 'count_app_device_day_hour', 'count_app_device_channel_day_hour', 'recumcount_app_device_os_day', 'var_ip_device_hour', 'count_app_day_hourminute']\n",
    "        cat_patterns = ['cat_os', 'cat_hour', 'cat_device', 'cat_dayhourcount_ip', 'cat_com1_ip', 'cat_channel', 'cat_app']\n",
    "    elif 'kerasBest' == feat_opt:\n",
    "        numerical_patterns = ['uniqueCountRatio_day_ip_machine', 'uniqueCountRatio_day_ip_app', 'uniqueCountRatio_day_ip_channel', 'uniqueCount_day_ip_machine', 'uniqueCount_day_ip_app', 'uniqueCount_day_ip_channel', 'uniqueCount_machine_app', 'uniqueCount_machine_channel', 'uniqueCount_machine_ip', 'nextClickLeakDay', 'dayhourcount_ip', 'count_ip', 'count_ip_app_device_os_day_hour', 'count_app_channel', 'cumcount_ip_app_device_os_day_hour', 'count_device_os_day_hourminute10', 'count_app_device_day_hour', 'dayhourminute10count_ip']\n",
    "        cat_patterns = ['cat_nextClickLeakDay', 'cat_nextNextClickLeakDay', 'cat_app', 'cat_device', 'cat_os', 'cat_count_ip', 'cat_count_app_channel', 'cat_hour', 'cat_dayhourcount_ip']\n",
    "    else:\n",
    "        print('ERR: no valid feat !!!!!!!!!!!!!!!!')\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"start reading feature for\",feat_opt)\n",
    "\n",
    "    # all cache\n",
    "    tgt = 'model=' + get_opt('model','none')\n",
    "    tgt += '_nrows=' + get_opt('nrows','0') \n",
    "    tgt += '_feat=' + get_opt('feat','0') \n",
    "    tgt += '_categoricalThreVal=' + get_opt('categoricalThreVal','1000') \n",
    "    tgt += '_offlineADD=' + get_opt('offlineADD','off') \n",
    "    tgt += '_sample=' + get_opt('sample','0.0') \n",
    "    tgt += '_noTestSample=' + get_opt('noTestSample','off') \n",
    "    tgt += '_noLogDev=' + get_opt('noLogDev','off') \n",
    "    tgt += '_smallTest=' + get_opt('smallTest','off') \n",
    "    tgt += '_ver=3'\n",
    "    tr_pkl_file = '../work/train_' + tgt + '.pkl'\n",
    "    te_pkl_file = '../work/test_supplement_' + tgt + '.pkl'\n",
    "    if os.path.isfile(tr_pkl_file) == True and os.path.isfile(te_pkl_file) == True:\n",
    "        with open(tr_pkl_file, 'rb') as pk:\n",
    "            print(\"loading\",tr_pkl_file)\n",
    "            train_df = pickle.load(pk)\n",
    "        with open(te_pkl_file, 'rb') as pk:\n",
    "            print(\"loading\",te_pkl_file)\n",
    "            test_df = pickle.load(pk)\n",
    "        gc.collect()\n",
    "        return train_df, test_df, numerical_patterns, cat_patterns\n",
    "\n",
    "    # reading base data\n",
    "    train_df = read_csv(work+\"train_base.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel','day','hour','is_attributed'],nrows=nrows)\n",
    "    test_df = read_csv(work+\"test_supplement_base.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel','day','hour'],nrows=nrows)\n",
    "    test_df['is_attributed'] = 0\n",
    "\n",
    "    # reading numerical data\n",
    "    n = 0\n",
    "    for ptn in numerical_patterns:\n",
    "        n+=1\n",
    "        print('start for',ptn,n,'/',len(numerical_patterns))\n",
    "        if ptn in train_df.columns: continue\n",
    "        train_df[ptn] = read_csv(work + 'train_' + ptn + '.csv', nrows=nrows, df_len=len(train_df))\n",
    "        test_df[ptn] = read_csv(work + 'test_supplement_' + ptn + '.csv', nrows=nrows, df_len=len(test_df))\n",
    "    \n",
    "    #reading categorical data\n",
    "    n = 0\n",
    "    for ptn in cat_patterns:\n",
    "        n+=1\n",
    "        print('start categorical convert for',ptn,n,'/',len(cat_patterns))\n",
    "        if ptn in train_df.columns:\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! warning cat ptn is in train_df.columns !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(ptn,train_df.columns)\n",
    "        org_ptn = ptn[4:]\n",
    "        if org_ptn in train_df.columns:\n",
    "            _train_df = train_df[[org_ptn]]\n",
    "            _test_df = test_df[[org_ptn]]\n",
    "        else:\n",
    "            _train_df = read_csv(work + 'train_' + org_ptn + '.csv', nrows=nrows, df_len=len(train_df))\n",
    "            _test_df = read_csv(work + 'test_supplement_' + org_ptn + '.csv', nrows=nrows, df_len=len(test_df))\n",
    "        _train_df = _train_df.rename(columns={org_ptn: ptn})\n",
    "        _test_df = _test_df.rename(columns={org_ptn: ptn})\n",
    "        \n",
    "        len_train = len(_train_df)\n",
    "        _df = _train_df.append(_test_df)\n",
    "        thre_val = get_opt('categoricalThreVal',1000)\n",
    "        max_val = _df[ptn].max()\n",
    "        if 'cat_device' == ptn and get_opt('noLogDev','-') == 'on':\n",
    "            _df[ptn] = LabelEncoder().fit_transform(_df[ptn])\n",
    "        elif thre_val > 0 and max_val > thre_val:\n",
    "            if 'cumratio' in ptn:\n",
    "                fixed_vals = (10000*df[ptn]).astype('uint16')\n",
    "            else:\n",
    "                fixed_vals = (np.log2(_df[ptn]+1)*thre_val/100).astype('uint16')\n",
    "            _df[ptn] = LabelEncoder().fit_transform(fixed_vals)\n",
    "            print('logged for',ptn,max_val,fixed_vals.max(), _df[ptn].max())\n",
    "        else:\n",
    "            _df[ptn] = LabelEncoder().fit_transform(_df[ptn])\n",
    "        _df[ptn] = _df[ptn].astype(get_type(_df,ptn))\n",
    "\n",
    "        train_df[ptn] = _df[:len_train]\n",
    "        test_df[ptn] = _df[len_train:]\n",
    "        gc.collect()\n",
    "\n",
    "    # numerical data conversion\n",
    "    for ptn in numerical_patterns:\n",
    "        if get_opt('model','-') == 'keras':\n",
    "            print('start for numerical convert',ptn)\n",
    "            all_df = train_df[[ptn]].append(test_df[[ptn]])\n",
    "            if 'cumratio' in ptn or 'CVRTgt' in ptn or 'WOETgt' in ptn:\n",
    "                pass\n",
    "            else:\n",
    "                all_df = np.log2(all_df+1)\n",
    "            all_df = StandardScaler().fit_transform(all_df).astype('float16')\n",
    "            train_df[ptn] = all_df[:len(train_df)]\n",
    "            test_df[ptn] = all_df[len(train_df):]\n",
    "\n",
    "    # saving cache\n",
    "    print(\"saving\",tr_pkl_file)\n",
    "    with open(tr_pkl_file+str(os.getpid()), 'wb') as pk:\n",
    "        pickle.dump(train_df,pk,protocol=4)\n",
    "    shutil.move(tr_pkl_file+str(os.getpid()), tr_pkl_file)\n",
    "    print(\"saving\",te_pkl_file)\n",
    "    with open(te_pkl_file+str(os.getpid()), 'wb') as pk:\n",
    "        pickle.dump(test_df,pk,protocol=4)\n",
    "    shutil.move(te_pkl_file+str(os.getpid()), te_pkl_file)\n",
    "    print('saved cache file')\n",
    "\n",
    "    gc.collect()\n",
    "    return train_df, test_df, numerical_patterns, cat_patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training / prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:33:37.544837Z",
     "start_time": "2018-05-30T17:33:37.540184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start for BatchNormalization=on_sameNDenseAsEmb=off_model=keras_feat=kerasBest_validation=team_params=-,20000,1000,1,0.2,100,2,0.001,0.0001,0.001,100,2,3\n"
     ]
    }
   ],
   "source": [
    "target=get_target()\n",
    "print('start for',target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:35:14.471020Z",
     "start_time": "2018-05-30T17:34:57.782293Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading feature for kerasBest\n",
      "loading ../work/train_model=keras_nrows=0_feat=kerasBest_categoricalThreVal=1000_offlineADD=off_sample=0.0_noTestSample=off_noLogDev=off_smallTest=off_ver=3.pkl\n",
      "loading ../work/test_supplement_model=keras_nrows=0_feat=kerasBest_categoricalThreVal=1000_offlineADD=off_sample=0.0_noTestSample=off_noLogDev=off_smallTest=off_ver=3.pkl\n",
      "CPU times: user 4.73 s, sys: 12 s, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df, test_df, numerical_patterns, cat_patterns = read_data_ph1()\n",
    "predictors = numerical_patterns + cat_patterns\n",
    "categorical = cat_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:38:03.219474Z",
     "start_time": "2018-05-30T17:38:03.216550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uniqueCountRatio_day_ip_machine',\n",
       " 'uniqueCountRatio_day_ip_app',\n",
       " 'uniqueCountRatio_day_ip_channel',\n",
       " 'uniqueCount_day_ip_machine',\n",
       " 'uniqueCount_day_ip_app',\n",
       " 'uniqueCount_day_ip_channel',\n",
       " 'uniqueCount_machine_app',\n",
       " 'uniqueCount_machine_channel',\n",
       " 'uniqueCount_machine_ip',\n",
       " 'nextClickLeakDay',\n",
       " 'dayhourcount_ip',\n",
       " 'count_ip',\n",
       " 'count_ip_app_device_os_day_hour',\n",
       " 'count_app_channel',\n",
       " 'cumcount_ip_app_device_os_day_hour',\n",
       " 'count_device_os_day_hourminute10',\n",
       " 'count_app_device_day_hour',\n",
       " 'dayhourminute10count_ip',\n",
       " 'cat_nextClickLeakDay',\n",
       " 'cat_nextNextClickLeakDay',\n",
       " 'cat_app',\n",
       " 'cat_device',\n",
       " 'cat_os',\n",
       " 'cat_count_ip',\n",
       " 'cat_count_app_channel',\n",
       " 'cat_hour',\n",
       " 'cat_dayhourcount_ip']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:36:38.282367Z",
     "start_time": "2018-05-30T17:36:38.279627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184903890, 35)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:40:48.579558Z",
     "start_time": "2018-05-30T17:40:12.906240Z"
    }
   },
   "outputs": [],
   "source": [
    "is_val = (train_df['day'] == 9) & ((train_df['hour'] == 13) |(train_df['hour'] == 17) |(train_df['hour'] == 21))\n",
    "val_df = train_df[is_val]\n",
    "train_df = train_df[~is_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:27:52.960290Z",
     "start_time": "2018-05-30T17:27:52.943972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174788422, 35)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:28:33.434261Z",
     "start_time": "2018-05-30T17:28:33.431574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10115468, 35)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:31:41.349346Z",
     "start_time": "2018-05-30T17:31:40.825986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ip', 'app', 'device', 'os', 'channel', 'day', 'hour', 'is_attributed',\n",
       "       'uniqueCountRatio_day_ip_machine', 'uniqueCountRatio_day_ip_app',\n",
       "       'uniqueCountRatio_day_ip_channel', 'uniqueCount_day_ip_machine',\n",
       "       'uniqueCount_day_ip_app', 'uniqueCount_day_ip_channel',\n",
       "       'uniqueCount_machine_app', 'uniqueCount_machine_channel',\n",
       "       'uniqueCount_machine_ip', 'nextClickLeakDay', 'dayhourcount_ip',\n",
       "       'count_ip', 'count_ip_app_device_os_day_hour', 'count_app_channel',\n",
       "       'cumcount_ip_app_device_os_day_hour',\n",
       "       'count_device_os_day_hourminute10', 'count_app_device_day_hour',\n",
       "       'dayhourminute10count_ip', 'cat_nextClickLeakDay',\n",
       "       'cat_nextNextClickLeakDay', 'cat_app', 'cat_device', 'cat_os',\n",
       "       'cat_count_ip', 'cat_count_app_channel', 'cat_hour',\n",
       "       'cat_dayhourcount_ip'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-30T17:52:52.572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************params**************\n",
      "batch_size: 20000\n",
      "dense_cate: 1000\n",
      "dense_nume_n_layers: 1\n",
      "drop: 0.2\n",
      "emb_cate: 100\n",
      "epochs_for_lr: 2\n",
      "lr: 0.001\n",
      "lr_fin: 0.0001\n",
      "lr_init: 0.001\n",
      "max_epochs: 100\n",
      "n_layers: 2\n",
      "patience: 3\n",
      "Embedding size: 139 100 138 138 138 100 cat_nextClickLeakDay\n",
      "Embedding size: 139 100 138 138 138 100 cat_nextNextClickLeakDay\n",
      "Embedding size: 769 100 768 768 535 100 cat_app\n",
      "Embedding size: 96 100 95 95 90 100 cat_device\n",
      "Embedding size: 957 100 956 954 606 100 cat_os\n",
      "Embedding size: 168 100 167 167 167 100 cat_count_ip\n",
      "Embedding size: 193 100 192 192 192 100 cat_count_app_channel\n",
      "Embedding size: 24 100 23 21 23 100 cat_hour\n",
      "Embedding size: 98 100 97 97 97 100 cat_dayhourcount_ip\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cat_nextClickLeakDay (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_nextNextClickLeakDay (Input (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_app (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_device (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_os (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_count_ip (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_count_app_channel (InputLay (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_hour (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat_dayhourcount_ip (InputLayer (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 100)       13900       cat_nextClickLeakDay[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 100)       13900       cat_nextNextClickLeakDay[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 100)       76900       cat_app[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1, 100)       9600        cat_device[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1, 100)       95700       cat_os[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 100)       16800       cat_count_ip[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 100)       19300       cat_count_app_channel[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 100)       2400        cat_hour[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1, 100)       9800        cat_dayhourcount_ip[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "numerical (InputLayer)          (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 900)       0           embedding_10[0][0]               \n",
      "                                                                 embedding_11[0][0]               \n",
      "                                                                 embedding_12[0][0]               \n",
      "                                                                 embedding_13[0][0]               \n",
      "                                                                 embedding_14[0][0]               \n",
      "                                                                 embedding_15[0][0]               \n",
      "                                                                 embedding_16[0][0]               \n",
      "                                                                 embedding_17[0][0]               \n",
      "                                                                 embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1000)         19000       numerical[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 1, 900)       0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 900)          0           spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000)         4000        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1900)         0           flatten_2[0][0]                  \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1000)         1901000     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1000)         4000        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1000)         1001000     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1000)         4000        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            1001        batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,192,301\n",
      "Trainable params: 3,186,301\n",
      "Non-trainable params: 6,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 174788422 samples, validate on 10115468 samples\n",
      "Epoch 1/100\n",
      "174780000/174788422 [============================>.] - ETA: 0s - loss: 0.0094roc-auc: 0 - roc-auc_val: 0.983393 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.929.hdf5.0\n",
      "174788422/174788422 [==============================] - 1269s 7us/step - loss: 0.0094 - val_loss: 0.0059\n",
      "Epoch 2/100\n",
      " 26420000/174788422 [===>..........................] - ETA: 17:54 - loss: 0.0053"
     ]
    }
   ],
   "source": [
    "auc = Predict(train_df,val_df,test_df,predictors,categorical,seed=get_opt('seed',2018))\n",
    "print('validation auc:',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_df = test_df[['pred']].rename(columns={'pred': 'is_attributed'})\n",
    "mapping = read_csv('../input/mapping.csv')\n",
    "click_id = read_csv('../input/sample_submission.csv',usecols=['click_id'])\n",
    "test_df = test_df.reset_index().merge(mapping, left_on='index', right_on='old_click_id', how='left')\n",
    "test_df = click_id.merge(test_df,on='click_id',how='left')\n",
    "outfile = '../csv/pred_test_'+target+'.csv'\n",
    "print('writing to',outfile)\n",
    "test_df[['click_id','is_attributed']].to_csv(outfile,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-30T17:27:38.755950Z",
     "start_time": "2018-05-30T17:27:38.748761Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7070b677c02f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'auc' is not defined"
     ]
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T15:15:41.135575Z",
     "start_time": "2018-05-29T15:15:36.272648Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************params**************\n",
      "batch_size: 20000\n",
      "dense_cate: 1000\n",
      "dense_nume_n_layers: 1\n",
      "drop: 0.2\n",
      "emb_cate: 100\n",
      "epochs_for_lr: 2\n",
      "lr: 0.001\n",
      "lr_fin: 0.0001\n",
      "lr_init: 0.001\n",
      "max_epochs: 100\n",
      "n_layers: 2\n",
      "patience: 3\n",
      "Embedding size: 212620 100 212619 211839 126388 100 ip\n",
      "Embedding size: 537 100 536 64 259 100 app\n",
      "Embedding size: 608 100 607 607 109 100 os\n",
      "Embedding size: 499 100 498 489 497 100 channel\n",
      "Embedding size: 3033 100 3032 3032 1473 100 device\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ip (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "app (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "os (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "channel (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "device (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 100)       21262000    ip[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 100)       53700       app[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 100)       60800       os[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 100)       49900       channel[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 100)       303300      device[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 500)       0           embedding_6[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "                                                                 embedding_9[0][0]                \n",
      "                                                                 embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 1, 500)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 500)          0           spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         501000      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1000)         4000        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1000)         1001000     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1000)         4000        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            1001        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 23,240,701\n",
      "Trainable params: 23,236,701\n",
      "Non-trainable params: 4,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "roc-auc: 0 - roc-auc_val: 0.556225 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.0\n",
      " - 3s - loss: 0.8108 - val_loss: 0.9788\n",
      "Epoch 2/100\n",
      "roc-auc: 0 - roc-auc_val: 0.851406 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.1\n",
      " - 0s - loss: 1.0055 - val_loss: 0.6259\n",
      "Epoch 3/100\n",
      "roc-auc: 0 - roc-auc_val: 0.823293 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.2\n",
      " - 0s - loss: 0.7842 - val_loss: 0.6430\n",
      "Epoch 4/100\n",
      "roc-auc: 0 - roc-auc_val: 0.849398 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.3\n",
      " - 0s - loss: 0.7698 - val_loss: 0.7882\n",
      "Epoch 5/100\n",
      "roc-auc: 0 - roc-auc_val: 0.915663 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.4\n",
      " - 0s - loss: 0.7432 - val_loss: 0.8853\n",
      "Epoch 6/100\n",
      "roc-auc: 0 - roc-auc_val: 0.794177 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.5\n",
      " - 0s - loss: 0.7049 - val_loss: 0.8585\n",
      "Epoch 7/100\n",
      "roc-auc: 0 - roc-auc_val: 0.76506 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.6\n",
      " - 0s - loss: 0.6989 - val_loss: 0.7193\n",
      "Epoch 8/100\n",
      "roc-auc: 0 - roc-auc_val: 0.866466 - roc-auc_test: 0                                                                                                    \n",
      "saving ../work/weights.1459.hdf5.7\n",
      " - 0s - loss: 0.6909 - val_loss: 0.6394\n",
      "Epoch  4 : EarlyStopping\n",
      "loading ../work/weights.1459.hdf5.4\n",
      "validation auc: 0.9156626506024096\n"
     ]
    }
   ],
   "source": [
    "auc = Predict(train_df,val_df,test_df,predictors=cat_cols,cat_feats=cat_cols,seed=get_opt('seed',2018))\n",
    "print('validation auc:',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
