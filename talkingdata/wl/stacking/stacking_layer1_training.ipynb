{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entry = 5\n",
    "K = 3\n",
    "folds = [0, 1, 2]\n",
    "folds_comp = [[1, 2], [0, 2], [0, 1]]\n",
    "base_path = '/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/'\n",
    "base_path_test = '/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/'\n",
    "\n",
    "func_pool = ['count', 'mean', 'reversemean', 'time2nextclick', \n",
    "             'time2previousclick', 'countfromfuture', 'countfrompast', 'lasttimediff']\n",
    "\n",
    "func_pool = ['count']\n",
    "\n",
    "target = 'is_attributed'\n",
    "categorical_feature = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "combine = 0\n",
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_rounds': 2000,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'num_threads': 4, # best speed: set to number of real cpu cores, which is vCPU/2\n",
    "        'device': 'cpu',\n",
    "        'max_depth': -1, # no limit. This is used to deal with over-fitting when #data is small.\n",
    "        'min_data_in_leaf': 390,  #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.7, #For example, if set to 0.8, will select 80% features before training each tree.  speed up training / deal with over-fitting\n",
    "        'feature_fraction_seed': 1,\n",
    "        'early_stopping_round':100,\n",
    "        'bagging_fraction': 0.7, #Randomly select part of data without resampling\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration. to enable bagging, bagging_fraction should be set as well\n",
    "        'bagging_seed': 1,\n",
    "        'verbose': 0,\n",
    "        'scale_pos_weight': 400,\n",
    "        'metric' : [ 'auc']\n",
    "    }\n",
    "\n",
    "categorical_col = [ 'app', 'device', 'os', 'channel', 'hour', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_val(df, mode='shuffle', ratio=0.2, seed=19):\n",
    "    if mode == 'shuffle':\n",
    "        trainset, valset = train_test_split(df,test_size=ratio, random_state=seed)\n",
    "    return (trainset, valset)\n",
    "\n",
    "def train_lightgbm(x_train, x_val, feature_cols, categorical_feature, params, best_round = None, target='is_attributed'):\n",
    "    param = params.copy()\n",
    "    y_train = x_train[target].values\n",
    "    y_val = x_val[target].values\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train[feature_cols], y_train, categorical_feature = categorical_feature)\n",
    "    lgb_val = lgb.Dataset(x_val[feature_cols], y_val, categorical_feature = categorical_feature)\n",
    "    if best_round is not None:\n",
    "        param['num_rounds'] = best_round\n",
    "        del param['early_stopping_round']\n",
    "    print('start training')\n",
    "    model = lgb.train(param, train_set=lgb_train, valid_sets=lgb_val, verbose_eval=10)\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n",
      "starting training function : count\n",
      "column pressed: 0\n",
      "=================\n",
      "loading df_train done!\n",
      "train_fold0_count_0.csv\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1027: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:668: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.970854\n",
      "[20]\tvalid_0's auc: 0.972561\n",
      "[30]\tvalid_0's auc: 0.974158\n",
      "[40]\tvalid_0's auc: 0.975203\n",
      "[50]\tvalid_0's auc: 0.975944\n",
      "[60]\tvalid_0's auc: 0.976471\n",
      "[70]\tvalid_0's auc: 0.976803\n",
      "[80]\tvalid_0's auc: 0.976973\n",
      "[90]\tvalid_0's auc: 0.977052\n",
      "[100]\tvalid_0's auc: 0.977144\n",
      "[110]\tvalid_0's auc: 0.977251\n",
      "[120]\tvalid_0's auc: 0.977261\n",
      "[130]\tvalid_0's auc: 0.977272\n",
      "[140]\tvalid_0's auc: 0.977317\n",
      "[150]\tvalid_0's auc: 0.97737\n",
      "[160]\tvalid_0's auc: 0.977366\n",
      "[170]\tvalid_0's auc: 0.977382\n",
      "[180]\tvalid_0's auc: 0.977385\n",
      "[190]\tvalid_0's auc: 0.977401\n",
      "[200]\tvalid_0's auc: 0.977424\n",
      "[210]\tvalid_0's auc: 0.977436\n",
      "[220]\tvalid_0's auc: 0.977417\n",
      "[230]\tvalid_0's auc: 0.97742\n",
      "[240]\tvalid_0's auc: 0.977423\n",
      "[250]\tvalid_0's auc: 0.977423\n",
      "[260]\tvalid_0's auc: 0.977433\n",
      "[270]\tvalid_0's auc: 0.977456\n",
      "[280]\tvalid_0's auc: 0.977433\n",
      "[290]\tvalid_0's auc: 0.977432\n",
      "[300]\tvalid_0's auc: 0.977406\n",
      "[310]\tvalid_0's auc: 0.977402\n",
      "[320]\tvalid_0's auc: 0.977402\n",
      "[330]\tvalid_0's auc: 0.977417\n",
      "[340]\tvalid_0's auc: 0.977414\n",
      "[350]\tvalid_0's auc: 0.977407\n",
      "[360]\tvalid_0's auc: 0.977388\n",
      "[370]\tvalid_0's auc: 0.977397\n",
      "Early stopping, best iteration is:\n",
      "[274]\tvalid_0's auc: 0.977462\n",
      "fold: 0 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold1_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold0_predonfold1_count_0_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold2_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold0_predonfold2_count_0_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/test_fold0_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/result/test_fold0_count_0_param0.npy\n",
      "=================\n",
      "loading df_train done!\n",
      "train_fold1_count_0.csv\n",
      "start training\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.972454\n",
      "[20]\tvalid_0's auc: 0.974178\n",
      "[30]\tvalid_0's auc: 0.975399\n",
      "[40]\tvalid_0's auc: 0.976279\n",
      "[50]\tvalid_0's auc: 0.976927\n",
      "[60]\tvalid_0's auc: 0.977369\n",
      "[70]\tvalid_0's auc: 0.977618\n",
      "[80]\tvalid_0's auc: 0.977899\n",
      "[90]\tvalid_0's auc: 0.978042\n",
      "[100]\tvalid_0's auc: 0.978084\n",
      "[110]\tvalid_0's auc: 0.978163\n",
      "[120]\tvalid_0's auc: 0.978208\n",
      "[130]\tvalid_0's auc: 0.978192\n",
      "[140]\tvalid_0's auc: 0.978217\n",
      "[150]\tvalid_0's auc: 0.978264\n",
      "[160]\tvalid_0's auc: 0.978257\n",
      "[170]\tvalid_0's auc: 0.978253\n",
      "[180]\tvalid_0's auc: 0.978286\n",
      "[190]\tvalid_0's auc: 0.978275\n",
      "[200]\tvalid_0's auc: 0.978257\n",
      "[210]\tvalid_0's auc: 0.978256\n",
      "[220]\tvalid_0's auc: 0.97824\n",
      "[230]\tvalid_0's auc: 0.978223\n",
      "[240]\tvalid_0's auc: 0.978239\n",
      "[250]\tvalid_0's auc: 0.978256\n",
      "[260]\tvalid_0's auc: 0.978241\n",
      "[270]\tvalid_0's auc: 0.978231\n",
      "[280]\tvalid_0's auc: 0.978242\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid_0's auc: 0.978298\n",
      "fold: 1 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold0_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold1_predonfold0_count_0_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold2_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold1_predonfold2_count_0_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/test_fold1_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/result/test_fold1_count_0_param0.npy\n",
      "=================\n",
      "loading df_train done!\n",
      "train_fold2_count_0.csv\n",
      "start training\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.967956\n",
      "[20]\tvalid_0's auc: 0.969609\n",
      "[30]\tvalid_0's auc: 0.970964\n",
      "[40]\tvalid_0's auc: 0.972223\n",
      "[50]\tvalid_0's auc: 0.972891\n",
      "[60]\tvalid_0's auc: 0.973386\n",
      "[70]\tvalid_0's auc: 0.973766\n",
      "[80]\tvalid_0's auc: 0.973996\n",
      "[90]\tvalid_0's auc: 0.974152\n",
      "[100]\tvalid_0's auc: 0.974245\n",
      "[110]\tvalid_0's auc: 0.974361\n",
      "[120]\tvalid_0's auc: 0.974408\n",
      "[130]\tvalid_0's auc: 0.974406\n",
      "[140]\tvalid_0's auc: 0.974438\n",
      "[150]\tvalid_0's auc: 0.974467\n",
      "[160]\tvalid_0's auc: 0.974443\n",
      "[170]\tvalid_0's auc: 0.974453\n",
      "[180]\tvalid_0's auc: 0.974472\n",
      "[190]\tvalid_0's auc: 0.974479\n",
      "[200]\tvalid_0's auc: 0.974508\n",
      "[210]\tvalid_0's auc: 0.974526\n",
      "[220]\tvalid_0's auc: 0.97454\n",
      "[230]\tvalid_0's auc: 0.974546\n",
      "[240]\tvalid_0's auc: 0.974558\n",
      "[250]\tvalid_0's auc: 0.974582\n",
      "[260]\tvalid_0's auc: 0.974603\n",
      "[270]\tvalid_0's auc: 0.974599\n",
      "[280]\tvalid_0's auc: 0.974582\n",
      "[290]\tvalid_0's auc: 0.974596\n",
      "[300]\tvalid_0's auc: 0.97459\n",
      "[310]\tvalid_0's auc: 0.974543\n",
      "[320]\tvalid_0's auc: 0.974536\n",
      "[330]\tvalid_0's auc: 0.974554\n",
      "[340]\tvalid_0's auc: 0.974534\n",
      "[350]\tvalid_0's auc: 0.974541\n",
      "[360]\tvalid_0's auc: 0.974523\n",
      "[370]\tvalid_0's auc: 0.974541\n",
      "Early stopping, best iteration is:\n",
      "[274]\tvalid_0's auc: 0.974609\n",
      "fold: 2 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold0_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold2_predonfold0_count_0_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold1_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold2_predonfold1_count_0_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/test_fold2_count_0.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/result/test_fold2_count_0_param0.npy\n",
      "column pressed: 1\n",
      "=================\n",
      "loading df_train done!\n",
      "train_fold0_count_1.csv\n",
      "start training\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.970536\n",
      "[20]\tvalid_0's auc: 0.972452\n",
      "[30]\tvalid_0's auc: 0.973825\n",
      "[40]\tvalid_0's auc: 0.974993\n",
      "[50]\tvalid_0's auc: 0.975712\n",
      "[60]\tvalid_0's auc: 0.976224\n",
      "[70]\tvalid_0's auc: 0.976554\n",
      "[80]\tvalid_0's auc: 0.976801\n",
      "[90]\tvalid_0's auc: 0.976933\n",
      "[100]\tvalid_0's auc: 0.97704\n",
      "[110]\tvalid_0's auc: 0.977093\n",
      "[120]\tvalid_0's auc: 0.977163\n",
      "[130]\tvalid_0's auc: 0.977183\n",
      "[140]\tvalid_0's auc: 0.977228\n",
      "[150]\tvalid_0's auc: 0.977261\n",
      "[160]\tvalid_0's auc: 0.977285\n",
      "[170]\tvalid_0's auc: 0.977306\n",
      "[180]\tvalid_0's auc: 0.977358\n",
      "[190]\tvalid_0's auc: 0.977328\n",
      "[200]\tvalid_0's auc: 0.977347\n",
      "[210]\tvalid_0's auc: 0.977379\n",
      "[220]\tvalid_0's auc: 0.977388\n",
      "[230]\tvalid_0's auc: 0.977392\n",
      "[240]\tvalid_0's auc: 0.97737\n",
      "[250]\tvalid_0's auc: 0.977369\n",
      "[260]\tvalid_0's auc: 0.977355\n",
      "[270]\tvalid_0's auc: 0.977339\n",
      "[280]\tvalid_0's auc: 0.977318\n",
      "[290]\tvalid_0's auc: 0.977326\n",
      "[300]\tvalid_0's auc: 0.97731\n",
      "[310]\tvalid_0's auc: 0.977318\n",
      "[320]\tvalid_0's auc: 0.977289\n",
      "Early stopping, best iteration is:\n",
      "[223]\tvalid_0's auc: 0.977414\n",
      "fold: 0 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold1_count_1.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold0_predonfold1_count_1_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold2_count_1.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold0_predonfold2_count_1_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/test_fold0_count_1.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/result/test_fold0_count_1_param0.npy\n",
      "=================\n",
      "loading df_train done!\n",
      "train_fold1_count_1.csv\n",
      "start training\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.9728\n",
      "[20]\tvalid_0's auc: 0.974217\n",
      "[30]\tvalid_0's auc: 0.975376\n",
      "[40]\tvalid_0's auc: 0.976169\n",
      "[50]\tvalid_0's auc: 0.976763\n",
      "[60]\tvalid_0's auc: 0.977208\n",
      "[70]\tvalid_0's auc: 0.977473\n",
      "[80]\tvalid_0's auc: 0.977644\n",
      "[90]\tvalid_0's auc: 0.977733\n",
      "[100]\tvalid_0's auc: 0.977823\n",
      "[110]\tvalid_0's auc: 0.977855\n",
      "[120]\tvalid_0's auc: 0.977894\n",
      "[130]\tvalid_0's auc: 0.977966\n",
      "[140]\tvalid_0's auc: 0.977989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\tvalid_0's auc: 0.977996\n",
      "[160]\tvalid_0's auc: 0.978002\n",
      "[170]\tvalid_0's auc: 0.977984\n",
      "[180]\tvalid_0's auc: 0.97799\n",
      "[190]\tvalid_0's auc: 0.97798\n",
      "[200]\tvalid_0's auc: 0.977971\n",
      "[210]\tvalid_0's auc: 0.977995\n",
      "[220]\tvalid_0's auc: 0.978028\n",
      "[230]\tvalid_0's auc: 0.97803\n",
      "[240]\tvalid_0's auc: 0.978067\n",
      "[250]\tvalid_0's auc: 0.978105\n",
      "[260]\tvalid_0's auc: 0.978134\n",
      "[270]\tvalid_0's auc: 0.978118\n",
      "[280]\tvalid_0's auc: 0.978097\n",
      "[290]\tvalid_0's auc: 0.978111\n",
      "[300]\tvalid_0's auc: 0.978079\n",
      "[310]\tvalid_0's auc: 0.978064\n",
      "[320]\tvalid_0's auc: 0.978048\n",
      "[330]\tvalid_0's auc: 0.978036\n",
      "[340]\tvalid_0's auc: 0.97802\n",
      "[350]\tvalid_0's auc: 0.978013\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's auc: 0.978141\n",
      "fold: 1 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold0_count_1.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold1_predonfold0_count_1_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold2_count_1.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold1_predonfold2_count_1_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/test_fold1_count_1.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/result/test_fold1_count_1_param0.npy\n",
      "=================\n",
      "loading df_train done!\n",
      "train_fold2_count_1.csv\n",
      "start training\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.968359\n",
      "[20]\tvalid_0's auc: 0.969651\n",
      "[30]\tvalid_0's auc: 0.970901\n",
      "[40]\tvalid_0's auc: 0.972001\n",
      "[50]\tvalid_0's auc: 0.972792\n",
      "[60]\tvalid_0's auc: 0.973301\n",
      "[70]\tvalid_0's auc: 0.973597\n",
      "[80]\tvalid_0's auc: 0.97379\n",
      "[90]\tvalid_0's auc: 0.973959\n",
      "[100]\tvalid_0's auc: 0.974018\n",
      "[110]\tvalid_0's auc: 0.974078\n",
      "[120]\tvalid_0's auc: 0.974133\n",
      "[130]\tvalid_0's auc: 0.974194\n",
      "[140]\tvalid_0's auc: 0.974219\n",
      "[150]\tvalid_0's auc: 0.974193\n",
      "[160]\tvalid_0's auc: 0.974187\n",
      "[170]\tvalid_0's auc: 0.974172\n",
      "[180]\tvalid_0's auc: 0.974221\n",
      "[190]\tvalid_0's auc: 0.974207\n",
      "[200]\tvalid_0's auc: 0.974245\n",
      "[210]\tvalid_0's auc: 0.974269\n",
      "[220]\tvalid_0's auc: 0.974279\n",
      "[230]\tvalid_0's auc: 0.974272\n",
      "[240]\tvalid_0's auc: 0.974298\n",
      "[250]\tvalid_0's auc: 0.974296\n",
      "[260]\tvalid_0's auc: 0.974303\n",
      "[270]\tvalid_0's auc: 0.974325\n",
      "[280]\tvalid_0's auc: 0.974337\n",
      "[290]\tvalid_0's auc: 0.974378\n",
      "[300]\tvalid_0's auc: 0.974364\n",
      "[310]\tvalid_0's auc: 0.97438\n",
      "[320]\tvalid_0's auc: 0.974391\n",
      "[330]\tvalid_0's auc: 0.974419\n",
      "[340]\tvalid_0's auc: 0.974394\n",
      "[350]\tvalid_0's auc: 0.974408\n",
      "[360]\tvalid_0's auc: 0.974397\n",
      "[370]\tvalid_0's auc: 0.974417\n",
      "[380]\tvalid_0's auc: 0.974415\n",
      "[390]\tvalid_0's auc: 0.974415\n",
      "[400]\tvalid_0's auc: 0.974443\n",
      "[410]\tvalid_0's auc: 0.974438\n",
      "[420]\tvalid_0's auc: 0.974438\n",
      "[430]\tvalid_0's auc: 0.974419\n",
      "[440]\tvalid_0's auc: 0.974416\n",
      "[450]\tvalid_0's auc: 0.974392\n",
      "[460]\tvalid_0's auc: 0.974371\n",
      "[470]\tvalid_0's auc: 0.97434\n",
      "[480]\tvalid_0's auc: 0.974318\n",
      "[490]\tvalid_0's auc: 0.974307\n",
      "[500]\tvalid_0's auc: 0.974336\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalid_0's auc: 0.974445\n",
      "fold: 2 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold0_count_1.csv\n",
      "[100]\tvalid_0's auc: 0.978158\n",
      "[110]\tvalid_0's auc: 0.978214\n",
      "[120]\tvalid_0's auc: 0.978232\n",
      "[130]\tvalid_0's auc: 0.978282\n",
      "[140]\tvalid_0's auc: 0.978316\n",
      "[150]\tvalid_0's auc: 0.978344\n",
      "[160]\tvalid_0's auc: 0.978372\n",
      "[170]\tvalid_0's auc: 0.978387\n",
      "[180]\tvalid_0's auc: 0.978395\n",
      "[190]\tvalid_0's auc: 0.97844\n",
      "[200]\tvalid_0's auc: 0.978436\n",
      "[210]\tvalid_0's auc: 0.978456\n",
      "[220]\tvalid_0's auc: 0.978469\n",
      "[230]\tvalid_0's auc: 0.978453\n",
      "[240]\tvalid_0's auc: 0.978487\n",
      "[250]\tvalid_0's auc: 0.978507\n",
      "[260]\tvalid_0's auc: 0.97853\n",
      "[270]\tvalid_0's auc: 0.97852\n",
      "[280]\tvalid_0's auc: 0.97854\n",
      "[290]\tvalid_0's auc: 0.978533\n",
      "[300]\tvalid_0's auc: 0.978517\n",
      "[310]\tvalid_0's auc: 0.978529\n",
      "[320]\tvalid_0's auc: 0.978521\n",
      "[330]\tvalid_0's auc: 0.978509\n",
      "[340]\tvalid_0's auc: 0.978498\n",
      "[350]\tvalid_0's auc: 0.978502\n",
      "[360]\tvalid_0's auc: 0.978495\n",
      "[370]\tvalid_0's auc: 0.978502\n",
      "Early stopping, best iteration is:\n",
      "[275]\tvalid_0's auc: 0.978543\n",
      "fold: 0 \t function: count \t\n",
      "start predicting\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold1_count_2.csv\n",
      "predicting...\n",
      "saving files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/train_fold0_predonfold1_count_2_param0.npy\n",
      "loading files: \n",
      "/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/train_fold2_count_2.csv\n"
     ]
    }
   ],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "combine = 0 # for lightgbm params\n",
    "\n",
    "for func in func_pool:\n",
    "    print('starting training function : {}'.format(func))\n",
    "    for entry_pressed in range(entry):\n",
    "        print('column pressed: {}'.format(entry_pressed))\n",
    "        for fold in range(K):\n",
    "            fold_trainset = folds[fold]\n",
    "            fold_pred_train0, fold_pred_train1 = folds_comp[fold] \n",
    "            trainset_name = 'train_fold{}_{}_{}.csv'.format(fold_trainset, func, entry_pressed)\n",
    "            pred_dict = {'train0': fold_pred_train0, 'train1': fold_pred_train1, 'test':fold_trainset}\n",
    "            print('=================')\n",
    "            df_train = pd.read_csv(base_path + trainset_name)\n",
    "            print('loading df_train done!')\n",
    "            print(trainset_name)\n",
    "            trainset, valset = get_train_val(df_train)\n",
    "            feature_cols = list(set(trainset.columns) - set([target]))\n",
    "            model = train_lightgbm(trainset, valset, feature_cols, categorical_feature, params)\n",
    "#             train on all. comment to save time\n",
    "#             best_round = model.best_iteration\n",
    "#             model = train_lightgbm(df_train, valset, feature_cols, categorical_feature, params, best_round)\n",
    "            del df_train\n",
    "            gc.collect()\n",
    "            ###log\n",
    "            all_str = 'fold: {} \\t function: {} \\t'.format(fold, func)\n",
    "            print(all_str)\n",
    "            with open('feature_all.txt', 'w') as text_file:\n",
    "                text_file.write(all_str + '\\n')\n",
    "            #!!!\n",
    "            print('start predicting')\n",
    "            for each_fold in pred_dict:\n",
    "                if each_fold == 'test':\n",
    "                    file_name = 'test_fold{}_{}_{}.csv'.format(pred_dict[each_fold], func, entry_pressed)\n",
    "                    read_path = base_path_test + file_name\n",
    "                    save_name = 'test_fold{}_{}_{}_param{}.npy'.format(pred_dict[each_fold], func, entry_pressed, combine)\n",
    "                    save_path = base_path_test + 'result/' + save_name\n",
    "                else:\n",
    "                    file_name = 'train_fold{}_{}_{}.csv'.format(pred_dict[each_fold], func, entry_pressed)\n",
    "                    read_path = base_path + file_name\n",
    "                    save_name = 'train_fold{}_predonfold{}_{}_{}_param{}.npy'.format(fold_trainset, pred_dict[each_fold], func, entry_pressed, combine)\n",
    "                    save_path = base_path + 'result/'+ save_name\n",
    "                print('loading files: \\n{}'.format(read_path))\n",
    "                df_pred = pd.read_csv(read_path)\n",
    "                print('predicting...')\n",
    "                preds = model.predict(df_pred[feature_cols])\n",
    "                del df_pred\n",
    "                gc.collect()\n",
    "                print('saving files: \\n{}'.format(save_path))\n",
    "                np.save(save_path, preds)\n",
    "                ###log\n",
    "\n",
    "                with open('feature_all.txt', 'w') as text_file:\n",
    "                    text_file.write('saving ' + save_path + '\\n')\n",
    "                #!!!\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(save_path, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
