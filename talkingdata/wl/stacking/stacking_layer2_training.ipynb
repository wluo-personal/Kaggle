{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entry = 5\n",
    "K = 3\n",
    "folds = [0, 1, 2]\n",
    "folds_comp = [[1, 2], [0, 2], [0, 1]]\n",
    "combine = 0\n",
    "\n",
    "base_path = '/home/kai/data/kaggle/talkingdata/wl/data/stacking/train/result/layer2/'\n",
    "base_path_test = '/home/kai/data/kaggle/talkingdata/wl/data/stacking/test/result/layer2/'\n",
    "\n",
    "func_pool = ['count', 'mean', 'reversemean', 'time2nextclick', \n",
    "             'time2previousclick', 'countfromfuture', 'countfrompast', 'lasttimediff']\n",
    "\n",
    "func_pool = ['count']\n",
    "target = 'is_attributed'\n",
    "\n",
    "filebase = 'train_fold{}_predonfold{}_{}_{}_param{}.npy'\n",
    "filebase_test = 'test_fold{}_{}_{}_param{}.npy'\n",
    "label_file = '/home/kai/data/kaggle/talkingdata/data/train_label.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_rounds': 2000,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 2,\n",
    "        'num_threads': 4, # best speed: set to number of real cpu cores, which is vCPU/2\n",
    "        'device': 'cpu',\n",
    "        'max_depth': 2, # no limit. This is used to deal with over-fitting when #data is small.\n",
    "        'min_data_in_leaf': 2000,  #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.7, #For example, if set to 0.8, will select 80% features before training each tree.  speed up training / deal with over-fitting\n",
    "        'feature_fraction_seed': 1,\n",
    "        'early_stopping_round':100,\n",
    "        'bagging_fraction': 0.7, #Randomly select part of data without resampling\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration. to enable bagging, bagging_fraction should be set as well\n",
    "        'bagging_seed': 1,\n",
    "        'verbose': 0,\n",
    "        'scale_pos_weight': 401,\n",
    "        'metric' : [ 'auc']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading training df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n",
      "train_layer2_count_0_param0.npy\n",
      "layer2_count_0_param0\n",
      "loading...\n",
      "train_layer2_count_1_param0.npy\n",
      "layer2_count_1_param0\n",
      "loading...\n",
      "train_layer2_count_2_param0.npy\n",
      "layer2_count_2_param0\n",
      "loading...\n",
      "train_layer2_count_3_param0.npy\n",
      "layer2_count_3_param0\n",
      "loading...\n",
      "train_layer2_count_4_param0.npy\n",
      "layer2_count_4_param0\n",
      "adding label...\n",
      "done\n",
      "loading...\n",
      "test_layer2_count_0_param0.npy\n",
      "layer2_count_0_param0\n",
      "loading...\n",
      "test_layer2_count_1_param0.npy\n",
      "layer2_count_1_param0\n",
      "loading...\n",
      "test_layer2_count_2_param0.npy\n",
      "layer2_count_2_param0\n",
      "loading...\n",
      "test_layer2_count_3_param0.npy\n",
      "layer2_count_3_param0\n",
      "loading...\n",
      "test_layer2_count_4_param0.npy\n",
      "layer2_count_4_param0\n",
      "done\n",
      "Training set length: 184903890 size:8.265848107635975\n",
      "Testing set length: 18790469 size:0.6999996155500412\n"
     ]
    }
   ],
   "source": [
    "def encapsulate_train(func_pool, entry, base_path, label_file, target='is_attributed',combine=0):\n",
    "    df = pd.DataFrame()\n",
    "    for func in func_pool:\n",
    "        for ent in range(entry):\n",
    "            file_name = 'train_layer2_{}_{}_param{}.npy'.format(func, ent, combine)\n",
    "            print('loading...\\n{}'.format(file_name))\n",
    "            load_path = base_path + file_name\n",
    "            pred = np.load(load_path)\n",
    "            feature_name = 'layer2_{}_{}_param{}'.format(func, ent, combine)\n",
    "            df[feature_name] = pred\n",
    "            print(feature_name)\n",
    "    print('adding label...')\n",
    "    label = np.load(label_file)\n",
    "    df[target] = label\n",
    "    print('done')\n",
    "    return df\n",
    "\n",
    "def encapsulate_test(func_pool, entry, base_path, target='is_attributed',combine=0):\n",
    "    df = pd.DataFrame()\n",
    "    for func in func_pool:\n",
    "        for ent in range(entry):\n",
    "            file_name = 'test_layer2_{}_{}_param{}.npy'.format(func, ent, combine)\n",
    "            print('loading...\\n{}'.format(file_name))\n",
    "            load_path = base_path + file_name\n",
    "            pred = np.load(load_path)\n",
    "            feature_name = 'layer2_{}_{}_param{}'.format(func, ent, combine)\n",
    "            df[feature_name] = pred\n",
    "            print(feature_name)\n",
    "    print('done')\n",
    "    return df\n",
    "\n",
    "df = encapsulate_train(func_pool, entry, base_path, label_file, target=target,combine=0)\n",
    "df_test = encapsulate_test(func_pool, entry, base_path_test,  target=target,combine=0)\n",
    "    \n",
    "print('Training set length: {} size:{}'.format(len(df), sys.getsizeof(df)/ 1024**3))\n",
    "print('Testing set length: {} size:{}'.format(len(df_test), sys.getsizeof(df_test)/ 1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer2_count_1_param0',\n",
       " 'layer2_count_2_param0',\n",
       " 'layer2_count_3_param0',\n",
       " 'layer2_count_0_param0',\n",
       " 'layer2_count_4_param0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = list(set(df.columns) - set([target]))\n",
    "categorical_feature = None\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting done\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.959916\n",
      "[20]\tvalid_0's auc: 0.969442\n",
      "[30]\tvalid_0's auc: 0.973527\n",
      "[40]\tvalid_0's auc: 0.976445\n",
      "[50]\tvalid_0's auc: 0.976994\n",
      "[60]\tvalid_0's auc: 0.977247\n",
      "[70]\tvalid_0's auc: 0.977331\n",
      "[80]\tvalid_0's auc: 0.977367\n",
      "[90]\tvalid_0's auc: 0.977372\n",
      "[100]\tvalid_0's auc: 0.977375\n",
      "[110]\tvalid_0's auc: 0.97738\n",
      "[120]\tvalid_0's auc: 0.977388\n",
      "[130]\tvalid_0's auc: 0.977393\n",
      "[140]\tvalid_0's auc: 0.977393\n",
      "[150]\tvalid_0's auc: 0.977398\n",
      "[160]\tvalid_0's auc: 0.977401\n",
      "[170]\tvalid_0's auc: 0.9774\n",
      "[180]\tvalid_0's auc: 0.977403\n",
      "[190]\tvalid_0's auc: 0.977404\n",
      "[200]\tvalid_0's auc: 0.97744\n",
      "[210]\tvalid_0's auc: 0.977443\n",
      "[220]\tvalid_0's auc: 0.977446\n",
      "[230]\tvalid_0's auc: 0.977447\n",
      "[240]\tvalid_0's auc: 0.977449\n",
      "[250]\tvalid_0's auc: 0.97745\n",
      "[260]\tvalid_0's auc: 0.977451\n",
      "[270]\tvalid_0's auc: 0.977452\n",
      "[280]\tvalid_0's auc: 0.977454\n",
      "[290]\tvalid_0's auc: 0.977455\n",
      "[300]\tvalid_0's auc: 0.977457\n",
      "[310]\tvalid_0's auc: 0.977457\n",
      "[320]\tvalid_0's auc: 0.977458\n",
      "[330]\tvalid_0's auc: 0.97746\n",
      "[340]\tvalid_0's auc: 0.977461\n",
      "[350]\tvalid_0's auc: 0.977461\n",
      "[360]\tvalid_0's auc: 0.977464\n",
      "[370]\tvalid_0's auc: 0.977465\n",
      "[380]\tvalid_0's auc: 0.977467\n",
      "[390]\tvalid_0's auc: 0.977469\n",
      "[400]\tvalid_0's auc: 0.977471\n",
      "[410]\tvalid_0's auc: 0.977471\n",
      "[420]\tvalid_0's auc: 0.977473\n",
      "[430]\tvalid_0's auc: 0.977473\n",
      "[440]\tvalid_0's auc: 0.977475\n",
      "[450]\tvalid_0's auc: 0.977476\n",
      "[460]\tvalid_0's auc: 0.977476\n",
      "[470]\tvalid_0's auc: 0.977478\n",
      "[480]\tvalid_0's auc: 0.977479\n",
      "[490]\tvalid_0's auc: 0.97748\n",
      "[500]\tvalid_0's auc: 0.97748\n",
      "[510]\tvalid_0's auc: 0.97748\n",
      "[520]\tvalid_0's auc: 0.977481\n",
      "[530]\tvalid_0's auc: 0.977483\n",
      "[540]\tvalid_0's auc: 0.977484\n",
      "[550]\tvalid_0's auc: 0.977485\n",
      "[560]\tvalid_0's auc: 0.977486\n",
      "[570]\tvalid_0's auc: 0.977486\n",
      "[580]\tvalid_0's auc: 0.977488\n",
      "[590]\tvalid_0's auc: 0.977488\n",
      "[600]\tvalid_0's auc: 0.97749\n",
      "[610]\tvalid_0's auc: 0.97749\n",
      "[620]\tvalid_0's auc: 0.977491\n",
      "[630]\tvalid_0's auc: 0.97749\n",
      "[640]\tvalid_0's auc: 0.977491\n",
      "[650]\tvalid_0's auc: 0.977492\n",
      "[660]\tvalid_0's auc: 0.977492\n",
      "[670]\tvalid_0's auc: 0.977493\n",
      "[680]\tvalid_0's auc: 0.977493\n",
      "[690]\tvalid_0's auc: 0.977494\n",
      "[700]\tvalid_0's auc: 0.977494\n",
      "[710]\tvalid_0's auc: 0.977495\n",
      "[720]\tvalid_0's auc: 0.977496\n",
      "[730]\tvalid_0's auc: 0.977496\n",
      "[740]\tvalid_0's auc: 0.977498\n",
      "[750]\tvalid_0's auc: 0.977498\n",
      "[760]\tvalid_0's auc: 0.977499\n",
      "[770]\tvalid_0's auc: 0.977499\n",
      "[780]\tvalid_0's auc: 0.9775\n",
      "[790]\tvalid_0's auc: 0.9775\n",
      "[800]\tvalid_0's auc: 0.9775\n",
      "[810]\tvalid_0's auc: 0.977501\n",
      "[820]\tvalid_0's auc: 0.977501\n",
      "[830]\tvalid_0's auc: 0.977502\n",
      "[840]\tvalid_0's auc: 0.977502\n",
      "[850]\tvalid_0's auc: 0.977502\n",
      "[860]\tvalid_0's auc: 0.977503\n",
      "[870]\tvalid_0's auc: 0.977503\n",
      "[880]\tvalid_0's auc: 0.977505\n",
      "[890]\tvalid_0's auc: 0.977505\n",
      "[900]\tvalid_0's auc: 0.977506\n",
      "[910]\tvalid_0's auc: 0.977505\n",
      "[920]\tvalid_0's auc: 0.977506\n",
      "[930]\tvalid_0's auc: 0.977506\n",
      "[940]\tvalid_0's auc: 0.977508\n",
      "[950]\tvalid_0's auc: 0.977508\n",
      "[960]\tvalid_0's auc: 0.977507\n",
      "[970]\tvalid_0's auc: 0.977507\n",
      "[980]\tvalid_0's auc: 0.977507\n",
      "[990]\tvalid_0's auc: 0.977507\n",
      "[1000]\tvalid_0's auc: 0.977507\n",
      "[1010]\tvalid_0's auc: 0.977507\n",
      "[1020]\tvalid_0's auc: 0.977508\n",
      "[1030]\tvalid_0's auc: 0.977508\n",
      "[1040]\tvalid_0's auc: 0.977508\n",
      "[1050]\tvalid_0's auc: 0.977508\n",
      "[1060]\tvalid_0's auc: 0.977508\n",
      "[1070]\tvalid_0's auc: 0.977509\n",
      "[1080]\tvalid_0's auc: 0.977508\n",
      "[1090]\tvalid_0's auc: 0.977509\n",
      "[1100]\tvalid_0's auc: 0.97751\n",
      "[1110]\tvalid_0's auc: 0.977511\n",
      "[1120]\tvalid_0's auc: 0.977511\n",
      "[1130]\tvalid_0's auc: 0.977511\n",
      "[1140]\tvalid_0's auc: 0.977511\n",
      "[1150]\tvalid_0's auc: 0.977512\n",
      "[1160]\tvalid_0's auc: 0.977512\n",
      "[1170]\tvalid_0's auc: 0.977513\n",
      "[1180]\tvalid_0's auc: 0.977514\n",
      "[1190]\tvalid_0's auc: 0.977513\n",
      "[1200]\tvalid_0's auc: 0.977512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def get_train_val(df, mode='shuffle', ratio=0.1, seed=19):\n",
    "    if mode == 'shuffle':\n",
    "        trainset, valset = train_test_split(df,test_size=ratio, random_state=seed)\n",
    "    return (trainset, valset)\n",
    "\n",
    "\n",
    "def train_lightgbm(x_train, x_val, feature_cols, categorical_feature, params, best_round = None, target='is_attributed'):\n",
    "    param = params.copy()\n",
    "    y_train = x_train[target].values\n",
    "    y_val = x_val[target].values\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train[feature_cols], y_train, categorical_feature = categorical_feature)\n",
    "    lgb_val = lgb.Dataset(x_val[feature_cols], y_val, categorical_feature = categorical_feature)\n",
    "    if best_round is not None:\n",
    "        param['num_rounds'] = best_round\n",
    "        del param['early_stopping_round']\n",
    "    print('start training')\n",
    "    model = lgb.train(param, train_set=lgb_train, valid_sets=lgb_val, verbose_eval=10)\n",
    "    return model\n",
    "\n",
    "trainset,valset = get_train_val(df, ratio=0.1)\n",
    "print('splitting done')\n",
    "model = train_lightgbm(trainset, valset, feature_cols, categorical_feature, params)\n",
    "#             train on all. comment to save time\n",
    "# best_round = model.best_iteration\n",
    "# print('training on best round')\n",
    "# model = train_lightgbm(df, valset, feature_cols, categorical_feature, params, best_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n",
      "predicting done\n",
      "ROC is: 0.9769058528809066\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# trainset,valset = get_train_val(df)\n",
    "\n",
    "# def logisticReg(df,feature_cols, target):\n",
    "#     x = df[feature_cols].values\n",
    "#     y = df[target].values\n",
    "#     model_log = LogisticRegression(C=1)\n",
    "#     model_log.fit(x, y)\n",
    "#     return model_log\n",
    "\n",
    "# model_log = logisticReg(trainset, feature_cols, target)\n",
    "# print('training done')\n",
    "# pred_val = model_log.predict_proba(valset[feature_cols].values)[:,1]\n",
    "# print('predicting done')\n",
    "# roc = roc_auc_score(valset[target].values, pred_val)\n",
    "# print('ROC is: {}'.format(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw file done!\n",
      "predicting file done!\n",
      "saving done\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "\n",
    "df_test_raw = pd.read_csv('/home/kai/data/kaggle/talkingdata/data/test.csv')\n",
    "print('loading raw file done!')\n",
    "\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['click_id'] = df_test_raw['click_id']\n",
    "df_sub['is_attributed'] = model.predict(df_test)\n",
    "print('predicting file done!')\n",
    "\n",
    "df_sub.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/submission/layer2_combine01234_leaf3.csv.gz', compression='gzip', index=False)\n",
    "print('saving done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valset.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
