{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Dropout, concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day7_features_matrixFactv1.csv\n",
      "day8_features_matrixFactv1.csv\n",
      "day9_features_matrixFactv1.csv\n",
      "test_features_matrixFactv1.csv\n"
     ]
    }
   ],
   "source": [
    "load_path = '/home/kai/data/kaggle/talkingdata/wl/data/equalhour/'\n",
    "file_format = '{}_features_matrixFactv1.csv'\n",
    "day_list = ['day7', 'day8', 'day9']\n",
    "df_dict = {}\n",
    "for file in ['day7', 'day8', 'day9', 'test']: \n",
    "    df_dict[file] = pd.read_csv(load_path+file_format.format(file))\n",
    "    print(file_format.format(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "categorical_col = [ 'app', 'device', 'os', 'channel', 'hour']\n",
    "target = 'is_attributed'\n",
    "numeric_col = set(df_dict['day7'].columns) - set(categorical_col) - set([target])\n",
    "\n",
    "    \n",
    "def get_encoder(df_all, categorical_col):\n",
    "    encoder = {}\n",
    "    for each in categorical_col:\n",
    "        print('processing {}'.format(each))\n",
    "        coder = LabelEncoder()\n",
    "        coder.fit(df_all[each])\n",
    "        encoder[each] = coder\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def apply_encoder(df, encoder):\n",
    "    for col in encoder:\n",
    "        print('apply encoder to col: {}'.format(col))\n",
    "        df.loc[:,col] =  encoder[col].transform(df[col])\n",
    "    return df.copy()\n",
    "\n",
    "def max_input_length(encoder):\n",
    "    max_dict = {}\n",
    "    for col in encoder:\n",
    "        max_dict[col] = len(encoder[col].classes_)\n",
    "    return max_dict\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1, batch_size=100000):\n",
    "        super(Callback, self).__init__()\n",
    "        print('RocAuc evaluating batch size is: {}'.format(batch_size))\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            print('\\n on epoch end, start predicting validation set')\n",
    "            y_pred = self.model.predict(self.X_val, batch_size=batch_size,verbose=1)\n",
    "            print('\\n start calculating ROC-AUC')\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "            \n",
    "def get_keras_data(dataset, numeric_col):\n",
    "    X = {\n",
    "        'app': dataset.app.values,\n",
    "        'channel': dataset.channel.values,\n",
    "        'os': dataset.os.values,\n",
    "        'device': dataset.device.values,\n",
    "        'hour': dataset.hour.values,\n",
    "        'nc': dataset.loc[:,numeric_col].values\n",
    "    }\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dict['day7'].channel.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val day7\n",
      "processing: app\n",
      "train index length is: 492\n",
      "test index length is: 432\n",
      "intersection index length is: 368\n",
      "---\n",
      "processing: device\n",
      "train index length is: 2110\n",
      "test index length is: 1504\n",
      "intersection index length is: 1176\n",
      "---\n",
      "processing: os\n",
      "train index length is: 436\n",
      "test index length is: 332\n",
      "intersection index length is: 276\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 190\n",
      "test index length is: 179\n",
      "intersection index length is: 176\n",
      "---\n",
      "========================================\n",
      "val day8\n",
      "processing: app\n",
      "train index length is: 443\n",
      "test index length is: 439\n",
      "intersection index length is: 389\n",
      "---\n",
      "processing: device\n",
      "train index length is: 1820\n",
      "test index length is: 1492\n",
      "intersection index length is: 1201\n",
      "---\n",
      "processing: os\n",
      "train index length is: 379\n",
      "test index length is: 332\n",
      "intersection index length is: 274\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 187\n",
      "test index length is: 179\n",
      "intersection index length is: 175\n",
      "---\n",
      "========================================\n",
      "val day9\n",
      "processing: app\n",
      "train index length is: 440\n",
      "test index length is: 407\n",
      "intersection index length is: 354\n",
      "---\n",
      "processing: device\n",
      "train index length is: 1493\n",
      "test index length is: 1705\n",
      "intersection index length is: 1087\n",
      "---\n",
      "processing: os\n",
      "train index length is: 333\n",
      "test index length is: 360\n",
      "intersection index length is: 256\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 180\n",
      "test index length is: 182\n",
      "intersection index length is: 171\n",
      "---\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# for file in ['day7', 'day8', 'day9','test']:\n",
    "file = 'day7'\n",
    "# print(file)\n",
    "#     temp = intersec_category(df_dict[file], df_dict['test'], df_dict[file],col_list)\n",
    "print('val day7')\n",
    "temp = intersec_category(pd.concat([df_dict['day8'],df_dict['day9']]), df_dict['day7'], df_dict[file],col_list)\n",
    "print('========================================')\n",
    "print('val day8')\n",
    "temp = intersec_category(pd.concat([df_dict['day7'],df_dict['day9']]), df_dict['day8'], df_dict[file],col_list)\n",
    "print('========================================')\n",
    "print('val day9')\n",
    "temp = intersec_category(pd.concat([df_dict['day7'],df_dict['day8']]), df_dict['day9'], df_dict[file],col_list)\n",
    "print('========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: app\n",
      "train index length is: 493\n",
      "test index length is: 417\n",
      "intersection index length is: 363\n",
      "---\n",
      "processing: device\n",
      "train index length is: 2111\n",
      "test index length is: 1985\n",
      "intersection index length is: 1357\n",
      "---\n",
      "processing: os\n",
      "train index length is: 437\n",
      "test index length is: 395\n",
      "intersection index length is: 291\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 191\n",
      "test index length is: 178\n",
      "intersection index length is: 176\n",
      "---\n",
      "processing: app\n",
      "train index length is: 493\n",
      "test index length is: 417\n",
      "intersection index length is: 363\n",
      "---\n",
      "processing: device\n",
      "train index length is: 2111\n",
      "test index length is: 1985\n",
      "intersection index length is: 1357\n",
      "---\n",
      "processing: os\n",
      "train index length is: 437\n",
      "test index length is: 395\n",
      "intersection index length is: 291\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 191\n",
      "test index length is: 178\n",
      "intersection index length is: 176\n",
      "---\n",
      "processing: app\n",
      "train index length is: 493\n",
      "test index length is: 417\n",
      "intersection index length is: 363\n",
      "---\n",
      "processing: device\n",
      "train index length is: 2111\n",
      "test index length is: 1985\n",
      "intersection index length is: 1357\n",
      "---\n",
      "processing: os\n",
      "train index length is: 437\n",
      "test index length is: 395\n",
      "intersection index length is: 291\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 191\n",
      "test index length is: 178\n",
      "intersection index length is: 176\n",
      "---\n",
      "processing: app\n",
      "train index length is: 493\n",
      "test index length is: 417\n",
      "intersection index length is: 363\n",
      "---\n",
      "processing: device\n",
      "train index length is: 2111\n",
      "test index length is: 1985\n",
      "intersection index length is: 1357\n",
      "---\n",
      "processing: os\n",
      "train index length is: 437\n",
      "test index length is: 395\n",
      "intersection index length is: 291\n",
      "---\n",
      "processing: channel\n",
      "train index length is: 191\n",
      "test index length is: 178\n",
      "intersection index length is: 176\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def intersec_category(df_train_all, df_test_all, df, col_list):\n",
    "    for col in col_list:\n",
    "        print('processing: {}'.format(col))\n",
    "        train_index = set(df_train_all[col].value_counts().index)\n",
    "        test_index = set(df_test_all[col].value_counts().index)                 \n",
    "        inter_index = list(train_index.intersection(test_index))\n",
    "        print('train index length is: {}'.format(len(train_index)))\n",
    "        print('test index length is: {}'.format(len(test_index)))\n",
    "        print('intersection index length is: {}'.format(len(inter_index)))\n",
    "        index_map = pd.Series(inter_index, index=inter_index)\n",
    "        df.loc[:,col] = df[col].map(index_map).fillna(-1)\n",
    "        print('---')\n",
    "    return df\n",
    "\n",
    "col_list = [ 'app', 'device', 'os', 'channel']\n",
    "df_train_all = pd.concat([df_dict[day_list[0]],df_dict[day_list[1]], df_dict[day_list[2]]])\n",
    "# for file in ['day7', 'day8', 'day9','test']: \n",
    "for file in ['day7', 'day8', 'day9','test']:\n",
    "#     df_dict[file] = intersec_category(df_train_all, df_dict['test'], df_dict[file],col_list)\n",
    "    tmp = intersec_category(df_train_all, df_dict['test'], df_dict[file],col_list)\n",
    "    \n",
    "del df_train_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neuron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn(emb_n, dense_n, batch_size, epochs, df_train, num_col_shape, max_length):\n",
    "\n",
    "    \n",
    "\n",
    "    in_app = Input(shape=[1], name = 'app')\n",
    "    emb_app = Embedding(max_length['app'], emb_n)(in_app)\n",
    "    in_channel = Input(shape=[1], name = 'channel')\n",
    "    emb_channel = Embedding(max_length['channel'], emb_n)(in_channel)\n",
    "    in_os = Input(shape=[1], name = 'os')\n",
    "    emb_os = Embedding(max_length['os'], emb_n)(in_os)\n",
    "    in_device = Input(shape=[1], name = 'device')\n",
    "    emb_device = Embedding(max_length['device'], emb_n)(in_device)\n",
    "    in_hour= Input(shape=[1], name = 'hour')\n",
    "    emb_hour = Embedding(max_length['hour'], emb_n)(in_hour)\n",
    "    \n",
    "    ### numeric input shape\n",
    "    in_num= Input(shape=[num_col_shape], name = 'nc')\n",
    "    \n",
    "    fe = concatenate([(emb_app), (emb_channel), (emb_os), (emb_device), (emb_hour)])\n",
    "    s_dout = SpatialDropout1D(0.2)(fe)\n",
    "    fl1 = Flatten()(s_dout)\n",
    "    conv = Conv1D(10, kernel_size=4, strides=1, padding='same')(s_dout)\n",
    "    fl2 = Flatten()(conv)\n",
    "    \n",
    "    \n",
    "#     f1_dense = Dropout(0.2)(Dense(10,activation='relu')(fl1))\n",
    "#     f1_dense = Dropout(0.2)(Dense(10,activation='relu')(f1_dense))  # categori - dense \n",
    "    f1_dense = Dropout(0.2)(Dense(100,activation='relu')(fl1))  # categori - dense - layers cannot be deep\n",
    "    \n",
    "    f2_dense = Dropout(0.2)(Dense(100,activation='relu')(fl2))\n",
    "#     f2_dense = Dropout(0.2)(Dense(10,activation='relu')(f2_dense)) # categori - convolution\n",
    "#     f2_dense = Dropout(0.2)(Dense(10,activation='relu')(f2_dense)) # categori - convolution - layers cannot be deep\n",
    "    \n",
    "    fnu_dense = Dropout(0.2)(Dense(100,activation='relu')(in_num))\n",
    "    fnu_dense = Dropout(0.2)(Dense(50,activation='relu')(fnu_dense))\n",
    "    fnu_dense = Dropout(0.2)(Dense(5,activation='relu')(fnu_dense)) # numeric - dense\n",
    "    \n",
    "#     concat = concatenate([(fl1), (fl2), (in_num),])\n",
    "    concat = concatenate([(f1_dense), (f2_dense), ])\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Dropout(0.2)(Dense(dense_n,activation='relu')(concat))\n",
    "    x = Dropout(0.2)(Dense(dense_n,activation='relu')(x))\n",
    "    \n",
    "    x_concat = concatenate([(x), (fnu_dense), ])\n",
    "\n",
    "    outp = Dense(1,activation='sigmoid')(x_concat)\n",
    "\n",
    "    input_list = [in_app, in_channel, in_os, in_device, in_hour, in_num]\n",
    "    model = Model(inputs=input_list, outputs=outp)\n",
    "\n",
    "    \n",
    "    \n",
    "    exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "    steps = int(len(df_train['app']) / batch_size) * epochs\n",
    "    lr_init, lr_fin = 0.002, 0.0002\n",
    "    lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "#     optimizer_adam = Adam(lr=0.002, decay=lr_decay)\n",
    "    optimizer_adam = Adam(lr=0.002)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer_adam, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app': 9, 'channel': 8, 'device': 11, 'hour': 4, 'os': 9}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebd_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn(emb_n, dense_n, batch_size, epochs, df_train, num_col_shape, max_length, ebd_length):\n",
    "\n",
    "    print(ebd_length)\n",
    "\n",
    "    in_app = Input(shape=[1], name = 'app')\n",
    "    emb_app = Embedding(max_length['app'], ebd_length['app'])(in_app)\n",
    "    in_channel = Input(shape=[1], name = 'channel')\n",
    "    emb_channel = Embedding(max_length['channel'], ebd_length['channel'])(in_channel)\n",
    "    in_os = Input(shape=[1], name = 'os')\n",
    "    emb_os = Embedding(max_length['os'], ebd_length['os'])(in_os)\n",
    "    in_device = Input(shape=[1], name = 'device')\n",
    "    emb_device = Embedding(max_length['device'], ebd_length['device'])(in_device)\n",
    "    in_hour= Input(shape=[1], name = 'hour')\n",
    "    emb_hour = Embedding(max_length['hour'], ebd_length['hour'])(in_hour)\n",
    "    \n",
    "    ### numeric input shape\n",
    "    in_num= Input(shape=[num_col_shape], name = 'nc')\n",
    "    \n",
    "    fe = concatenate([(emb_app), (emb_channel), (emb_os), (emb_device), (emb_hour)])\n",
    "    s_dout = SpatialDropout1D(0.2)(fe)\n",
    "    fl1 = Flatten()(s_dout)\n",
    "    conv = Conv1D(10, kernel_size=4, strides=1, padding='same')(s_dout)\n",
    "    fl2 = Flatten()(conv)\n",
    "    \n",
    "    \n",
    "#     f1_dense = Dropout(0.2)(Dense(10,activation='relu')(fl1))\n",
    "#     f1_dense = Dropout(0.2)(Dense(10,activation='relu')(f1_dense))  # categori - dense \n",
    "    f1_dense = Dropout(0.2)(Dense(10,activation='relu')(fl1))  # categori - dense - layers cannot be deep\n",
    "    \n",
    "    f2_dense = Dropout(0.2)(Dense(10,activation='relu')(fl2))\n",
    "#     f2_dense = Dropout(0.2)(Dense(10,activation='relu')(f2_dense)) # categori - convolution\n",
    "#     f2_dense = Dropout(0.2)(Dense(10,activation='relu')(f2_dense)) # categori - convolution - layers cannot be deep\n",
    "    \n",
    "#     fnu_dense = Dropout(0.2)(Dense(100,activation='relu')(in_num))\n",
    "#     fnu_dense = Dropout(0.2)(Dense(50,activation='relu')(fnu_dense))\n",
    "#     fnu_dense = Dropout(0.2)(Dense(5,activation='relu')(fnu_dense)) # numeric - dense\n",
    "    \n",
    "#     concat = concatenate([(fl1), (fl2), (in_num),])\n",
    "#     concat = concatenate([(f1_dense), (f2_dense), ])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     x = Dropout(0.2)(Dense(dense_n,activation='relu')(concat))\n",
    "    x = Dropout(0.2)(Dense(dense_n,activation='relu')(f1_dense))\n",
    "    x = Dropout(0.2)(Dense(dense_n,activation='relu')(x))\n",
    "    \n",
    "#     x_concat = concatenate([(x),  ])\n",
    "\n",
    "    outp = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    input_list = [in_app, in_channel, in_os, in_device, in_hour, in_num]\n",
    "    model = Model(inputs=input_list, outputs=outp)\n",
    "\n",
    "    \n",
    "    \n",
    "    exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "    steps = int(len(df_train['app']) / batch_size) * epochs\n",
    "    lr_init, lr_fin = 0.002, 0.0002\n",
    "    lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "#     optimizer_adam = Adam(lr=0.002, decay=lr_decay)\n",
    "    optimizer_adam = Adam(lr=0.002)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer_adam, metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing app\n",
      "processing device\n",
      "processing os\n",
      "processing channel\n",
      "processing hour\n",
      "get encoder done\n",
      "apply encoder to col: channel\n",
      "apply encoder to col: hour\n",
      "apply encoder to col: device\n",
      "apply encoder to col: os\n",
      "apply encoder to col: app\n",
      "apply encoder to col: channel\n",
      "apply encoder to col: hour\n",
      "apply encoder to col: device\n",
      "apply encoder to col: os\n",
      "apply encoder to col: app\n",
      "apply encoder to col: channel\n",
      "apply encoder to col: hour\n",
      "apply encoder to col: device\n",
      "apply encoder to col: os\n",
      "apply encoder to col: app\n",
      "apply encoder done!\n",
      "get keras data done\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "#1. get encoder\n",
    "df_all = pd.concat([df_dict['day7'], df_dict['day8'],df_dict['day9'],df_dict['test']])\n",
    "encoder = get_encoder(df_all, categorical_col)\n",
    "del df_all\n",
    "gc.collect()\n",
    "print('get encoder done')\n",
    "\n",
    "#. 2 apply encoder to trainset\n",
    "\n",
    "df_train = pd.concat([df_dict['day8'], df_dict['day9']])\n",
    "y_train = df_train[target].values\n",
    "df_val = df_dict['day7']\n",
    "y_val = df_val[target].values\n",
    "df_train= apply_encoder(df_train, encoder)\n",
    "df_val= apply_encoder(df_val, encoder)\n",
    "df_test = df_dict['test']\n",
    "df_test = apply_encoder(df_test, encoder)\n",
    "print('apply encoder done!')\n",
    "\n",
    "#. 3 get keras data\n",
    "df_train = get_keras_data(df_train, numeric_col)\n",
    "df_val = get_keras_data(df_val, numeric_col)\n",
    "df_test = get_keras_data(df_test, numeric_col)\n",
    "print('get keras data done')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ebd_length(count_dict):\n",
    "    ebd_length = {}\n",
    "    for cat in count_dict:\n",
    "        ebd_length[cat] = int(np.ceil(np.log2(count_dict[cat])))\n",
    "    return ebd_length\n",
    "unique_count ={'app': 382, 'device':1557, 'os':322, 'channel':172, 'hour':9}\n",
    "ebd_length = get_ebd_length(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'channel': 8, 'hour': 4, 'app': 9, 'os': 9, 'device': 11}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "app (InputLayer)                (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "channel (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "os (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "device (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hour (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_36 (Embedding)        (None, 1, 9)         3357        app[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_37 (Embedding)        (None, 1, 8)         1432        channel[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_38 (Embedding)        (None, 1, 9)         2709        os[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_39 (Embedding)        (None, 1, 11)        15818       device[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_40 (Embedding)        (None, 1, 4)         36          hour[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1, 41)        0           embedding_36[0][0]               \n",
      "                                                                 embedding_37[0][0]               \n",
      "                                                                 embedding_38[0][0]               \n",
      "                                                                 embedding_39[0][0]               \n",
      "                                                                 embedding_40[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, 1, 41)        0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 41)           0           spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 10)           420         flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 10)           0           dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 100)          1100        dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 100)          0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 100)          10100       dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 100)          0           dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 1)            101         dropout_50[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 35,073\n",
      "Trainable params: 35,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "RocAuc evaluating batch size is: 100000\n",
      "Train on 41345165 samples, validate on 19534560 samples\n",
      "Epoch 1/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9857\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 10s 1us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.967597\n",
      "41345165/41345165 [==============================] - 93s 2us/step - loss: 0.2322 - acc: 0.9857 - val_loss: 0.0873 - val_acc: 0.9861\n",
      "Epoch 2/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1833 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.968486\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1834 - acc: 0.9875 - val_loss: 0.0762 - val_acc: 0.9874\n",
      "Epoch 3/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1817 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.968741\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1817 - acc: 0.9875 - val_loss: 0.0897 - val_acc: 0.9860\n",
      "Epoch 4/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 8s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.968895\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1807 - acc: 0.9875 - val_loss: 0.0782 - val_acc: 0.9851\n",
      "Epoch 5/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1800 - acc: 0.9874\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.969251\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1800 - acc: 0.9874 - val_loss: 0.0727 - val_acc: 0.9873\n",
      "Epoch 6/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.969251\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1794 - acc: 0.9875 - val_loss: 0.0726 - val_acc: 0.9856\n",
      "Epoch 7/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1794 - acc: 0.9874\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.969153\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1793 - acc: 0.9874 - val_loss: 0.0713 - val_acc: 0.9864\n",
      "Epoch 8/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9874\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.969361\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1788 - acc: 0.9874 - val_loss: 0.0791 - val_acc: 0.9857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9874\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.969240\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1788 - acc: 0.9874 - val_loss: 0.0763 - val_acc: 0.9851\n",
      "Epoch 10/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1787 - acc: 0.9874\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.969450\n",
      "41345165/41345165 [==============================] - 76s 2us/step - loss: 0.1787 - acc: 0.9874 - val_loss: 0.0774 - val_acc: 0.9868\n",
      "Epoch 11/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 8s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 11 - score: 0.969603\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1783 - acc: 0.9875 - val_loss: 0.0799 - val_acc: 0.9851\n",
      "Epoch 12/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 12 - score: 0.969648\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1783 - acc: 0.9875 - val_loss: 0.0744 - val_acc: 0.9865\n",
      "Epoch 13/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 13 - score: 0.969548\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1781 - acc: 0.9875 - val_loss: 0.0649 - val_acc: 0.9869\n",
      "Epoch 14/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1779 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 8s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 14 - score: 0.969698\n",
      "41345165/41345165 [==============================] - 79s 2us/step - loss: 0.1779 - acc: 0.9876 - val_loss: 0.0754 - val_acc: 0.9866\n",
      "Epoch 15/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 15 - score: 0.969926\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1781 - acc: 0.9876 - val_loss: 0.0706 - val_acc: 0.9866\n",
      "Epoch 16/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9874\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 16 - score: 0.969811\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1777 - acc: 0.9874 - val_loss: 0.0693 - val_acc: 0.9872\n",
      "Epoch 17/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1775 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 17 - score: 0.969858\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1775 - acc: 0.9876 - val_loss: 0.0733 - val_acc: 0.9858\n",
      "Epoch 18/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 18 - score: 0.969733\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1778 - acc: 0.9875 - val_loss: 0.0700 - val_acc: 0.9863\n",
      "Epoch 19/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1778 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 19 - score: 0.969835\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1778 - acc: 0.9875 - val_loss: 0.0755 - val_acc: 0.9856\n",
      "Epoch 20/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 20 - score: 0.969774\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1776 - acc: 0.9875 - val_loss: 0.0740 - val_acc: 0.9862\n",
      "Epoch 21/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 8s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 21 - score: 0.970154\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1773 - acc: 0.9876 - val_loss: 0.0686 - val_acc: 0.9863\n",
      "Epoch 22/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 22 - score: 0.969797\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1774 - acc: 0.9876 - val_loss: 0.0791 - val_acc: 0.9854\n",
      "Epoch 23/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1774 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 23 - score: 0.969742\n",
      "41345165/41345165 [==============================] - 76s 2us/step - loss: 0.1775 - acc: 0.9876 - val_loss: 0.0760 - val_acc: 0.9863\n",
      "Epoch 24/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 8s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 24 - score: 0.969974\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1773 - acc: 0.9876 - val_loss: 0.0772 - val_acc: 0.9861\n",
      "Epoch 25/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9875\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 25 - score: 0.969839\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1771 - acc: 0.9875 - val_loss: 0.0737 - val_acc: 0.9854\n",
      "Epoch 26/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1772 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 26 - score: 0.969852\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1771 - acc: 0.9876 - val_loss: 0.0794 - val_acc: 0.9849\n",
      "Epoch 27/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 8s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 27 - score: 0.969907\n",
      "41345165/41345165 [==============================] - 79s 2us/step - loss: 0.1770 - acc: 0.9876 - val_loss: 0.0690 - val_acc: 0.9869\n",
      "Epoch 28/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 28 - score: 0.969671\n",
      "41345165/41345165 [==============================] - 77s 2us/step - loss: 0.1770 - acc: 0.9876 - val_loss: 0.0856 - val_acc: 0.9860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 29 - score: 0.969918\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1770 - acc: 0.9876 - val_loss: 0.0753 - val_acc: 0.9853\n",
      "Epoch 30/30\n",
      "41300000/41345165 [============================>.] - ETA: 0s - loss: 0.1770 - acc: 0.9876\n",
      " on epoch end, start predicting validation set\n",
      "19534560/19534560 [==============================] - 7s 0us/step\n",
      "\n",
      " start calculating ROC-AUC\n",
      "\n",
      " ROC-AUC - epoch: 30 - score: 0.969964\n",
      "41345165/41345165 [==============================] - 78s 2us/step - loss: 0.1770 - acc: 0.9876 - val_loss: 0.0689 - val_acc: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f591dbc9780>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100000\n",
    "epochs = 30\n",
    "emb_n = 25\n",
    "dense_n = 100\n",
    "# np.random.seed(2018)\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "num_col_shape = df_train['nc'].shape[1]\n",
    "max_length = max_input_length(encoder)\n",
    "model = get_nn(emb_n, dense_n, batch_size, epochs, df_train,num_col_shape, max_length, ebd_length)\n",
    "RocAuc = RocAucEvaluation(validation_data=(df_val, y_val), interval=1, batch_size=batch_size)\n",
    "# class_weight = {0:1,1:398.7} # magic\n",
    "class_weight = {0:1,1:99} # magic\n",
    "model.fit(df_train,\n",
    "          y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          class_weight=class_weight, \n",
    "          shuffle=True, \n",
    "          verbose=1, \n",
    "          callbacks = [RocAuc],\n",
    "          validation_data=(df_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print val auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19534560/19534560 [==============================] - 9s 0us/step\n",
      "validation set ROC: 0.8221294055910194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "pred_val = model.predict(df_val, batch_size=batch_size, verbose=1)\n",
    "score = roc_auc_score(y_val, pred_val)\n",
    "print('validation set ROC: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(df_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    " # prediction\n",
    "df_test_raw = pd.read_csv('/home/kai/data/kaggle/talkingdata/data/test.csv')\n",
    "print('loading file done!')\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['click_id'] = df_test_raw['click_id']\n",
    "df_sub['is_attributed'] = pred\n",
    "print('predicting file done!')\n",
    "df_sub.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/submission/equal_hour_{}{}_val_{}_matrixregV1_nn.csv.gz'.format(train_day[0],train_day[1],day), compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13646527],\n",
       "       [0.15837546],\n",
       "       [0.03275833],\n",
       "       ...,\n",
       "       [0.9134049 ],\n",
       "       [0.99874324],\n",
       "       [0.2917996 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
