{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### load training index\n",
    "indexfile = '/home/kai/data/kaggle/talkingdata/wl/data/trainset/train_index.npy'\n",
    "train_index = np.load(indexfile)\n",
    "\n",
    "### load feature cols\n",
    "import json as js\n",
    "featurefile = '/home/kai/data/kaggle/talkingdata/wl/data/features/feature_cols.json'\n",
    "with open(featurefile, 'r') as myjs:\n",
    "    added_features = js.load(myjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/kai/data/kaggle/talkingdata/data/'\n",
    "train = pd.read_csv(path + 'train_cleaned_final.csv')\n",
    "test = pd.read_csv(path + 'test_cleaned_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_index = list(set(range(len(train))) - set(train_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.154054783284664\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(train)/ 1024 **3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get history and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_history = train.iloc[history_index].copy()\n",
    "df_train = train.iloc[train_index].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history size: 12.0613476857543, length is: 134903890\n",
      "train size: 4.470348380506039, length is: 50000000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('history size: {}, length is: {}'.format(sys.getsizeof(df_history)/ 1024 **3, len(df_history))   )\n",
    "print('train size: {}, length is: {}'.format(sys.getsizeof(df_train)/ 1024 **3, len(df_train))   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ip', 'app', 'device', 'os', 'channel', 'day', 'hour', 'timestamp',\n",
       "       'minute', 'second', 'is_attributed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = {}\n",
    "feature_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel',\n",
    "              'day',\n",
    "              'hour',\n",
    "              'minute',\n",
    "              'second']\n",
    "\n",
    "# feature_col = ['ip', \n",
    "#               'app', \n",
    "#               'device', \n",
    "#               'os', \n",
    "#               'channel']\n",
    "for col in feature_col:\n",
    "    orders[col] = 10 ** (int(np.log(max(train[col].max(),test[col].max() ) + 1) / np.log(10)) + 1)\n",
    "def get_group(df, cols):\n",
    "    \"\"\"\n",
    "    define an encoding method which can ganrantee the adding value will be unique.\n",
    "    eg: artist_name_composer will be a combination of (artist_name,composer) and the encoding will reflect the unqiue combination of those two\n",
    "    \"\"\"\n",
    "    group = df[cols[0]].copy()\n",
    "    for col in cols[1:]:\n",
    "        group = group * orders[col] + df[col]\n",
    "        \n",
    "    return group\n",
    "\n",
    "import gc\n",
    "# del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app': 1000,\n",
       " 'channel': 1000,\n",
       " 'day': 100,\n",
       " 'device': 10000,\n",
       " 'hour': 100,\n",
       " 'ip': 1000000,\n",
       " 'minute': 100,\n",
       " 'os': 1000,\n",
       " 'second': 100}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count\n",
    "plan 1. count from historical data  \n",
    "plan 2. count from all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(df_history, df_train, cols, target=None):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df.count the number of records for each feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "    \n",
    "    group = get_group(df_train, cols)\n",
    "    group_all = get_group(df_history, cols)\n",
    "    \n",
    "    count_map = group_all.value_counts()\n",
    "    \n",
    "    return group.map(count_map).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean\n",
    "mean P(target | feature combination)\n",
    "\n",
    "purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer))\n",
    "Get the conditional Probability only from historical data and apply to train data.\n",
    "\n",
    "P(replay | X feature combination) = P( replay & X feature combination) / P (X feature combination)  \n",
    "=(count(replay & X feature combination) / count(total)) / (count(X feature combination) / count(total))  \n",
    "= count(replay & X feature combination) / count(X feature combination)  \n",
    "= sum((replay & X feature combination)) / count(X feature combination)  \n",
    "= sum((replay or not replayed & X feature combination)) / count(X feature combination)# since replay is 1, not replay is 0  \n",
    "= sum( X feature combination) / count(X feature combination)  \n",
    "= mean(X feature combination)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaller(num):\n",
    "    sca = 1\n",
    "    while num * sca < 1:\n",
    "        sca *= 10\n",
    "    return sca\n",
    "\n",
    "def mean(df_history, df_train, cols, target):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "  \n",
    "    # encoding df's cols into a new series\n",
    "    group = get_group(df_train, cols)\n",
    "    # encoding df_history's cols into a new series\n",
    "    group_history = get_group(df_history, cols)\n",
    "    # get the conditional probability p(target| feature combination. eg, artist_name_composer) \n",
    "    mean_map = df_history.groupby(group_history)[target].mean()\n",
    "    # mean_map: key - encoding, value - target mean\n",
    "#     ### sca\n",
    "#     m_min = mean_map[mean_map > 0].min()\n",
    "#     sca = scaller(m_min)\n",
    "#     mean_map *= sca\n",
    "#     ###\n",
    "\n",
    "    return group.map(mean_map).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reversemean\n",
    "reverse mean P(feature combination | target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reversemean(df_history, df_train, cols, target):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "  \n",
    "    # encoding df's cols into a new series\n",
    "    group = get_group(df_train, cols)\n",
    "    # encoding df_history's cols into a new series\n",
    "    group_history = get_group(df_history, cols)\n",
    "    # get the conditional probability p(target| feature combination. eg, artist_name_composer) \n",
    "    positive = group_history[df_history[target] == 1]\n",
    "    negative = group_history[df_history[target] == 0]\n",
    "    index_p = set(positive.unique())\n",
    "    index_n = set(negative.unique())\n",
    "    index_n.difference_update(index_p)\n",
    "    map_reverse_p = positive.groupby(positive).count() / len(positive)\n",
    "    map_reverse_n = pd.Series(np.zeros(len(index_n)), index=index_n)\n",
    "    map_reverse = pd.concat([map_reverse_p, map_reverse_n])\n",
    "    return group.map(map_reverse).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1:   ip_mean   \t\t\t size: 4.84287741035223 G.\n",
      "test 1:   ip_mean   \t\t\t size: 1.5399990379810333 G.\n",
      "------\n",
      "train 2:   ip_app_mean   \t\t\t size: 5.2154064401984215 G.\n",
      "test 2:   ip_app_mean   \t\t\t size: 1.679998941719532 G.\n",
      "------\n",
      "train 3:   ip_channel_count   \t\t\t size: 5.587935470044613 G.\n",
      "test 3:   ip_channel_count   \t\t\t size: 1.8199988454580307 G.\n",
      "------\n",
      "train 4:   ip_channel_mean   \t\t\t size: 5.960464499890804 G.\n",
      "test 4:   ip_channel_mean   \t\t\t size: 1.9599987491965294 G.\n",
      "------\n",
      "train 5:   ip_minute_count   \t\t\t size: 6.332993529736996 G.\n",
      "test 5:   ip_minute_count   \t\t\t size: 2.099998652935028 G.\n",
      "------\n",
      "train 6:   ip_second_mean   \t\t\t size: 6.705522559583187 G.\n",
      "test 6:   ip_second_mean   \t\t\t size: 2.2399985566735268 G.\n",
      "------\n",
      "train 7:   device_minute_mean   \t\t\t size: 7.0780515894293785 G.\n",
      "test 7:   device_minute_mean   \t\t\t size: 2.3799984604120255 G.\n",
      "------\n",
      "train 8:   ip_app_device_mean   \t\t\t size: 7.45058061927557 G.\n",
      "test 8:   ip_app_device_mean   \t\t\t size: 2.519998364150524 G.\n",
      "------\n",
      "train 9:   ip_os_hour_count   \t\t\t size: 7.823109649121761 G.\n",
      "test 9:   ip_os_hour_count   \t\t\t size: 2.659998267889023 G.\n",
      "------\n",
      "train 10:   app_os_channel_mean   \t\t\t size: 8.195638678967953 G.\n",
      "test 10:   app_os_channel_mean   \t\t\t size: 2.7999981716275215 G.\n",
      "------\n",
      "train 11:   app_os_hour_mean   \t\t\t size: 8.568167708814144 G.\n",
      "test 11:   app_os_hour_mean   \t\t\t size: 2.93999807536602 G.\n",
      "------\n",
      "train 12:   app_channel_hour_mean   \t\t\t size: 8.940696738660336 G.\n",
      "test 12:   app_channel_hour_mean   \t\t\t size: 3.079997979104519 G.\n",
      "------\n",
      "train 13:   device_minute_second_mean   \t\t\t size: 9.313225768506527 G.\n",
      "test 13:   device_minute_second_mean   \t\t\t size: 3.2199978828430176 G.\n",
      "------\n",
      "train 14:   hour_minute_second_mean   \t\t\t size: 9.685754798352718 G.\n",
      "test 14:   hour_minute_second_mean   \t\t\t size: 3.3599977865815163 G.\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "combine_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel', \n",
    "              'day',\n",
    "              'hour',\n",
    "              'minute',\n",
    "              'second']\n",
    "\n",
    "def col_name(cols, func=None):\n",
    "    if func is None:\n",
    "        return '_'.join(cols)\n",
    "    else:\n",
    "        return '_'.join(cols) + '_' + func.__name__\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "\n",
    "for num_col in [1,2,3]:\n",
    "    for cols in combinations(combine_col, num_col):\n",
    "        for func in [count, mean]:\n",
    "            feature_name = col_name(cols, func=func)\n",
    "            if feature_name in added_features:\n",
    "                counter += 1\n",
    "#                 df_train[feature_name] = func(df_history, df_train, cols, target='is_attributed')\n",
    "#                 test[feature_name] = func(df_history, test, cols, target='is_attributed')\n",
    "                df_train[feature_name] = func(train, df_train, cols, target='is_attributed')\n",
    "                test[feature_name] = func(train, test, cols, target='is_attributed')\n",
    "                gc.collect()\n",
    "                train_str = 'train {}:   {}   \\t\\t\\t size: {} G.'.format(counter, feature_name, sys.getsizeof(df_train)/ 1024 **3)\n",
    "                test_str = 'test {}:   {}   \\t\\t\\t size: {} G.'.format(counter, feature_name, sys.getsizeof(test)/ 1024 **3)\n",
    "                print(train_str)\n",
    "                print(test_str)\n",
    "                print('------')\n",
    "                with open('feature_all.txt', 'w') as text_file:\n",
    "                    text_file.write(train_str + '\\n')\n",
    "                    text_file.write(test_str + '\\n')\n",
    "                    text_file.write('------' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training saving done!\n",
      "testing saving done!\n"
     ]
    }
   ],
   "source": [
    "target = 'is_attributed'\n",
    "train_col = []\n",
    "train_col = added_features.copy()\n",
    "train_col.append(target)\n",
    "\n",
    "df_train = df_train[train_col]\n",
    "test = test[added_features]\n",
    "df_train.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/features/train_all_5000k_17cols.csv', index=False)\n",
    "print('training saving done!')\n",
    "test.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/features/test_all_5000k_17cols.csv', index=False)\n",
    "print('testing saving done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
