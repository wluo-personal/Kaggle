{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/kai/data/kaggle/talkingdata/data/'\n",
    "train = pd.read_csv(path + 'train_cleaned_final.csv')\n",
    "test = pd.read_csv(path + 'test_cleaned_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### load feature cols\n",
    "import json as js\n",
    "featurefile = '/home/kai/data/kaggle/talkingdata/wl/data/features/feature_cols.json'\n",
    "with open(featurefile, 'r') as myjs:\n",
    "    added_features = js.load(myjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use last 65million as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = 65 * 1000000\n",
    "front = len(train) - length\n",
    "df_history = train.iloc[:front].copy()\n",
    "df_train = train.iloc[front:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.327165227383375\n",
      "65000000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(df_train)/ 1024 **3)\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ip', 'app', 'device', 'os', 'channel', 'day', 'hour', 'timestamp',\n",
       "       'minute', 'second', 'is_attributed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3213"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = {}\n",
    "feature_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel',\n",
    "              'day',\n",
    "              'hour',\n",
    "              'minute',\n",
    "              'second']\n",
    "\n",
    "# feature_col = ['ip', \n",
    "#               'app', \n",
    "#               'device', \n",
    "#               'os', \n",
    "#               'channel']\n",
    "for col in feature_col:\n",
    "    orders[col] = 10 ** (int(np.log(max(train[col].max(),test[col].max() ) + 1) / np.log(10)) + 1)\n",
    "def get_group(df, cols):\n",
    "    \"\"\"\n",
    "    define an encoding method which can ganrantee the adding value will be unique.\n",
    "    eg: artist_name_composer will be a combination of (artist_name,composer) and the encoding will reflect the unqiue combination of those two\n",
    "    \"\"\"\n",
    "    group = df[cols[0]].copy()\n",
    "    for col in cols[1:]:\n",
    "        group = group * orders[col] + df[col]\n",
    "        \n",
    "    return group\n",
    "\n",
    "import gc\n",
    "# del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app': 1000,\n",
       " 'channel': 1000,\n",
       " 'day': 100,\n",
       " 'device': 10000,\n",
       " 'hour': 100,\n",
       " 'ip': 1000000,\n",
       " 'minute': 100,\n",
       " 'os': 1000,\n",
       " 'second': 100}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count\n",
    "plan 1. count from historical data  \n",
    "plan 2. count from all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(df_history, df_train, cols, target=None):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df.count the number of records for each feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "    \n",
    "    group = get_group(df_train, cols)\n",
    "    group_all = get_group(df_history, cols)\n",
    "    \n",
    "    count_map = group_all.value_counts()\n",
    "    \n",
    "    return group.map(count_map).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean\n",
    "mean P(target | feature combination)\n",
    "\n",
    "purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer))\n",
    "Get the conditional Probability only from historical data and apply to train data.\n",
    "\n",
    "P(replay | X feature combination) = P( replay & X feature combination) / P (X feature combination)  \n",
    "=(count(replay & X feature combination) / count(total)) / (count(X feature combination) / count(total))  \n",
    "= count(replay & X feature combination) / count(X feature combination)  \n",
    "= sum((replay & X feature combination)) / count(X feature combination)  \n",
    "= sum((replay or not replayed & X feature combination)) / count(X feature combination)# since replay is 1, not replay is 0  \n",
    "= sum( X feature combination) / count(X feature combination)  \n",
    "= mean(X feature combination)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaller(num):\n",
    "    sca = 1\n",
    "    while num * sca < 1:\n",
    "        sca *= 10\n",
    "    return sca\n",
    "\n",
    "def mean(df_history, df_train, cols, target):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "  \n",
    "    # encoding df's cols into a new series\n",
    "    group = get_group(df_train, cols)\n",
    "    # encoding df_history's cols into a new series\n",
    "    group_history = get_group(df_history, cols)\n",
    "    # get the conditional probability p(target| feature combination. eg, artist_name_composer) \n",
    "    mean_map = df_history.groupby(group_history)[target].mean()\n",
    "    # mean_map: key - encoding, value - target mean\n",
    "#     ### sca\n",
    "#     m_min = mean_map[mean_map > 0].min()\n",
    "#     sca = scaller(m_min)\n",
    "#     mean_map *= sca\n",
    "#     ###\n",
    "\n",
    "    return group.map(mean_map).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reversemean\n",
    "reverse mean P(feature combination | target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reversemean(df_history, df_train, cols, target):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "  \n",
    "    # encoding df's cols into a new series\n",
    "    group = get_group(df_train, cols)\n",
    "    # encoding df_history's cols into a new series\n",
    "    group_history = get_group(df_history, cols)\n",
    "    # get the conditional probability p(target| feature combination. eg, artist_name_composer) \n",
    "    positive = group_history[df_history[target] == 1]\n",
    "    negative = group_history[df_history[target] == 0]\n",
    "    index_p = set(positive.unique())\n",
    "    index_n = set(negative.unique())\n",
    "    index_n.difference_update(index_p)\n",
    "    map_reverse_p = positive.groupby(positive).count() / len(positive)\n",
    "    map_reverse_n = pd.Series(np.zeros(len(index_n)), index=index_n)\n",
    "    map_reverse = pd.concat([map_reverse_p, map_reverse_n])\n",
    "    return group.map(map_reverse).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate all cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1:   ip_mean   \t\t\t size: 5.811452966183424 G.\n",
      "test 1:   ip_mean   \t\t\t size: 1.5399990379810333 G.\n",
      "------\n",
      "train 2:   ip_app_mean   \t\t\t size: 6.295740704983473 G.\n",
      "test 2:   ip_app_mean   \t\t\t size: 1.679998941719532 G.\n",
      "------\n",
      "train 3:   ip_channel_count   \t\t\t size: 6.780028443783522 G.\n",
      "test 3:   ip_channel_count   \t\t\t size: 1.8199988454580307 G.\n",
      "------\n",
      "train 4:   ip_channel_mean   \t\t\t size: 7.2643161825835705 G.\n",
      "test 4:   ip_channel_mean   \t\t\t size: 1.9599987491965294 G.\n",
      "------\n",
      "train 5:   ip_minute_count   \t\t\t size: 7.748603921383619 G.\n",
      "test 5:   ip_minute_count   \t\t\t size: 2.099998652935028 G.\n",
      "------\n",
      "train 6:   ip_second_mean   \t\t\t size: 8.232891660183668 G.\n",
      "test 6:   ip_second_mean   \t\t\t size: 2.2399985566735268 G.\n",
      "------\n",
      "train 7:   device_minute_mean   \t\t\t size: 8.717179398983717 G.\n",
      "test 7:   device_minute_mean   \t\t\t size: 2.3799984604120255 G.\n",
      "------\n",
      "train 8:   ip_app_device_mean   \t\t\t size: 9.201467137783766 G.\n",
      "test 8:   ip_app_device_mean   \t\t\t size: 2.519998364150524 G.\n",
      "------\n",
      "train 9:   ip_os_hour_count   \t\t\t size: 9.685754876583815 G.\n",
      "test 9:   ip_os_hour_count   \t\t\t size: 2.659998267889023 G.\n",
      "------\n",
      "train 10:   app_os_channel_mean   \t\t\t size: 10.170042615383863 G.\n",
      "test 10:   app_os_channel_mean   \t\t\t size: 2.7999981716275215 G.\n",
      "------\n",
      "train 11:   app_os_hour_mean   \t\t\t size: 10.654330354183912 G.\n",
      "test 11:   app_os_hour_mean   \t\t\t size: 2.93999807536602 G.\n",
      "------\n",
      "train 12:   app_channel_hour_mean   \t\t\t size: 11.138618092983961 G.\n",
      "test 12:   app_channel_hour_mean   \t\t\t size: 3.079997979104519 G.\n",
      "------\n",
      "train 13:   device_minute_second_mean   \t\t\t size: 11.62290583178401 G.\n",
      "test 13:   device_minute_second_mean   \t\t\t size: 3.2199978828430176 G.\n",
      "------\n",
      "train 14:   hour_minute_second_mean   \t\t\t size: 12.107193570584059 G.\n",
      "test 14:   hour_minute_second_mean   \t\t\t size: 3.3599977865815163 G.\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "combine_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel', \n",
    "              'day',\n",
    "              'hour',\n",
    "              'minute',\n",
    "              'second']\n",
    "\n",
    "def col_name(cols, func=None):\n",
    "    if func is None:\n",
    "        return '_'.join(cols)\n",
    "    else:\n",
    "        return '_'.join(cols) + '_' + func.__name__\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "\n",
    "for num_col in [1,2,3]:\n",
    "    for cols in combinations(combine_col, num_col):\n",
    "        for func in [count, mean]:\n",
    "            feature_name = col_name(cols, func=func)\n",
    "            if feature_name in added_features:\n",
    "                counter += 1\n",
    "                df_train[feature_name] = func(df_history, df_train, cols, target='is_attributed')\n",
    "                test[feature_name] = func(df_history, test, cols, target='is_attributed')\n",
    "                gc.collect()\n",
    "                train_str = 'train {}:   {}   \\t\\t\\t size: {} G.'.format(counter, feature_name, sys.getsizeof(df_train)/ 1024 **3)\n",
    "                test_str = 'test {}:   {}   \\t\\t\\t size: {} G.'.format(counter, feature_name, sys.getsizeof(test)/ 1024 **3)\n",
    "                print(train_str)\n",
    "                print(test_str)\n",
    "                print('------')\n",
    "                with open('feature_all.txt', 'w') as text_file:\n",
    "                    text_file.write(train_str + '\\n')\n",
    "                    text_file.write(test_str + '\\n')\n",
    "                    text_file.write('------' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-processing version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build kwargs done!\n",
      "thread start !!!!!!!\n",
      "thread start !!!!!!!\n",
      "thread start !!!!!!!\n",
      "ip_count\n",
      "app_channel_minute_count\n",
      "hour_second_count\n",
      "hour_second_mean\n",
      "app_channel_minute_mean\n",
      "ip_mean\n",
      "minute_second_count\n",
      "app_channel_second_count\n",
      "minute_second_mean\n",
      "app_channel_second_mean\n",
      "app_count\n",
      "ip_app_device_count\n",
      "app_mean\n",
      "app_day_hour_count\n",
      "device_count\n",
      "app_day_hour_mean\n",
      "device_mean\n",
      "app_day_minute_count\n",
      "ip_app_device_mean\n",
      "os_count\n",
      "app_day_minute_mean\n",
      "os_mean\n",
      "channel_count\n",
      "app_day_second_count\n",
      "channel_mean\n",
      "app_day_second_mean\n",
      "ip_app_os_count\n",
      "day_count\n",
      "app_hour_minute_count\n",
      "day_mean\n",
      "app_hour_minute_mean\n",
      "hour_count\n",
      "hour_mean\n",
      "app_hour_second_count\n",
      "ip_app_os_mean\n",
      "minute_count\n",
      "app_hour_second_mean\n",
      "minute_mean\n",
      "second_count\n",
      "app_minute_second_count\n",
      "second_mean\n",
      "app_minute_second_mean\n",
      "ip_app_count\n",
      "device_os_channel_count\n",
      "ip_app_channel_count\n",
      "device_os_channel_mean\n",
      "ip_app_mean\n",
      "device_os_day_count\n",
      "device_os_day_mean\n",
      "ip_app_channel_mean\n",
      "device_os_hour_count\n",
      "ip_device_count\n",
      "device_os_hour_mean\n",
      "ip_device_mean\n",
      "device_os_minute_count\n",
      "device_os_minute_mean\n",
      "ip_os_count\n",
      "ip_app_day_count\n",
      "device_os_second_count\n",
      "device_os_second_mean\n",
      "ip_os_mean\n",
      "device_channel_day_count\n",
      "ip_app_day_mean\n",
      "device_channel_day_mean\n",
      "device_channel_hour_count\n",
      "ip_channel_count\n",
      "device_channel_hour_mean\n",
      "device_channel_minute_count\n",
      "ip_app_hour_count\n",
      "device_channel_minute_mean\n",
      "ip_channel_mean\n",
      "device_channel_second_count\n",
      "ip_app_hour_mean\n",
      "device_channel_second_mean\n",
      "device_day_hour_count\n",
      "ip_day_count\n",
      "device_day_hour_mean\n",
      "device_day_minute_count\n",
      "ip_day_mean\n",
      "device_day_minute_mean\n",
      "ip_app_minute_count\n",
      "ip_hour_count\n",
      "device_day_second_count\n",
      "device_day_second_mean\n",
      "ip_hour_mean\n",
      "device_hour_minute_count\n",
      "ip_app_minute_mean\n",
      "device_hour_minute_mean\n",
      "ip_minute_count\n",
      "device_hour_second_count\n",
      "device_hour_second_mean\n",
      "ip_minute_mean\n",
      "device_minute_second_count\n",
      "device_minute_second_mean\n",
      "os_channel_day_count\n",
      "ip_second_count\n",
      "ip_app_second_count\n",
      "os_channel_day_mean\n",
      "os_channel_hour_count\n",
      "ip_second_mean\n",
      "os_channel_hour_mean\n",
      "ip_app_second_mean\n",
      "os_channel_minute_count\n",
      "os_channel_minute_mean\n",
      "app_device_count\n",
      "app_device_mean\n",
      "os_channel_second_count\n",
      "app_os_count\n",
      "app_os_mean\n",
      "os_channel_second_mean\n",
      "app_channel_count\n",
      "ip_device_os_count\n",
      "app_channel_mean\n",
      "os_day_hour_count\n",
      "app_day_count\n",
      "os_day_hour_mean\n",
      "app_day_mean\n",
      "ip_device_os_mean\n",
      "os_day_minute_count\n",
      "app_hour_count\n",
      "app_hour_mean\n",
      "os_day_minute_mean\n",
      "ip_device_channel_count\n",
      "app_minute_count\n",
      "os_day_second_count\n",
      "app_minute_mean\n",
      "os_day_second_mean\n",
      "app_second_count\n",
      "os_hour_minute_count\n",
      "ip_device_channel_mean\n",
      "app_second_mean\n",
      "os_hour_minute_mean\n",
      "device_os_count\n",
      "os_hour_second_count\n",
      "device_os_mean\n",
      "os_hour_second_mean\n",
      "device_channel_count\n",
      "os_minute_second_count\n",
      "device_channel_mean\n",
      "ip_device_day_count\n",
      "os_minute_second_mean\n",
      "device_day_count\n",
      "ip_device_day_mean\n",
      "device_day_mean\n",
      "channel_day_hour_count\n",
      "device_hour_count\n",
      "device_hour_mean\n",
      "channel_day_hour_mean\n",
      "ip_device_hour_count\n",
      "device_minute_count\n",
      "channel_day_minute_count\n",
      "device_minute_mean\n",
      "ip_device_hour_mean\n",
      "channel_day_minute_mean\n",
      "device_second_count\n",
      "device_second_mean\n",
      "channel_day_second_count\n",
      "os_channel_count\n",
      "channel_day_second_mean\n",
      "ip_device_minute_count\n",
      "os_channel_mean\n",
      "channel_hour_minute_count\n",
      "os_day_count\n",
      "channel_hour_minute_mean\n",
      "ip_device_minute_mean\n",
      "os_day_mean\n",
      "channel_hour_second_count\n",
      "os_hour_count\n",
      "channel_hour_second_mean\n",
      "os_hour_mean\n",
      "ip_device_second_count\n",
      "channel_minute_second_count\n",
      "os_minute_count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-14-c973b4268b5b>\", line 49, in _process_each_col\n",
      "    result_dict[feature_name] = func(df_history, df_train, cols, target=target)\n",
      "  File \"<ipython-input-10-4fbe1539f76c>\", line 7, in count\n",
      "    group_all = get_group(df_history, cols)\n",
      "  File \"<ipython-input-8-6613b36fca9d>\", line 18, in get_group\n",
      "    group = df[cols[0]].copy()\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\", line 3432, in copy\n",
      "    data = self._data.copy(deep=deep)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 3436, in copy\n",
      "    do_integrity_check=False)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 3091, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 629, in copy\n",
      "    values = values.copy()\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-14-c973b4268b5b>\", line 49, in _process_each_col\n",
      "    result_dict[feature_name] = func(df_history, df_train, cols, target=target)\n",
      "  File \"<ipython-input-10-4fbe1539f76c>\", line 7, in count\n",
      "    group_all = get_group(df_history, cols)\n",
      "  File \"<ipython-input-8-6613b36fca9d>\", line 18, in get_group\n",
      "    group = df[cols[0]].copy()\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\", line 3432, in copy\n",
      "    data = self._data.copy(deep=deep)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 3436, in copy\n",
      "    do_integrity_check=False)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 3091, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 629, in copy\n",
      "    values = values.copy()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c973b4268b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-c973b4268b5b>\u001b[0m in \u001b[0;36mmultiprocessing_features\u001b[0;34m(df_history, df_train, target, combine_col, func_pool, comb_total, workers, mode)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'build kwargs done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_process_each_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature processing done!!!!!!!!!!!!!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-14-c973b4268b5b>\", line 49, in _process_each_col\n",
      "    result_dict[feature_name] = func(df_history, df_train, cols, target=target)\n",
      "  File \"<ipython-input-10-4fbe1539f76c>\", line 9, in count\n",
      "    count_map = group_all.value_counts()\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/base.py\", line 938, in value_counts\n",
      "    normalize=normalize, bins=bins, dropna=dropna)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\", line 640, in value_counts\n",
      "    keys, counts = _value_counts_arraylike(values, dropna)\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py\", line 685, in _value_counts_arraylike\n",
      "    keys, counts = f(values, dropna)\n",
      "  File \"pandas/_libs/hashtable_func_helper.pxi\", line 504, in pandas._libs.hashtable.value_count_int64\n",
      "  File \"pandas/_libs/hashtable_func_helper.pxi\", line 529, in pandas._libs.hashtable.value_count_int64\n",
      "  File \"/home/kai/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\", line 463, in asarray\n",
      "    def asarray(a, dtype=None, order=None):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "\n",
    "# ####################\n",
    "# from itertools import combinations\n",
    "\n",
    "\n",
    "# combine_col = ['ip', \n",
    "#               'app', \n",
    "#               'device', \n",
    "#               'os', \n",
    "#               'channel', \n",
    "#               'day',\n",
    "#               'hour',\n",
    "#               'minute',\n",
    "#               'second']\n",
    "\n",
    "# func_pool = [count, mean]\n",
    "\n",
    "# target = 'is_attributed'\n",
    "\n",
    "# def col_name(cols, func=None):\n",
    "#     if func is None:\n",
    "#         return '_'.join(cols)\n",
    "#     else:\n",
    "#         return '_'.join(cols) + '_' + func.__name__\n",
    "# ###################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def _process_each_col(kwargs_list):\n",
    "#     print('thread start !!!!!!!')\n",
    "#     result_dict = {}\n",
    "    \n",
    "#     for kwargs in kwargs_list:\n",
    "# #         df_train = kwargs.get('df_train')\n",
    "# #         df_history = kwargs.get('df_history')\n",
    "# #         target = kwargs.get('target')\n",
    "# #         df_train = df_train\n",
    "# #         df_history = df_history\n",
    "# #         target = target\n",
    "#         cols = kwargs.get('cols')\n",
    "#         func = kwargs.get('func')\n",
    "#         mode = kwargs.get('mode')\n",
    "#         feature_name = col_name(cols, func=func)\n",
    "#         print(feature_name)\n",
    "#         if mode.lower() == 'train_history':\n",
    "#             result_dict[feature_name] = func(df_history, df_train, cols, target=target)\n",
    "#         elif mode.lower() == 'train_all':\n",
    "#             result_dict[feature_name] = func(train, df_train, cols, target=target)\n",
    "#         elif mode.lower() == 'test_history':\n",
    "#             result_dict[feature_name] = func(df_history, test, cols, target=target)\n",
    "#         elif mode.lower() == 'test_all':\n",
    "#             result_dict[feature_name] = func(train, test, cols, target=target)\n",
    "#         else:\n",
    "#             print('known mode !!!!')\n",
    "#     return result_dict\n",
    "\n",
    "\n",
    "# # def build_kwargs(df_history, df_train, target, combine_col, func_pool, comb_total=3):\n",
    "# #     kwargs_pool = []\n",
    "# #     for num_col in range(1, comb_total + 1):\n",
    "# #         for cols in combinations(combine_col, num_col):\n",
    "# #             for func in func_pool:\n",
    "# #                 kwargs = {}\n",
    "# #                 kwargs['df_history'] = df_history\n",
    "# #                 kwargs['df_train'] = df_train\n",
    "# #                 kwargs['target'] = target\n",
    "# #                 kwargs['cols'] = cols\n",
    "# #                 kwargs['func'] = func\n",
    "# #                 kwargs_pool.append(kwargs)\n",
    "# #     return kwargs_pool\n",
    "\n",
    "# def build_kwargs( combine_col, func_pool, mode='train',comb_total=3):\n",
    "#     kwargs_pool = []\n",
    "#     for num_col in range(1, comb_total + 1):\n",
    "#         for cols in combinations(combine_col, num_col):\n",
    "#             for func in func_pool:\n",
    "#                 kwargs = {}\n",
    "#                 kwargs['cols'] = cols\n",
    "#                 kwargs['func'] = func\n",
    "#                 kwargs['mode'] = mode\n",
    "#                 kwargs_pool.append(kwargs)\n",
    "#     return kwargs_pool\n",
    "\n",
    "# def multiprocessing_features(df_history, df_train, target, combine_col, func_pool, comb_total=3, workers=3, mode='train_history'):\n",
    "#     kwargs_pool = build_kwargs(combine_col, func_pool, mode, comb_total=3)\n",
    "#     print('build kwargs done!')\n",
    "#     pool = multiprocessing.Pool(processes=workers)\n",
    "#     result = pool.map(_process_each_col, [kwargs for kwargs in np.array_split(kwargs_pool, workers)])\n",
    "#     pool.close()\n",
    "#     print('feature processing done!!!!!!!!!!!!!!')\n",
    "#     for each_thread in result:\n",
    "#         for key in each_thread:\n",
    "#             df_train[key] = each_thread[key]\n",
    "#     return df_train\n",
    "\n",
    "\n",
    "# df_train = multiprocessing_features(df_history, df_train, target, combine_col, func_pool, comb_total=3, workers=3, mode='train_history')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training saving done!\n",
      "testing saving done!\n"
     ]
    }
   ],
   "source": [
    "df_train.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/features/train_last6.5k.csv', index=False)\n",
    "print('training saving done!')\n",
    "test.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/features/test_last6.5k.csv', index=False)\n",
    "print('testing saving done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training saving done!\n",
      "testing saving done!\n"
     ]
    }
   ],
   "source": [
    "#For float saving\n",
    "\n",
    "\n",
    "# df_train.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/features/train_fold_last_in_12_mean_1float.csv', index=False, float_format='%.1f')\n",
    "# print('training saving done!')\n",
    "# test.to_csv('/home/kai/data/kaggle/talkingdata/wl/data/features/test_fold_last_in_12_mean_1float.csv', index=False, float_format='%.1f')\n",
    "# print('testing saving done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
