{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/kai/data/kaggle/talkingdata/data/'\n",
    "train = pd.read_csv(path + 'train_cleaned_final.csv')\n",
    "test = pd.read_csv(path + 'test_cleaned_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_path = '/home/kai/data/kaggle/talkingdata/wl/data/compensate/'\n",
    "comp_index = np.load(load_path+'all_compen.npy')\n",
    "df_comp = train.iloc[comp_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate channel, see if in intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pool = set(test.channel.value_counts().index).intersection(set(train.iloc[index['day7']+index['day8']+index['day9']].copy().channel.value_counts().index))\n",
    "\n",
    "# f = lambda x: x in pool\n",
    "# day7_ori= train.iloc[index['day7']].copy()\n",
    "# day7 = day7_ori[day7_ori.channel.apply(f)]\n",
    "\n",
    "# day8_ori= train.iloc[index['day8']].copy()\n",
    "# day8 = day8_ori[day8_ori.channel.apply(f)]\n",
    "\n",
    "# day9_ori= train.iloc[index['day9']].copy()\n",
    "# day9 = day9_ori[day9_ori.channel.apply(f)]\n",
    "\n",
    "# print(len(day7_ori))\n",
    "# print(len(day7))\n",
    "# print(len(day8_ori))\n",
    "# print(len(day8))\n",
    "# print(len(day9_ori))\n",
    "# print(len(day9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "feature_count =  [\n",
    "                    'ip_day_hour_count',\n",
    "                    'ip_os_day_hour_count',\n",
    "                    'ip_app_day_hour_count',\n",
    "                    'ip_app_os_day_hour_count',\n",
    "                    'app_day_hour_count',\n",
    "                    'ip_device_os_count',\n",
    "                    'ip_app_device_os_count']\n",
    "\n",
    "feature_mean = ['ip_device_os_mean',\n",
    "                'ip_app_device_os_mean', 'ip_app_device_mean', 'app_device_os_mean']\n",
    "\n",
    "# feature_reversemean = ['ip_device_os_reversemean',\n",
    "#                 'ip_app_device_os_reversemean', 'ip_reversemean']\n",
    "feature_reversemean = []\n",
    "\n",
    "feature_time2nextclick = ['ip_device_os_time2nextclick',\n",
    "                            'ip_app_device_os_time2nextclick', 'ip_app_device_time2nextclick', 'app_device_os_time2nextclick']\n",
    "\n",
    "feature_time2previousclick = ['ip_device_os_time2previousclick', \n",
    "                                'ip_app_device_os_time2previousclick', 'ip_app_device_time2previousclick', 'app_device_os_time2previousclick']\n",
    "    \n",
    "    \n",
    "feature_countfromfuture = ['ip_device_os_countfromfuture',\n",
    "                            'ip_app_device_os_countfromfuture', 'ip_app_device_countfromfuture', 'app_device_os_countfromfuture']\n",
    "\n",
    "feature_countfrompast = ['ip_device_os_countfrompast',\n",
    "                            'ip_app_device_os_countfrompast', 'ip_app_device_countfrompast', 'app_device_os_countfrompast']\n",
    "    \n",
    "feature_lasttimediff =  ['ip_device_os_lasttimediff',\n",
    "                             'ip_app_device_os_lasttimediff', 'ip_app_device_lasttimediff', 'app_device_os_lasttimediff']\n",
    "\n",
    "# feature_matrixfac = ['matrixFact_user_iposdeviceapp_item_device', \n",
    "#                      'matrixFact_user_iposdeviceapp_item_app','matrixFact_user_iposdeviceapp_item_channel']\n",
    "feature_matrixfac = [ 'matrixFact_user_iposdeviceapp_item_app', 'matrixFact_user_ip_item_appdeviceos']\n",
    "\n",
    "\n",
    "\n",
    "feature_var = ['ip_app_os_var_hour', 'ip_app_channel_var_day']\n",
    "feature_var = [] # best result need to add var\n",
    "feature_regression = ['ip_device_os_regression', 'ip_app_device_os_regression', 'ip_app_device_regression', 'app_device_os_regression']\n",
    "                         \n",
    "\n",
    "feature_ori = ['app', 'channel', 'device', 'os', 'hour']\n",
    "\n",
    "feature_cols = []\n",
    "added_feature = []\n",
    "\n",
    "added_feature.extend(feature_count)\n",
    "added_feature.extend(feature_mean)\n",
    "added_feature.extend(feature_reversemean)\n",
    "added_feature.extend(feature_time2nextclick)\n",
    "added_feature.extend(feature_time2previousclick)\n",
    "added_feature.extend(feature_countfromfuture)\n",
    "added_feature.extend(feature_countfrompast)\n",
    "added_feature.extend(feature_lasttimediff)\n",
    "added_feature.extend(feature_matrixfac)\n",
    "added_feature.extend(feature_var)\n",
    "added_feature.extend(feature_regression)\n",
    "feature_cols.extend(added_feature)\n",
    "feature_cols.extend(feature_ori)\n",
    "\n",
    "train_cols = feature_cols.copy()\n",
    "train_cols.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ip_day_hour_count',\n",
       " 'ip_os_day_hour_count',\n",
       " 'ip_app_day_hour_count',\n",
       " 'ip_app_os_day_hour_count',\n",
       " 'app_day_hour_count',\n",
       " 'ip_device_os_count',\n",
       " 'ip_app_device_os_count',\n",
       " 'ip_device_os_mean',\n",
       " 'ip_app_device_os_mean',\n",
       " 'ip_app_device_mean',\n",
       " 'app_device_os_mean',\n",
       " 'ip_device_os_time2nextclick',\n",
       " 'ip_app_device_os_time2nextclick',\n",
       " 'ip_app_device_time2nextclick',\n",
       " 'app_device_os_time2nextclick',\n",
       " 'ip_device_os_time2previousclick',\n",
       " 'ip_app_device_os_time2previousclick',\n",
       " 'ip_app_device_time2previousclick',\n",
       " 'app_device_os_time2previousclick',\n",
       " 'ip_device_os_countfromfuture',\n",
       " 'ip_app_device_os_countfromfuture',\n",
       " 'ip_app_device_countfromfuture',\n",
       " 'app_device_os_countfromfuture',\n",
       " 'ip_device_os_countfrompast',\n",
       " 'ip_app_device_os_countfrompast',\n",
       " 'ip_app_device_countfrompast',\n",
       " 'app_device_os_countfrompast',\n",
       " 'ip_device_os_lasttimediff',\n",
       " 'ip_app_device_os_lasttimediff',\n",
       " 'ip_app_device_lasttimediff',\n",
       " 'app_device_os_lasttimediff',\n",
       " 'matrixFact_user_iposdeviceapp_item_app',\n",
       " 'matrixFact_user_ip_item_appdeviceos',\n",
       " 'ip_device_os_regression',\n",
       " 'ip_app_device_os_regression',\n",
       " 'ip_app_device_regression',\n",
       " 'app_device_os_regression',\n",
       " 'app',\n",
       " 'channel',\n",
       " 'device',\n",
       " 'os',\n",
       " 'hour']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count(df_history, df_train, cols, target=None):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df.count the number of records for each feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "    \n",
    "    group = get_group(df_train, cols)\n",
    "    group_all = get_group(df_history, cols)\n",
    "    \n",
    "    count_map = group_all.value_counts()\n",
    "    \n",
    "    return group.map(count_map).fillna(0)\n",
    "\n",
    "def countsort(df_history, df_train, cols, target=None):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df.count the number of records for each feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "    \n",
    "    group = get_group(df_train, cols)\n",
    "    group_all = get_group(df_history, cols)\n",
    "    \n",
    "    count_map = group_all.value_counts().iloc[::-1]\n",
    "    count_map.iloc[:] = list(range(1, len(count_map) + 1))\n",
    "    \n",
    "    return group.map(count_map).fillna(-1)\n",
    "\n",
    "\n",
    "\n",
    "def mean(df_history, df_train, cols, target):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    group = get_group(df_train, cols)\n",
    "    group_history = get_group(df_history, cols)\n",
    "    mean_map = df_history.groupby(group_history)[target].mean()\n",
    "    return group.map(mean_map).fillna(-0.01)\n",
    "\n",
    "\n",
    "def reversemean(df_history, df_train, cols, target):\n",
    "    \"\"\"\n",
    "    Purpose: add a new feature to training df. conditional probability P(replay (target) | feature combination (eg, artist_name_composer)) \n",
    "    \"\"\"\n",
    "  \n",
    "    # encoding df's cols into a new series\n",
    "    group = get_group(df_train, cols)\n",
    "    # encoding df_history's cols into a new series\n",
    "    group_history = get_group(df_history, cols)\n",
    "    # get the conditional probability p(target| feature combination. eg, artist_name_composer) \n",
    "    positive = group_history[df_history[target] == 1]\n",
    "    negative = group_history[df_history[target] == 0]\n",
    "    index_p = set(positive.unique())\n",
    "    index_n = set(negative.unique())\n",
    "    index_n.difference_update(index_p)\n",
    "    map_reverse_p = positive.groupby(positive).count() / len(positive)\n",
    "    map_reverse_n = pd.Series(np.zeros(len(index_n)), index=index_n)\n",
    "    map_reverse = pd.concat([map_reverse_p, map_reverse_n])\n",
    "    return group.map(map_reverse).fillna(-1)\n",
    "\n",
    "\n",
    "def time2nextclick(df_history, df_train, cols, target, timecol='timestamp'):\n",
    "    \n",
    "    result = []\n",
    "    df_reverse = df_train.sort_index(ascending=False)\n",
    "    group = get_group(df_reverse,  cols)\n",
    "    \n",
    "    next_heard = {}\n",
    "    for g, t in zip(group, df_reverse[timecol]):\n",
    "        if g in next_heard:\n",
    "            result.append(next_heard[g] - t)\n",
    "        else:\n",
    "            result.append(-1)\n",
    "        next_heard[g] = t\n",
    "    \n",
    "    result.reverse()\n",
    "    return result\n",
    "\n",
    "def time2previousclick(df_history, df_train, cols, target, timecol='timestamp'):\n",
    "    \n",
    "    result = []\n",
    "    group = get_group(df_train, cols)\n",
    "\n",
    "    last_heard = {}\n",
    "    for t, g in zip(df_train[timecol], group):\n",
    "        if g in last_heard:\n",
    "            result.append(t - last_heard[g])\n",
    "        else:\n",
    "            result.append(-1)\n",
    "        last_heard[g] = t\n",
    "        \n",
    "    return result\n",
    "\n",
    "def countfrompast(df_history, df_train, cols, target, timecol='timestamp'):\n",
    "    \n",
    "    group = get_group(df_train, cols)\n",
    "    \n",
    "    count = {}\n",
    "    result = []\n",
    "    for g in group.values:\n",
    "        if g not in count:\n",
    "            count[g] = 0\n",
    "        else:\n",
    "            count[g] += 1\n",
    "        result.append(count[g])\n",
    "        \n",
    "    return result\n",
    "\n",
    "def countfromfuture(df_history, df_train, cols, target, timecol='timestamp'):\n",
    "    \n",
    "    result = []\n",
    "    df_reverse = df_train.sort_index(ascending=False)\n",
    "    group = get_group(df_reverse,  cols)\n",
    "    \n",
    "    count = {}\n",
    "    for g in group.values:\n",
    "        if g in count:\n",
    "            result.append(count[g])\n",
    "            count[g] += 1 \n",
    "        else:\n",
    "            result.append(0)\n",
    "            count[g] = 1\n",
    "    \n",
    "    result.reverse()\n",
    "    return result\n",
    "\n",
    "def lasttimediff(df_history, df_train, cols, target, timecol='timestamp'):\n",
    "    \n",
    "    group = get_group(df_train, cols)\n",
    "        \n",
    "    last_time = df_train.groupby(group)[timecol].last()\n",
    "    \n",
    "    return group.map(last_time) - df_train[timecol]\n",
    "\n",
    "def col_name(cols, func=None):\n",
    "    if func is None:\n",
    "        return '_'.join(cols)\n",
    "    else:\n",
    "        return '_'.join(cols) + '_' + func.__name__\n",
    "    \n",
    "    \n",
    "from lightfm import LightFM\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import coo_matrix\n",
    "from lightfm import LightFM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_var(df_history, df, group_col, agg_col):\n",
    "    group = get_group(df, group_col)\n",
    "    group_history = get_group(df_history, group_col)\n",
    "    df_temp = pd.DataFrame()\n",
    "    df_temp['group'] = group_history.values\n",
    "    df_temp['agg'] = df_history[agg_col].values\n",
    "    group_map =df_temp.groupby('group')['agg'].var()\n",
    "    result = group.map(group_map).fillna(0)\n",
    "    return result\n",
    "\n",
    "def matrix_factorization(df_history, df, target, item_col, userid_col, userraw_col):\n",
    "    \"\"\"\n",
    "    userid_col is unique user id\n",
    "    item_col is unique itme id\n",
    "    userraw_col is used to construct user feature. dim: user_id*userraw\n",
    "    \"\"\"\n",
    "    dff = pd.DataFrame()\n",
    "    dff_history = pd.DataFrame()\n",
    "\n",
    "\n",
    "    #1. process item\n",
    "    if item_col is None:\n",
    "        dff['item'] = np.zeros(len(df))\n",
    "        dff_history['item'] = np.zeros(len(df_history))\n",
    "    else:\n",
    "        encoder = LabelEncoder()\n",
    "        group = get_group(df, item_col)\n",
    "        group_history = get_group(df_history, item_col)\n",
    "        encoder.fit(pd.concat([group, group_history]))\n",
    "        dff['item'] = encoder.transform(group)\n",
    "        dff_history['item'] = encoder.transform(group_history)\n",
    "#     print('processing item done!')\n",
    "\n",
    "    #2. user raw\n",
    "    group = get_group(df, userraw_col)\n",
    "    group_history = get_group(df_history, userraw_col)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(pd.concat([group, group_history]))\n",
    "    dff['userraw'] = encoder.transform(group)\n",
    "    dff_history['userraw'] = encoder.transform(group_history)\n",
    "#     print('processing user raw done')\n",
    "\n",
    "\n",
    "    #3. user_id\n",
    "    group = get_group(df, userid_col)\n",
    "    group_history = get_group(df_history, userid_col)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(pd.concat([group, group_history]))\n",
    "    dff['user_id'] = encoder.transform(group)\n",
    "    dff_history['user_id'] = encoder.transform(group_history)\n",
    "#     print('processing user id done')\n",
    "\n",
    "\n",
    "\n",
    "    num_users = max(dff.user_id.max(), dff_history.user_id.max()) + 1\n",
    "    num_items = max(dff.item.max(), dff_history.item.max()) + 1\n",
    "    num_userraw = max(dff.userraw.max(), dff_history.userraw.max()) + 1\n",
    "\n",
    "    M = coo_matrix(\n",
    "            (df_history[target], ( dff_history.user_id, dff_history.item)),\n",
    "            shape=(num_users, num_items)\n",
    "        )\n",
    "\n",
    "    user_features = pd.concat([dff, dff_history])[['userraw', 'user_id']].drop_duplicates()\n",
    "\n",
    "    user_features = coo_matrix(\n",
    "        (np.ones(len(user_features)), (user_features.user_id, user_features.userraw)),\n",
    "        shape=(num_users, num_userraw)\n",
    "    )\n",
    "\n",
    "    user_features = sp.hstack([sp.eye(num_users), user_features])\n",
    "\n",
    "    model = LightFM(no_components=50, learning_rate=0.1)\n",
    "    print('fitting lightFM')\n",
    "    model.fit(\n",
    "            M, \n",
    "            epochs=2, \n",
    "            num_threads=36, \n",
    "            user_features=user_features,\n",
    "        )\n",
    "    print('predicting lightFM')\n",
    "    result = model.predict(\n",
    "        dff.user_id.values, \n",
    "        dff.item.values, \n",
    "        user_features=user_features,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def regression(df_history, df, cols, target= 'is_attributed', time_col='timestamp', shift=1500000000):\n",
    "    df = df.copy()\n",
    "    df_history = df_history.copy()\n",
    "    df.loc[:,time_col] = df.loc[:,time_col] - shift\n",
    "    df_history.loc[:,time_col] = df_history.loc[:,time_col] - shift\n",
    "    group = get_group(df, cols)\n",
    "    group_history = get_group(df_history, cols)\n",
    "\n",
    "    targets = {}\n",
    "    times = {}\n",
    "    for (y, t), u in zip(df_history[[target, time_col]].values, group_history):\n",
    "        if u not in targets:\n",
    "            targets[u] = [y]\n",
    "            times[u] = [t]\n",
    "        else:\n",
    "            targets[u].append(y)\n",
    "            times[u].append(t)\n",
    "\n",
    "    linal_user = {}\n",
    "    for u in times:\n",
    "        if len(times[u]) > 1:\n",
    "            A = np.vstack([times[u], np.ones(len(times[u]))]).T\n",
    "            linal_user[u] = np.linalg.inv(A.T.dot(A)).dot(A.T).dot(targets[u])\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for t, u in zip(df[time_col], group):\n",
    "        if u not in times:\n",
    "            result.append(-0.5)\n",
    "        else:\n",
    "            if len(times[u]) < 2:\n",
    "                result.append(-0.5)\n",
    "            else:\n",
    "                result.append(linal_user[u].dot([t, 1]))\n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = {}\n",
    "feature_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel',\n",
    "              'day',\n",
    "              'hour',]\n",
    "\n",
    "# feature_col = ['ip', \n",
    "#               'app', \n",
    "#               'device', \n",
    "#               'os', \n",
    "#               'channel']\n",
    "for col in feature_col:\n",
    "    orders[col] = 10 ** (int(np.log(max(train[col].max(),test[col].max() ) + 1) / np.log(10)) + 1)\n",
    "def get_group(df, cols):\n",
    "    \"\"\"\n",
    "    define an encoding method which can ganrantee the adding value will be unique.\n",
    "    eg: artist_name_composer will be a combination of (artist_name,composer) and the encoding will reflect the unqiue combination of those two\n",
    "    \"\"\"\n",
    "    group = df[cols[0]].copy()\n",
    "    for col in cols[1:]:\n",
    "        group = group * orders[col] + df[col]\n",
    "        \n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got train data\n",
      "got historical data\n",
      "count function\n",
      "all 1:   ip_device_os_count   \t\t\t size: 8.555501699447632e-05 G.\n",
      "count function\n",
      "all 2:   ip_day_hour_count   \t\t\t size: 9.166449308395386e-05 G.\n",
      "count function\n",
      "all 3:   app_day_hour_count   \t\t\t size: 9.77739691734314e-05 G.\n",
      "count function\n",
      "all 4:   ip_app_device_os_count   \t\t\t size: 0.00010388344526290894 G.\n",
      "count function\n",
      "all 5:   ip_app_day_hour_count   \t\t\t size: 0.00010999292135238647 G.\n",
      "count function\n",
      "all 6:   ip_os_day_hour_count   \t\t\t size: 0.00011610239744186401 G.\n",
      "count function\n",
      "all 7:   ip_app_os_day_hour_count   \t\t\t size: 0.00012221187353134155 G.\n",
      "mean function\n",
      "all 8:   ip_app_device_mean   \t\t\t size: 0.0001283213496208191 G.\n",
      "mean function\n",
      "all 9:   ip_device_os_mean   \t\t\t size: 0.00013443082571029663 G.\n",
      "mean function\n",
      "all 10:   app_device_os_mean   \t\t\t size: 0.00014054030179977417 G.\n",
      "mean function\n",
      "all 11:   ip_app_device_os_mean   \t\t\t size: 0.0001466497778892517 G.\n",
      "df_all does not exist\n",
      "df_all does not exist\n",
      "time related function\n",
      "all 12:   ip_app_device_time2nextclick   \t\t\t size: 0.00015275925397872925 G.\n",
      "time related function\n",
      "all 13:   ip_device_os_time2nextclick   \t\t\t size: 0.0001588687300682068 G.\n",
      "time related function\n",
      "all 14:   app_device_os_time2nextclick   \t\t\t size: 0.00016497820615768433 G.\n",
      "time related function\n",
      "all 15:   ip_app_device_os_time2nextclick   \t\t\t size: 0.00017108768224716187 G.\n",
      "df_all does not exist\n",
      "time related function\n",
      "all 16:   ip_app_device_time2previousclick   \t\t\t size: 0.0001771971583366394 G.\n",
      "time related function\n",
      "all 17:   ip_device_os_time2previousclick   \t\t\t size: 0.00018330663442611694 G.\n",
      "time related function\n",
      "all 18:   app_device_os_time2previousclick   \t\t\t size: 0.00018941611051559448 G.\n",
      "time related function\n",
      "all 19:   ip_app_device_os_time2previousclick   \t\t\t size: 0.00019552558660507202 G.\n",
      "df_all does not exist\n",
      "time related function\n",
      "all 20:   ip_app_device_countfromfuture   \t\t\t size: 0.00020163506269454956 G.\n",
      "time related function\n",
      "all 21:   ip_device_os_countfromfuture   \t\t\t size: 0.0002077445387840271 G.\n",
      "time related function\n",
      "all 22:   app_device_os_countfromfuture   \t\t\t size: 0.00021385401487350464 G.\n",
      "time related function\n",
      "all 23:   ip_app_device_os_countfromfuture   \t\t\t size: 0.00021996349096298218 G.\n",
      "df_all does not exist\n",
      "time related function\n",
      "all 24:   ip_app_device_countfrompast   \t\t\t size: 0.00022607296705245972 G.\n",
      "time related function\n",
      "all 25:   ip_device_os_countfrompast   \t\t\t size: 0.00023218244314193726 G.\n",
      "time related function\n",
      "all 26:   app_device_os_countfrompast   \t\t\t size: 0.0002382919192314148 G.\n",
      "time related function\n",
      "all 27:   ip_app_device_os_countfrompast   \t\t\t size: 0.00024440139532089233 G.\n",
      "df_all does not exist\n",
      "time related function\n",
      "all 28:   ip_app_device_lasttimediff   \t\t\t size: 0.0002505108714103699 G.\n",
      "time related function\n",
      "all 29:   ip_device_os_lasttimediff   \t\t\t size: 0.0002566203474998474 G.\n",
      "time related function\n",
      "all 30:   app_device_os_lasttimediff   \t\t\t size: 0.00026272982358932495 G.\n",
      "time related function\n",
      "all 31:   ip_app_device_os_lasttimediff   \t\t\t size: 0.0002688392996788025 G.\n",
      "df_all does not exist\n",
      "regression function\n",
      "all 32:   ip_app_device_regression   \t\t\t size: 0.00027494877576828003 G.\n",
      "regression function\n",
      "all 33:   ip_device_os_regression   \t\t\t size: 0.00028105825185775757 G.\n",
      "regression function\n",
      "all 34:   app_device_os_regression   \t\t\t size: 0.0002871677279472351 G.\n",
      "regression function\n",
      "all 35:   ip_app_device_os_regression   \t\t\t size: 0.00029327720403671265 G.\n",
      "processing matrixFact_user_iposdeviceapp_item_app\n",
      "fitting lightFM\n",
      "predicting lightFM\n",
      "processing matrixFact_user_ip_item_appdeviceos\n",
      "fitting lightFM\n",
      "predicting lightFM\n",
      "all 37:   matrixFact_user_ip_item_appdeviceos   \t\t\t size: 0.0003054961562156677 G.\n",
      "------\n",
      "['ip_day_hour_count' 'ip_os_day_hour_count' 'ip_app_day_hour_count'\n",
      " 'ip_app_os_day_hour_count' 'app_day_hour_count' 'ip_device_os_count'\n",
      " 'ip_app_device_os_count' 'ip_device_os_mean' 'ip_app_device_os_mean'\n",
      " 'ip_app_device_mean' 'app_device_os_mean' 'ip_device_os_time2nextclick'\n",
      " 'ip_app_device_os_time2nextclick' 'ip_app_device_time2nextclick'\n",
      " 'app_device_os_time2nextclick' 'ip_device_os_time2previousclick'\n",
      " 'ip_app_device_os_time2previousclick' 'ip_app_device_time2previousclick'\n",
      " 'app_device_os_time2previousclick' 'ip_device_os_countfromfuture'\n",
      " 'ip_app_device_os_countfromfuture' 'ip_app_device_countfromfuture'\n",
      " 'app_device_os_countfromfuture' 'ip_device_os_countfrompast'\n",
      " 'ip_app_device_os_countfrompast' 'ip_app_device_countfrompast'\n",
      " 'app_device_os_countfrompast' 'ip_device_os_lasttimediff'\n",
      " 'ip_app_device_os_lasttimediff' 'ip_app_device_lasttimediff'\n",
      " 'app_device_os_lasttimediff' 'matrixFact_user_iposdeviceapp_item_app'\n",
      " 'matrixFact_user_ip_item_appdeviceos' 'ip_device_os_regression'\n",
      " 'ip_app_device_os_regression' 'ip_app_device_regression'\n",
      " 'app_device_os_regression' 'app' 'channel' 'device' 'os' 'hour'\n",
      " 'is_attributed']\n",
      "/home/kai/data/kaggle/talkingdata/wl/data/equalhour/Train_features_matrixregV2_comp.csv\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from itertools import combinations\n",
    "combine_col = ['ip', \n",
    "              'app', \n",
    "              'device', \n",
    "              'os', \n",
    "              'channel',\n",
    "              'day',\n",
    "              'hour',]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "df_train = df_comp.copy()\n",
    "print('got train data')\n",
    "history_index = list(set(train.index.values) - set(df_train.index.values))\n",
    "df_history = train.iloc[history_index].copy()\n",
    "print('got historical data')\n",
    "\n",
    "###########################################################################\n",
    "for func in [count, mean, reversemean,time2nextclick, time2previousclick, countfromfuture, countfrompast, lasttimediff, regression]:\n",
    "            if func.__name__ == count.__name__:\n",
    "                df_all = pd.concat([train, test])\n",
    "            else:\n",
    "                try:\n",
    "                    del df_all\n",
    "                    gc.collect()\n",
    "                except Exception:\n",
    "                    print('df_all does not exist')\n",
    "\n",
    "            for num_col in [1,2,3,4,5]:\n",
    "                for cols in combinations(combine_col, num_col):\n",
    "                    feature_name = col_name(cols, func=func)\n",
    "                    if feature_name not in added_feature:\n",
    "                           continue\n",
    "                    counter += 1\n",
    "                    if func.__name__ == count.__name__:\n",
    "                            print('count function')\n",
    "                            df_train[feature_name] = func(df_all, df_train, cols, target='is_attributed')\n",
    "\n",
    "                    elif func.__name__ == mean.__name__:\n",
    "                            print('mean function')\n",
    "                            df_train[feature_name] = func(df_history, df_train, cols, target='is_attributed')\n",
    "                    elif func.__name__ == reversemean.__name__:\n",
    "                            print('reverse mean function')\n",
    "                            df_train[feature_name] = func(df_history, df_train, cols, target='is_attributed')\n",
    "\n",
    "                    elif func.__name__ == regression.__name__:\n",
    "                            print('regression function')\n",
    "                            df_train[feature_name] = np.ones(len(df_train)) * (-0.5)\n",
    "\n",
    "                    else:\n",
    "                            print('time related function')\n",
    "                            df_train[feature_name] = np.zeros(len(df_train))\n",
    "\n",
    "                    all_str = 'all {}:   {}   \\t\\t\\t size: {} G.'.format(counter, feature_name, sys.getsizeof(df_train)/ 1024 **3)\n",
    "                    print(all_str)\n",
    "                    with open('feature_all.txt', 'w') as text_file:\n",
    "                        text_file.write(all_str + '\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "#     ### get val\n",
    "#     print('get val')\n",
    "#     df_all = pd.concat([train, test])\n",
    "#     df_train['ip_app_os_var_hour'] = get_var(df_all, df_train, ['ip','app', 'os'], 'hour')\n",
    "#     print('ip_app_os_var_hour done!')\n",
    "#     df_train['ip_app_channel_var_day'] = get_var(df_all, df_train, ['ip','app', 'channel'], 'day')\n",
    "#     print('ip_app_channel_var_day done!')\n",
    "#     del df_all\n",
    "#     gc.collect()\n",
    "    \n",
    "    \n",
    "### matrix - factorization\n",
    "counter += 1\n",
    "feature_name = 'matrixFact_user_iposdeviceapp_item_app'\n",
    "print('processing {}'.format(feature_name))\n",
    "df_train[feature_name] = matrix_factorization(df_history, df_train,target, item_col=['app'], userid_col=['ip','os','device','app'], userraw_col=['ip'])\n",
    "feature_name = 'matrixFact_user_ip_item_appdeviceos'\n",
    "print('processing {}'.format(feature_name))\n",
    "counter += 1\n",
    "df_train[feature_name] = matrix_factorization(df_history, df_train,target, item_col=['app', 'device', 'os'], userid_col=['ip'], userraw_col=['ip'])\n",
    "\n",
    "\n",
    "all_str = 'all {}:   {}   \\t\\t\\t size: {} G.'.format(counter, feature_name, sys.getsizeof(df_train)/ 1024 **3)\n",
    "print(all_str)\n",
    "with open('feature_all.txt', 'w') as text_file:\n",
    "    text_file.write(all_str + '\\n')\n",
    "\n",
    "\n",
    "save_file_name = '{}_features_matrixregV2_comp.csv'.format('Train')\n",
    "save_file_path = '/home/kai/data/kaggle/talkingdata/wl/data/equalhour/' + save_file_name\n",
    "df_train = df_train[train_cols]\n",
    "print('------')\n",
    "print(df_train.columns.values)\n",
    "df_train.to_csv(save_file_path, index=False)\n",
    "print(save_file_path)\n",
    "print('======================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
