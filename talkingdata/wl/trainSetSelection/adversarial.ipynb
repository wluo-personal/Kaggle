{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/kai/data/kaggle/talkingdata/data/'\n",
    "train = pd.read_csv(path + 'train_cleaned_final.csv')\n",
    "test = pd.read_csv(path + 'test_cleaned_final.csv')\n",
    "\n",
    "train.drop(['is_attributed', 'day', 'timestamp'], axis=1, inplace = True)\n",
    "test.drop(['day', 'timestamp'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test[list(train.columns)]\n",
    "df_all = pd.concat([train,test])\n",
    "\n",
    "train_length = len(train)\n",
    "test_length = len(test)\n",
    "\n",
    "import gc\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "label = np.concatenate([np.zeros(train_length), np.ones(test_length)]) \n",
    "df_all['label'] = label\n",
    "\n",
    "import sys\n",
    "print(sys.getsizeof(df_all) / 1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainset, valset = train_test_split(df_all.sample(frac=1, random_state=233),test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_col = ['ip', 'app', 'device', 'os', 'channel']\n",
    "feature_cols = ['ip', 'app', 'device', 'os', 'channel', 'hour', 'minute', 'second']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'label'\n",
    "y_train = trainset[target].values\n",
    "y_val = valset[target].values\n",
    "\n",
    "lgb_train = lgb.Dataset(trainset[feature_cols], y_train, categorical_feature = categorical_col)\n",
    "lgb_val = lgb.Dataset(valset[feature_cols], y_val, categorical_feature = categorical_col)\n",
    "\n",
    "zeros = len(y_train[y_train == 0])\n",
    "# scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1027: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/kai/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:668: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\tvalid_0's binary_logloss: 0.337234\n",
      "[20]\tvalid_0's binary_logloss: 0.249118\n",
      "[30]\tvalid_0's binary_logloss: 0.21775\n",
      "[40]\tvalid_0's binary_logloss: 0.20609\n",
      "[50]\tvalid_0's binary_logloss: 0.20056\n",
      "[60]\tvalid_0's binary_logloss: 0.197744\n",
      "[70]\tvalid_0's binary_logloss: 0.196125\n",
      "[80]\tvalid_0's binary_logloss: 0.195042\n",
      "[90]\tvalid_0's binary_logloss: 0.194081\n",
      "[100]\tvalid_0's binary_logloss: 0.193317\n",
      "[110]\tvalid_0's binary_logloss: 0.192665\n",
      "[120]\tvalid_0's binary_logloss: 0.191849\n",
      "[130]\tvalid_0's binary_logloss: 0.191469\n",
      "[140]\tvalid_0's binary_logloss: 0.191171\n",
      "[150]\tvalid_0's binary_logloss: 0.190881\n",
      "[160]\tvalid_0's binary_logloss: 0.190444\n",
      "[170]\tvalid_0's binary_logloss: 0.19\n",
      "[180]\tvalid_0's binary_logloss: 0.189669\n",
      "[190]\tvalid_0's binary_logloss: 0.189448\n",
      "[200]\tvalid_0's binary_logloss: 0.18911\n",
      "[210]\tvalid_0's binary_logloss: 0.188911\n",
      "[220]\tvalid_0's binary_logloss: 0.188544\n",
      "[230]\tvalid_0's binary_logloss: 0.188373\n",
      "[240]\tvalid_0's binary_logloss: 0.188225\n",
      "[250]\tvalid_0's binary_logloss: 0.187897\n",
      "[260]\tvalid_0's binary_logloss: 0.18778\n",
      "[270]\tvalid_0's binary_logloss: 0.187641\n",
      "[280]\tvalid_0's binary_logloss: 0.187457\n",
      "[290]\tvalid_0's binary_logloss: 0.187255\n",
      "[300]\tvalid_0's binary_logloss: 0.187144\n",
      "[310]\tvalid_0's binary_logloss: 0.187009\n",
      "[320]\tvalid_0's binary_logloss: 0.186926\n",
      "[330]\tvalid_0's binary_logloss: 0.186697\n",
      "[340]\tvalid_0's binary_logloss: 0.186619\n",
      "[350]\tvalid_0's binary_logloss: 0.186387\n",
      "[360]\tvalid_0's binary_logloss: 0.18631\n",
      "[370]\tvalid_0's binary_logloss: 0.186137\n",
      "[380]\tvalid_0's binary_logloss: 0.186026\n",
      "[390]\tvalid_0's binary_logloss: 0.18587\n",
      "[400]\tvalid_0's binary_logloss: 0.185738\n",
      "[410]\tvalid_0's binary_logloss: 0.185665\n",
      "[420]\tvalid_0's binary_logloss: 0.185569\n",
      "[430]\tvalid_0's binary_logloss: 0.185491\n",
      "[440]\tvalid_0's binary_logloss: 0.1854\n",
      "[450]\tvalid_0's binary_logloss: 0.185355\n",
      "[460]\tvalid_0's binary_logloss: 0.185278\n",
      "[470]\tvalid_0's binary_logloss: 0.185191\n",
      "[480]\tvalid_0's binary_logloss: 0.185044\n",
      "[490]\tvalid_0's binary_logloss: 0.184993\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_rounds': 500,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 9,\n",
    "        'num_threads': 20, # best speed: set to number of real cpu cores, which is vCPU/2\n",
    "        'device': 'cpu',\n",
    "        'max_depth': -1, # no limit. This is used to deal with over-fitting when #data is small.\n",
    "        'min_data_in_leaf': 20,  #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.9, #For example, if set to 0.8, will select 80% features before training each tree.  speed up training / deal with over-fitting\n",
    "        'feature_fraction_seed': 1,\n",
    "        'early_stopping_round':30,\n",
    "        'bagging_fraction': 0.9, #Randomly select part of data without resampling\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration. to enable bagging, bagging_fraction should be set as well\n",
    "        'bagging_seed': 1,\n",
    "        #'max_bin': 255,\n",
    "        'verbose': 0,\n",
    "#         'scale_pos_weight': scale_pos_weight,\n",
    "        'metric' : 'binary_logloss'\n",
    "    }\n",
    "\n",
    "model = lgb.train(params, train_set=lgb_train, valid_sets=lgb_val, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-108757bec662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "pred = model.predict(df_all[feature_cols])\n",
    "df_all['pred'] = pred\n",
    "print(roc_auc_score(df_all[target].values,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df_all.loc[:train_length]\n",
    "train_final = train.sort_values(['pred'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresh = 5000* 10000\n",
    "index = list(train_final.index.values[:thresh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json as js\n",
    "file = '/home/kai/data/kaggle/talkingdata/wl/data/trainset/train_index.json'\n",
    "with open(file, 'w') as myjs:\n",
    "    js.dump(all_features, myjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
