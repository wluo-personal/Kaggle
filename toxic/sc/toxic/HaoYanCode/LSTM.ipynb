{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py # this might not be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n",
      "72.20628647257263\n",
      "114.01717258139988\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "text_length = pd.concat([train_sentence.apply(lambda x: len(x.split())),\\\n",
    "                         test_sentence.apply(lambda x: len(x.split()))])\n",
    "\n",
    "mean_length = text_length.mean()\n",
    "std_length = text_length.std()\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(mean_length)\n",
    "print(std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "MAX_FEATURES = 100000 # max num of words\n",
    "MAX_LEN = np.round(mean_length + 3*std_length).astype(int) # max sequence length\n",
    "#EMBED_SIZE = 50 # embedding size\n",
    "LSTM_UNITS = 50 # LSTM hidden layer unit number\n",
    "DENSE_UNITS = 50\n",
    "DROPOUT = 0.3 # dropout rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(pd.concat([train_sentence, test_sentence]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edit make under my username hardcore metallica fan be revert ? they be not vandalism just closure on some gas after i vote at new york doll fac . and please do not remove the template from the talk page since i be retire now .'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(train_sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 46, 18)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_train), len(tokenized_train[0]), len(tokenized_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[628, 79, 2, 33, 46, 197, 26, 702, 3682, 8108, 798, 1, 140, 45, 1, 12, 225, 50, 4961, 17, 64, 2014, 160, 5, 491, 31, 127, 1166, 8109, 2202, 7, 51, 14, 12, 98, 2, 278, 28, 2, 43, 23, 151, 5, 1, 2499, 90]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(test_sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train[label_cols].values\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file, encoding='utf8'))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_lstm_model(embedding_file, embed_size, max_features, tokenizer,\\\n",
    "                   max_len, lstm_units, dense_units, label_cols, dropout):\n",
    "    embedding_matrix = get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer)\n",
    "    pdb.set_trace\n",
    "    input = Input(shape=(max_len, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(input)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.2,\\\n",
    "                           kernel_regularizer=regularizers.l2(0.01)))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dense(len(label_cols), activation='sigmoid')(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, file_path, batch_size, epochs, X_train, y):\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "    model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "    return model\n",
    "\n",
    "def predict(model, file_path, X_test):\n",
    "    model.load_weights(file_path)\n",
    "    return model.predict(X_test, verbose=1)\n",
    "    \n",
    "def save(model_name, y_test, label_cols, path, is_train=False):\n",
    "    if is_train:\n",
    "        submission = pd.read_csv(path + 'sample_train.csv')\n",
    "        file_name = 'train_' + model_name\n",
    "    else:\n",
    "        submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "        file_name = model_name # useless now\n",
    "    submission[label_cols] = y_test\n",
    "    print('submission shape:')\n",
    "    print(submission.shape)\n",
    "    submission.to_csv(path + 'sub_' + BUILD_ID + '.csv', index=False)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 414)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kai/data/resources/glove/glove.6B.100d.txt\n",
      "BUILD_ID: 100000_100_50_50_0.3_32_10_glove.6B.100d.txt_1518278460\n",
      "/home/kai/data/shiyi/toxic/models/mod_100000_100_50_50_0.3_32_10_glove.6B.100d.txt_1518278460.hdf5\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "getting model\n",
      "> <ipython-input-34-c7b3aa10e52b>(18)get_lstm_model()\n",
      "-> input = Input(shape=(max_len, ))\n",
      "(Pdb) c\n",
      "training\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9784\n",
      "Epoch 00001: val_loss improved from inf to 0.05333, saving model to /home/kai/data/shiyi/toxic/models/mod_100000_100_50_50_0.3_32_10_glove.6B.100d.txt_1518278460.hdf5\n",
      "143613/143613 [==============================] - 3870s 27ms/step - loss: 0.0955 - acc: 0.9784 - val_loss: 0.0533 - val_acc: 0.9820\n",
      "Epoch 2/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9825\n",
      "Epoch 00002: val_loss improved from 0.05333 to 0.05110, saving model to /home/kai/data/shiyi/toxic/models/mod_100000_100_50_50_0.3_32_10_glove.6B.100d.txt_1518278460.hdf5\n",
      "143613/143613 [==============================] - 3870s 27ms/step - loss: 0.0499 - acc: 0.9825 - val_loss: 0.0511 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9838\n",
      "Epoch 00003: val_loss did not improve\n",
      "143613/143613 [==============================] - 3869s 27ms/step - loss: 0.0449 - acc: 0.9838 - val_loss: 0.0520 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9848\n",
      "Epoch 00004: val_loss did not improve\n",
      "143613/143613 [==============================] - 3867s 27ms/step - loss: 0.0418 - acc: 0.9848 - val_loss: 0.0525 - val_acc: 0.9817\n",
      "Epoch 5/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9857\n",
      "Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 3868s 27ms/step - loss: 0.0391 - acc: 0.9857 - val_loss: 0.0541 - val_acc: 0.9814\n",
      "Epoch 6/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9865\n",
      "Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 3869s 27ms/step - loss: 0.0373 - acc: 0.9865 - val_loss: 0.0562 - val_acc: 0.9812\n",
      "Epoch 7/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9872\n",
      "Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 3868s 27ms/step - loss: 0.0355 - acc: 0.9872 - val_loss: 0.0554 - val_acc: 0.9809\n",
      "Epoch 8/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9878\n",
      "Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 3867s 27ms/step - loss: 0.0340 - acc: 0.9878 - val_loss: 0.0584 - val_acc: 0.9809\n",
      "Epoch 9/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9882\n",
      "Epoch 00009: val_loss did not improve\n",
      "143613/143613 [==============================] - 3867s 27ms/step - loss: 0.0327 - acc: 0.9882 - val_loss: 0.0587 - val_acc: 0.9803\n",
      "Epoch 10/10\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9885\n",
      "Epoch 00010: val_loss did not improve\n",
      "143613/143613 [==============================] - 3871s 27ms/step - loss: 0.0318 - acc: 0.9885 - val_loss: 0.0567 - val_acc: 0.9794\n",
      "training (hours) 10.323054284402234 \n",
      "predicting\n",
      "153164/153164 [==============================] - 775s 5ms/step\n",
      "predicting (hours) 0.20678298227723443\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "CPU times: user 17h 58min 15s, sys: 3h 1min 7s, total: 20h 59min 22s\n",
      "Wall time: 10h 58min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDING_PATH = '/home/kai/data/resources/glove/'\n",
    "for EMBED_SIZE in [100]:\n",
    "    EMBEDDING_FILE_NAME = 'glove.6B.{}d.txt'.format(EMBED_SIZE)\n",
    "    EMBEDDING_FILE = EMBEDDING_PATH + EMBEDDING_FILE_NAME\n",
    "    print(EMBEDDING_FILE)\n",
    "        \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    BUILD_ID = '{}_{}_{}_{}_{}_{}_{}_{}_{}'.format(MAX_FEATURES, EMBED_SIZE, LSTM_UNITS, DENSE_UNITS, DROPOUT, BATCH_SIZE, EPOCHS, EMBEDDING_FILE_NAME, int(time.time()))\n",
    "    print('BUILD_ID: ' + BUILD_ID)\n",
    "    SAVE_DIR = '/home/kai/data/shiyi/toxic/'\n",
    "    MODEL_FILE = SAVE_DIR + 'models/mod_' + BUILD_ID + '.hdf5'\n",
    "    print(MODEL_FILE)\n",
    "    SUB_DIR = SAVE_DIR + 'submissions/'\n",
    "    print(SUB_DIR)\n",
    "\n",
    "    print('getting model')\n",
    "    model = get_lstm_model(EMBEDDING_FILE, EMBED_SIZE, MAX_FEATURES, tokenizer,\\\n",
    "                           MAX_LEN, LSTM_UNITS, DENSE_UNITS, label_cols, DROPOUT)\n",
    "    print('training')\n",
    "    model = train_model(model, MODEL_FILE, BATCH_SIZE, EPOCHS, X_train, y)\n",
    "\n",
    "    elapsed_time = (timeit.default_timer() - start_time)/3600    \n",
    "    print('training (hours) {} '.format(elapsed_time))\n",
    "    with open('timefile.txt','a') as f:\n",
    "        f.write('##################################################################\\n')\n",
    "        f.write('##################################################################\\n')\n",
    "        f.write('\\nBUILD_ID: ' + BUILD_ID + '\\ntraining: '+str(elapsed_time))\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "    print('predicting')\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    y_test = predict(model, MODEL_FILE, X_test)\n",
    "    #print('train predicting')\n",
    "    #y_train = predict(model, MODEL_PATH, X_train)\n",
    "\n",
    "    elapsed_time = (timeit.default_timer() - start_time)/3600    \n",
    "    print('predicting (hours) {}'.format(elapsed_time))\n",
    "    with open('timefile.txt','a') as f:\n",
    "        f.write('\\npredicting: '+str(elapsed_time)+'\\n')\n",
    "\n",
    "    save('lstm', y_test, label_cols, SUB_DIR)\n",
    "    #save('lstm', y_train, label_cols, SUB_DIR, True)\n",
    "    \n",
    "    with open('timefile.txt','a') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        for key, value in model.history.history.items():\n",
    "            f.write('\\nperformance: '+str(key)+': '+str(value))\n",
    "        for key, value in model.history.params.items():\n",
    "            f.write('\\nparams: '+str(key)+': '+str(value))\n",
    "        f.write('\\n')\n",
    "    \n",
    "    print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "name": "LSTM",
  "notebookId": 3600042186122233
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
