{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastText import load_model\n",
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import timeit, time, datetime\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FT model\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading FT model')\n",
    "ft_model = load_model('/home/kai/data/resources/FastText/wiki.en.bin')\n",
    "n_features = ft_model.get_dimension()\n",
    "\n",
    "print(n_features)\n",
    "window_length = 200 # The amount of words we look at per example. Experiment with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # config\n",
    "# RNN_UNITS = 50 # LSTM hidden layer unit number\n",
    "# DENSE_UNITS = 50\n",
    "# DROPOUT = 0.3 # dropout rate\n",
    "# BATCH_SIZE = 128\n",
    "# EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    \"\"\"\n",
    "    #s = s.lower()\n",
    "    # Replace ips\n",
    "    #s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
    "    # Isolate punctuation\n",
    "    #s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    #s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', ' zero ')\n",
    "    s = s.replace('1', ' one ')\n",
    "    s = s.replace('2', ' two ')\n",
    "    s = s.replace('3', ' three ')\n",
    "    s = s.replace('4', ' four ')\n",
    "    s = s.replace('5', ' five ')\n",
    "    s = s.replace('6', ' six ')\n",
    "    s = s.replace('7', ' seven ')\n",
    "    s = s.replace('8', ' eight ')\n",
    "    s = s.replace('9', ' nine ')\n",
    "    return s\n",
    "\n",
    "print('\\nLoading data')\n",
    "train = pd.read_csv('/home/kai/data/haoyan/ToxicClassificationCopy3/data/cleaned_train.csv')\n",
    "test = pd.read_csv('/home/kai/data/haoyan/ToxicClassificationCopy3/data/cleaned_test.csv')\n",
    "#train['comment_text'] = train['comment_text'].fillna('_empty_')\n",
    "#test['comment_text'] = test['comment_text'].fillna('_empty_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = train.sample(frac=0.05)\n",
    "# test = test.sample(frac=0.01)\n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 27), (153164, 21))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    \"\"\"\n",
    "    Given a string, normalizes it, then splits it into words and finally converts\n",
    "    it to a sequence of word vectors.\n",
    "    \"\"\"\n",
    "    text = normalize(text)\n",
    "    words = text.split()\n",
    "    window = words[-window_length:]\n",
    "    \n",
    "    x = np.zeros((window_length, n_features))\n",
    "\n",
    "    for i, word in enumerate(window):\n",
    "        x[i, :] = ft_model.get_word_vector(word).astype('float32')\n",
    "\n",
    "    return x\n",
    "\n",
    "def df_to_data(df):\n",
    "    \"\"\"\n",
    "    Convert a given dataframe to a dataset of inputs for the NN.\n",
    "    \"\"\"\n",
    "    x = np.zeros((len(df), window_length, n_features), dtype='float32')\n",
    "\n",
    "    for i, comment in enumerate(df['comment_text_cleaned'].values):\n",
    "        x[i, :] = text_to_vector(comment)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "split_index = round(len(train) * 0.9) #################################\n",
    "shuffled_train = train#.sample(frac=1) # no shuffle so the last 10% is chosen as validation set\n",
    "df_train = shuffled_train.iloc[:split_index]\n",
    "df_val = shuffled_train.iloc[split_index:]\n",
    "# Convert validation set to fixed array\n",
    "x_val = df_to_data(df_val)\n",
    "y_val = df_val[label_cols].values\n",
    "# Get test data ready\n",
    "x_test = df_to_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15957, 200, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save('/home/kai/data/shiyi/toxic/saved_files/x_val_np', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "np.save('/home/kai/data/shiyi/toxic/saved_files/y_val_np', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(df, window_length, n_features, batch_size):\n",
    "    \"\"\"\n",
    "    Given a raw dataframe, generates infinite batches of FastText vectors.\n",
    "    \"\"\"\n",
    "    batch_i = 0 # Counter inside the current batch vector\n",
    "    batch_x = None # The current batch's x data\n",
    "    batch_y = None # The current batch's y data\n",
    "    \n",
    "    while True: # Loop forever\n",
    "        df = df.sample(frac=1) # Shuffle df each epoch # for bagging purpose, change it to a float. \n",
    "                                #(and/or sample with placement)\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            comment = row['comment_text_cleaned']\n",
    "            \n",
    "            if batch_x is None:\n",
    "                batch_x = np.zeros((batch_size, window_length, n_features), dtype='float32')\n",
    "                batch_y = np.zeros((batch_size, len(label_cols)), dtype='float32')\n",
    "                \n",
    "            batch_x[batch_i] = text_to_vector(comment)\n",
    "            batch_y[batch_i] = row[label_cols].values\n",
    "            batch_i += 1\n",
    "\n",
    "            if batch_i == batch_size:\n",
    "                # Ready to yield the batch\n",
    "                yield batch_x, batch_y\n",
    "                batch_x = None\n",
    "                batch_y = None\n",
    "                batch_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# def get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "#     embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file, encoding='utf8'))\n",
    "#     all_embs = np.stack(embeddings_index.values())\n",
    "#     word_index = tokenizer.word_index\n",
    "#     nb_words = min(max_features, len(word_index))\n",
    "#     embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), (nb_words, embed_size))\n",
    "#     for word, i in word_index.items():\n",
    "#         if i < max_features:\n",
    "#             embedding_vector = embeddings_index.get(word)\n",
    "#             if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "#     return embedding_matrix\n",
    "\n",
    "def get_lstm_model(window_length, n_features, rnn_units, dense_units, label_cols, dropout, mode='LSTM', bidirection = False ,load_model=False, load_model_file=None):\n",
    "    #embedding_matrix = get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer)\n",
    "    #input = Input(shape=(max_len, ))\n",
    "    input = Input(shape=(window_length, n_features))\n",
    "    #x = Embedding(max_features, embed_size, weights=[embedding_matrix])(input)\n",
    "    \n",
    "    #x = Bidirectional(LSTM(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))(input)\n",
    "    #x = LSTM(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)(input)\n",
    "    rnn_layer = LSTM(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)\n",
    "    if mode == 'GRU':\n",
    "        rnn_layer = GRU(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)\n",
    "    if bidirection:\n",
    "        x = Bidirectional(rnn_layer)(input)\n",
    "    else:\n",
    "        x = rnn_layer(input)\n",
    "    \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(len(label_cols), activation='sigmoid')(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    if (load_model):\n",
    "        model.load_weights(load_model_file)\n",
    "        \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# def get_lstm_model_temp():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(32, activation='relu', input_dim=100))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(optimizer='rmsprop',\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def train_model_temp(model, data, labels):\n",
    "#     model.fit(data, one_hot_)\n",
    "\n",
    "def train_model(model, model_file, window_length, n_features, batch_size, epochs, df_train, x_val, y_val):\n",
    "    # without generator, do some like this and feed to model.fit\n",
    "    # x_train = df_to_data(train)\n",
    "    # y_train = train[label_cols].values\n",
    "    \n",
    "    # with generator:\n",
    "    training_steps_per_epoch = round(len(df_train) / batch_size)\n",
    "    print('steps:')\n",
    "    print(training_steps_per_epoch)\n",
    "    training_generator = data_generator(df_train, window_length, n_features, batch_size)\n",
    "    \n",
    "    # start training\n",
    "    checkpoint = ModelCheckpoint(model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "        \n",
    "    #model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "    model.fit_generator(\n",
    "        generator=training_generator, \n",
    "        steps_per_epoch=training_steps_per_epoch, \n",
    "        epochs=epochs, \n",
    "        validation_data=(x_val, y_val), \n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def predict(model, file_path, x_test):\n",
    "    model.load_weights(file_path)\n",
    "    return model.predict(x_test, verbose=1)\n",
    "\n",
    "def evaluate(model, file_path, x_val, y_val, label_cols, metrics='ROC'):\n",
    "    model.load_weights(file_path)\n",
    "    y_val_preds = model.predict(x_val)\n",
    "    individual_label_auc_report = ''\n",
    "    auc_per_label = {}\n",
    "    for i in range(len(label_cols)):\n",
    "        fpr, tpr, thresholds = roc_curve(y_val[:,i], y_val_preds[:,i], pos_label=1.0)\n",
    "        auc_temp = auc(fpr, tpr)\n",
    "        auc_per_label[label_cols[i]] = auc_temp\n",
    "        individual_label_auc_report += '\\nLabel: {:20} Threashold count: {} \\t AUC: {}'.format(label_cols[i], len(thresholds), auc_temp)\n",
    "        #mean_auc += auc_temp # this can be computed by roc_auc_score\n",
    "    #mean_auc/=len(label_cols)\n",
    "    individual_label_auc_report+='\\n'\n",
    "    if metrics=='ROC':\n",
    "        return roc_auc_score(y_val, y_val_preds), individual_label_auc_report, auc_per_label\n",
    "    raise ValueError('The chosen metrics is not implemented yet')\n",
    "    \n",
    "def save(y_test, label_cols, path, is_train=False):\n",
    "    if is_train:\n",
    "        submission = pd.read_csv(path + 'sample_train.csv')\n",
    "        file_name = 'trn_'\n",
    "    else:\n",
    "        submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "        file_name = 'sub_'\n",
    "    submission[label_cols] = y_test\n",
    "    print('submission shape:')\n",
    "    print(submission.shape)\n",
    "    submission.to_csv(path + file_name + BUILD_ID + '.csv', index=False)\n",
    "\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename(file_dir, file_id):\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for filename in files:\n",
    "            if str(file_id) in filename:\n",
    "                return filename\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "getting fresh model\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_1_1519090847\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_1_1519090847.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9800\n",
      "Epoch 00001: val_loss improved from inf to 0.04801, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_1_1519090847.hdf5\n",
      "4488/4488 [==============================] - 1622s 361ms/step - loss: 0.0556 - acc: 0.9800 - val_loss: 0.0480 - val_acc: 0.9824\n",
      "training time: 1558.7051081610844s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 925 \t AUC: 0.9789213536454304\n",
      "Label: severe_toxic         Threashold count: 279 \t AUC: 0.9913409670820945\n",
      "Label: obscene              Threashold count: 575 \t AUC: 0.9891487975171223\n",
      "Label: threat               Threashold count: 155 \t AUC: 0.9826076570063496\n",
      "Label: insult               Threashold count: 685 \t AUC: 0.9829358059983604\n",
      "Label: identity_hate        Threashold count: 275 \t AUC: 0.9905977940091485\n",
      "\n",
      "ROC: 0.9859253958764177\n",
      "predicting\n",
      "153164/153164 [==============================] - 363s 2ms/step\n",
      "predicting time: 348.5259561568964s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519090847, epochs: 1\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_2_1519092872\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_2_1519092872.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9832\n",
      "Epoch 00001: val_loss improved from inf to 0.04189, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_2_1519092872.hdf5\n",
      "4488/4488 [==============================] - 1597s 356ms/step - loss: 0.0426 - acc: 0.9832 - val_loss: 0.0419 - val_acc: 0.9836\n",
      "training time: 1534.4153901371174s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 885 \t AUC: 0.9821149210356713\n",
      "Label: severe_toxic         Threashold count: 269 \t AUC: 0.9918702684464689\n",
      "Label: obscene              Threashold count: 524 \t AUC: 0.9915912929344239\n",
      "Label: threat               Threashold count: 150 \t AUC: 0.9917671465392595\n",
      "Label: insult               Threashold count: 679 \t AUC: 0.9853186255091201\n",
      "Label: identity_hate        Threashold count: 236 \t AUC: 0.9914093625498008\n",
      "\n",
      "ROC: 0.9890119361691241\n",
      "predicting\n",
      "153164/153164 [==============================] - 366s 2ms/step\n",
      "predicting time: 351.41214631684124s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519092872, epochs: 2\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_3_1519094876\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_3_1519094876.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9843\n",
      "Epoch 00001: val_loss improved from inf to 0.04159, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_3_1519094876.hdf5\n",
      "4488/4488 [==============================] - 1590s 354ms/step - loss: 0.0394 - acc: 0.9843 - val_loss: 0.0416 - val_acc: 0.9836\n",
      "training time: 1528.1889372828882s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 889 \t AUC: 0.9824983604458148\n",
      "Label: severe_toxic         Threashold count: 291 \t AUC: 0.991144851437127\n",
      "Label: obscene              Threashold count: 529 \t AUC: 0.9918306269646708\n",
      "Label: threat               Threashold count: 150 \t AUC: 0.994167347708556\n",
      "Label: insult               Threashold count: 675 \t AUC: 0.9861240013646603\n",
      "Label: identity_hate        Threashold count: 257 \t AUC: 0.990905645495619\n",
      "\n",
      "ROC: 0.9894451389027415\n",
      "predicting\n",
      "153164/153164 [==============================] - 366s 2ms/step\n",
      "predicting time: 351.6665858749766s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519094876, epochs: 3\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_4_1519096873\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_4_1519096873.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9850\n",
      "Epoch 00001: val_loss improved from inf to 0.04199, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_4_1519096873.hdf5\n",
      "4488/4488 [==============================] - 1585s 353ms/step - loss: 0.0373 - acc: 0.9850 - val_loss: 0.0420 - val_acc: 0.9831\n",
      "training time: 1523.2899584409315s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 901 \t AUC: 0.9833658651338203\n",
      "Label: severe_toxic         Threashold count: 263 \t AUC: 0.9916180595094999\n",
      "Label: obscene              Threashold count: 516 \t AUC: 0.9922290057814391\n",
      "Label: threat               Threashold count: 154 \t AUC: 0.9956019362544792\n",
      "Label: insult               Threashold count: 676 \t AUC: 0.9860651463533339\n",
      "Label: identity_hate        Threashold count: 261 \t AUC: 0.9907220764209476\n",
      "\n",
      "ROC: 0.9899336815755867\n",
      "predicting\n",
      "153164/153164 [==============================] - 362s 2ms/step\n",
      "predicting time: 348.3373366140295s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519096873, epochs: 4\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_5_1519098862\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_5_1519098862.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9853\n",
      "Epoch 00001: val_loss improved from inf to 0.04122, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_5_1519098862.hdf5\n",
      "4488/4488 [==============================] - 1634s 364ms/step - loss: 0.0359 - acc: 0.9853 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "training time: 1570.127245305106s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 862 \t AUC: 0.9829536947453604\n",
      "Label: severe_toxic         Threashold count: 271 \t AUC: 0.9918896691339281\n",
      "Label: obscene              Threashold count: 511 \t AUC: 0.9922529315159591\n",
      "Label: threat               Threashold count: 156 \t AUC: 0.9937939272018609\n",
      "Label: insult               Threashold count: 667 \t AUC: 0.9863140674795341\n",
      "Label: identity_hate        Threashold count: 235 \t AUC: 0.9910075307236662\n",
      "\n",
      "ROC: 0.9897019701333849\n",
      "predicting\n",
      "153164/153164 [==============================] - 365s 2ms/step\n",
      "predicting time: 349.767417563824s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519098862, epochs: 5\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_6_1519100901\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_6_1519100901.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9860\n",
      "Epoch 00001: val_loss improved from inf to 0.04351, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_6_1519100901.hdf5\n",
      "4488/4488 [==============================] - 1668s 372ms/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0435 - val_acc: 0.9834\n",
      "training time: 1602.9881047708914s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 916 \t AUC: 0.9828203362450687\n",
      "Label: severe_toxic         Threashold count: 275 \t AUC: 0.9910491132620569\n",
      "Label: obscene              Threashold count: 522 \t AUC: 0.9916439755613958\n",
      "Label: threat               Threashold count: 152 \t AUC: 0.99421386810838\n",
      "Label: insult               Threashold count: 681 \t AUC: 0.986244621799961\n",
      "Label: identity_hate        Threashold count: 248 \t AUC: 0.9896509640450544\n",
      "\n",
      "ROC: 0.9892704798369861\n",
      "predicting\n",
      "153164/153164 [==============================] - 366s 2ms/step\n",
      "predicting time: 350.8732415479608s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519100901, epochs: 6\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_7_1519102976\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_7_1519102976.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9863\n",
      "Epoch 00001: val_loss improved from inf to 0.04253, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_7_1519102976.hdf5\n",
      "4488/4488 [==============================] - 1697s 378ms/step - loss: 0.0333 - acc: 0.9863 - val_loss: 0.0425 - val_acc: 0.9839\n",
      "training time: 1632.3232697669882s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 893 \t AUC: 0.9834951095738365\n",
      "Label: severe_toxic         Threashold count: 260 \t AUC: 0.9913666940806816\n",
      "Label: obscene              Threashold count: 534 \t AUC: 0.9919694268989059\n",
      "Label: threat               Threashold count: 151 \t AUC: 0.9952398315207142\n",
      "Label: insult               Threashold count: 650 \t AUC: 0.985829079549662\n",
      "Label: identity_hate        Threashold count: 256 \t AUC: 0.9893268636213524\n",
      "\n",
      "ROC: 0.9895378342075255\n",
      "predicting\n",
      "153164/153164 [==============================] - 364s 2ms/step\n",
      "predicting time: 350.6671442028601s\n",
      "submission shape:\n",
      "(153164, 7)\n",
      "done\n",
      "/home/kai/data/shiyi/toxic/submissions/\n",
      "Saved model loaded: 1519102976, epochs: 7\n",
      "BUILD_ID: FastText_cleaned_GRU_True_200_300_50_50_0.1_32_8_1519105080\n",
      "Model will be saved at: /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_GRU_True_200_300_50_50_0.1_32_8_1519105080.hdf5\n",
      "training\n",
      "steps:\n",
      "4488\n",
      "Epoch 1/1\n",
      "4487/4488 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9888\n",
      "Epoch 00001: val_loss improved from inf to 0.04860, saving model to /home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_LSTM_True_200_300_50_50_0.1_32_12_1519136146.hdf5\n",
      "4488/4488 [==============================] - 2714s 605ms/step - loss: 0.0270 - acc: 0.9888 - val_loss: 0.0486 - val_acc: 0.9833\n",
      "training time: 2608.383683074033s\n",
      "evaluating\n",
      "ROC on each label: \n",
      "Label: toxic                Threashold count: 940 \t AUC: 0.9808629428021527\n",
      "Label: severe_toxic         Threashold count: 285 \t AUC: 0.9909360831699037\n",
      "Label: obscene              Threashold count: 546 \t AUC: 0.9921389008517563\n",
      "Label: threat               Threashold count: 152 \t AUC: 0.9873766266423587\n",
      "Label: insult               Threashold count: 702 \t AUC: 0.9855536413304468\n",
      "Label: identity_hate        Threashold count: 247 \t AUC: 0.9906289744022148\n",
      "\n",
      "ROC: 0.9879161948664722\n",
      "predicting\n",
      " 33664/153164 [=====>........................] - ETA: 5:46"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b36e26752367>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, file_path, x_test)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ROC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1800\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "records_file = 'timefile_FastText.txt'\n",
    "record_csv = 'fasttext_record.csv'\n",
    "\n",
    "#for window_length in [200,250,300]\n",
    "\n",
    "fresh_start = True # if not a fresh start, PREV_ID must be provided\n",
    "add_epoch = 10 # so the total epoch will be previous epoch + add_epoch - 1 (because range is right exclusive)\n",
    "PREV_ID = None #'1518995754'\n",
    "for mode in ['GRU','LSTM']:\n",
    "    for EPOCHS in [1 for i in range(add_epoch)]:\n",
    "\n",
    "        # in data_generator, there are another two parameters: frac and replace (sample with replacement)\n",
    "        DROPOUT = 0.1\n",
    "        BATCH_SIZE = 32    \n",
    "        RNN_UNITS = 50\n",
    "        bidirectional = True\n",
    "        DENSE_UNITS = 50\n",
    "        current_epoch = 1 # 1 for fresh start. if not fresh start, this var does not matter\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        SAVE_DIR = '/home/kai/data/shiyi/toxic/'\n",
    "        SUB_DIR = SAVE_DIR + 'submissions/'\n",
    "        print(SUB_DIR)\n",
    "\n",
    "        ID = str(int(time.time()))\n",
    "        if fresh_start:\n",
    "            print('getting fresh model')\n",
    "            model = get_lstm_model(window_length, n_features, RNN_UNITS, DENSE_UNITS, label_cols, DROPOUT, mode, bidirectional)\n",
    "            BUILD_ID = 'FastText_cleaned_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}'.format(mode, bidirectional, window_length, n_features, RNN_UNITS, DENSE_UNITS, DROPOUT, BATCH_SIZE, current_epoch, ID)\n",
    "            fresh_start = False\n",
    "        else:\n",
    "            if PREV_ID == None:\n",
    "                raise ValueError(\"Since it's not a fresh start, please provide the PREV_ID so the model can be loaded\")\n",
    "            model_saved_dir = SAVE_DIR + 'models/'\n",
    "            LOAD_MODEL_FILE = model_saved_dir + get_filename(model_saved_dir, PREV_ID)\n",
    "            model = get_lstm_model(window_length, n_features, RNN_UNITS, DENSE_UNITS, label_cols, DROPOUT, mode, bidirectional, load_model=True, load_model_file=LOAD_MODEL_FILE)\n",
    "            current_epoch = int(LOAD_MODEL_FILE.split('_')[-2]) + 1\n",
    "            print('Saved model loaded: {}, epochs: {}'.format(PREV_ID, current_epoch - 1))\n",
    "            BUILD_ID = 'FastText_cleaned_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}'.format(mode, bidirectional, window_length, n_features, RNN_UNITS, DENSE_UNITS, DROPOUT, BATCH_SIZE, current_epoch, ID)\n",
    "\n",
    "\n",
    "        PREV_ID = ID\n",
    "        print('BUILD_ID: ' + BUILD_ID)\n",
    "        MODEL_FILE = SAVE_DIR + 'models/mod_' + BUILD_ID + '.hdf5'\n",
    "        print('Model will be saved at: ' +str(MODEL_FILE))\n",
    "\n",
    "        print('training')\n",
    "        model = train_model(model, MODEL_FILE, window_length, n_features, BATCH_SIZE, EPOCHS, df_train, x_val, y_val)\n",
    "    #     print('loading and training')\n",
    "    #     model = train_model(model, MODEL_FILE, window_length, n_features, BATCH_SIZE, EPOCHS, df_train, x_val, y_val, \n",
    "    #                        load_model=True, load_model_file=LOAD_MODEL_FILE)\n",
    "\n",
    "        elapsed_time = timeit.default_timer() - start_time    \n",
    "        print('training time: {}s'.format(elapsed_time))\n",
    "        with open(records_file,'a') as f:\n",
    "            f.write('##################################################################\\n')\n",
    "            f.write('##################################################################\\n')\n",
    "            f.write('\\nBUILD_ID: ' + BUILD_ID + '\\ntraining: '+str(elapsed_time))\n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "        ############################################################################\n",
    "        print('evaluating')\n",
    "        roc, report, auc_per_label = evaluate(model, MODEL_FILE, x_val, y_val, label_cols)\n",
    "        print('ROC on each label: {}'.format(report))\n",
    "        print('ROC: {}'.format(roc))\n",
    "        with open(records_file,'a') as f:\n",
    "            f.write('\\nROC on each label: {}'.format(report))\n",
    "            f.write('\\nROC Average: {}'.format(roc))\n",
    "\n",
    "        ############################################################################\n",
    "        optimizer = 'adam'\n",
    "        rnn_mode = mode\n",
    "        dense_activation = 'tanh'\n",
    "        history = model.history.history\n",
    "        \n",
    "        result = '%s,%s,%s,%d,%d,%s,%s,%d,%d,%s,%d,%d,%d,%.6f,%.6f,%.6f,%.6f,%.6f,%.6f,%.6f,%.6f,%.6f,%.6f,%.6f\\n'\\\n",
    "                        %(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\\\n",
    "                        ID,optimizer,window_length,n_features,bidirectional,rnn_mode,\\\n",
    "                        RNN_UNITS,DENSE_UNITS,dense_activation,DROPOUT,BATCH_SIZE,current_epoch,\\\n",
    "                        history['loss'][0],history['acc'][0],history['val_loss'][0],history['val_acc'][0],\\\n",
    "                        roc,auc_per_label['toxic'],auc_per_label['severe_toxic'],auc_per_label['obscene'],\\\n",
    "                        auc_per_label['threat'],auc_per_label['insult'],auc_per_label['identity_hate'])        \n",
    "        \n",
    "        with open(record_csv, 'a') as f:\n",
    "            f.write(result)\n",
    "        ############################################################################\n",
    "        ############################################################################\n",
    "        print('predicting')\n",
    "        start_time = timeit.default_timer()\n",
    "        y_test = predict(model, MODEL_FILE, x_test)\n",
    "\n",
    "        #print('train predicting')\n",
    "        #y_train = predict(model, MODEL_PATH, X_train)\n",
    "        elapsed_time = timeit.default_timer() - start_time    \n",
    "        print('predicting time: {}s'.format(elapsed_time))\n",
    "        with open(records_file,'a') as f:\n",
    "            f.write('\\npredicting: '+str(elapsed_time)+'\\n')      \n",
    "\n",
    "        ############################################################################\n",
    "        ############################################################################\n",
    "        \n",
    "        save(y_test, label_cols, SUB_DIR)\n",
    "\n",
    "        #save('lstm', y_train, label_cols, SUB_DIR, True)\n",
    "\n",
    "        with open(records_file,'a') as f:\n",
    "            model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "            for key, value in model.history.history.items():\n",
    "                f.write('\\nperformance: '+str(key)+': '+str(value))\n",
    "            for key, value in model.history.params.items():\n",
    "                f.write('\\nparams: '+str(key)+': '+str(value))\n",
    "            f.write('\\n')\n",
    "\n",
    "        \n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record_csv = 'fasttext_record.csv'\n",
    "col_names = 'date,ID,optimizer,window_length,n_features,bidirectional,rnn_mode,RNN_UNITS,DENSE_UNITS,dense_activation,DROPOUT,BATCH_SIZE,EPOCH,trn_loss,trn_acc,val_loss,val_acc,val_auc,toxic,severe_toxic,obscene,threat,insult,identity_hate\\n'\n",
    "with open(record_csv, 'a') as f:\n",
    "    f.write(col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x_trn = df_to_data(shuffled_train)\n",
    "y_trn = shuffled_train[label_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_trn.shape, y_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SAVE_DIR = '/home/kai/data/shiyi/toxic/'\n",
    "model_saved_dir = SAVE_DIR + 'models/'\n",
    "model_file = model_saved_dir + get_filename(model_saved_dir, '1518834132')\n",
    "model_9839 = get_lstm_model(200, 300, 50, 50, label_cols, 0.1, 'LSTM', True, load_model=True, load_model_file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kai/data/shiyi/toxic/models/mod_FastText_cleaned_200_300_50_50_0.1_32_6_1518834132.hdf5'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571/159571 [==============================] - 455s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_trn_preds = predict(model_9839, model_file, x_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUB_DIR = SAVE_DIR + 'submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9839_1519155658'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUILD_ID = '9839_' + str(int(time.time()))\n",
    "BUILD_IDLD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission shape:\n",
      "(159571, 7)\n"
     ]
    }
   ],
   "source": [
    "save(y_trn_preds, label_cols, SUB_DIR, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 433s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = predict(model_9839, model_file, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission shape:\n",
      "(153164, 7)\n"
     ]
    }
   ],
   "source": [
    "save(y_test, label_cols, SUB_DIR, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
