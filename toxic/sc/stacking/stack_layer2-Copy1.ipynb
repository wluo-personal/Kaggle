{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from base_layer_utils import BaseLayerDataRepo, BaseLayerResultsRepo, ModelName\n",
    "from base_layer_utils import SklearnBLE\n",
    "\n",
    "#from fast_text_data import fasttext_data_process\n",
    "#from tfidf_data import tfidf_data_process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.cross_validation import KFold # replace with model_selection?\n",
    "#from sklearn.model_selection import KFold\n",
    "import time, re, gc\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n",
      "(153164, 2)\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "test = pd.read_csv(PATH + 'test.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer2 feature engineer (Add some features and combine them with layer1 model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#######################\n",
    "# FEATURE ENGINEERING #\n",
    "#######################\n",
    "\"\"\"\n",
    "Main function\n",
    "Input: pandas Series and a feature engineering function\n",
    "Output: pandas Series\n",
    "\"\"\"\n",
    "def engineer_feature(series, func, normalize=True):\n",
    "    feature = series.apply(func)\n",
    "       \n",
    "    if normalize:\n",
    "        feature = pd.Series(z_normalize(feature.values.reshape(-1,1)).reshape(-1,))\n",
    "    feature.name = func.__name__ \n",
    "    return feature\n",
    "\n",
    "\"\"\"\n",
    "Engineer features\n",
    "Input: pandas Series and a list of feature engineering functions\n",
    "Output: pandas DataFrame\n",
    "\"\"\"\n",
    "def engineer_features(series, funclist, normalize=True):\n",
    "    features = pd.DataFrame()\n",
    "    for func in funclist:\n",
    "        feature = engineer_feature(series, func, normalize)\n",
    "        features[feature.name] = feature\n",
    "    return features\n",
    "\n",
    "\"\"\"\n",
    "Normalizer\n",
    "Input: NumPy array\n",
    "Output: NumPy array\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "def z_normalize(data):\n",
    "    scaler.fit(data)\n",
    "    return scaler.transform(data)\n",
    "    \n",
    "\"\"\"\n",
    "Feature functions\n",
    "\"\"\"\n",
    "def asterix_freq(x):\n",
    "    return x.count('!')/len(x)\n",
    "\n",
    "def uppercase_freq(x):\n",
    "    return len(re.findall(r'[A-Z]',x))/len(x)\n",
    "\n",
    "INPUT_COLUMN = \"comment_text\"\n",
    "# Engineer features\n",
    "feature_functions = [len, asterix_freq, uppercase_freq]\n",
    "features = [f.__name__ for f in feature_functions]\n",
    "F_train = engineer_features(train[INPUT_COLUMN], feature_functions)\n",
    "F_test = engineer_features(test[INPUT_COLUMN], feature_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_layer_oof_per_label(layer1_oof_dict, label):\n",
    "    \"\"\"\n",
    "    Util method for stacking\n",
    "    \"\"\"\n",
    "    x = None\n",
    "    data_list = layer1_oof_dict[label]\n",
    "    for i in range(len(data_list)):\n",
    "        if i == 0:\n",
    "            x = data_list[0]\n",
    "        else:\n",
    "            x = np.concatenate((x, data_list[i]), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simple blend of two models.\n",
    "### (Ignore this method for now and check out: 2. stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "result = np.empty((test.shape[0],len(label_cols)))\n",
    "\n",
    "# mix the first two models\n",
    "for i, label in enumerate(label_cols):\n",
    "    x_train = combine_layer_oof_per_label(layer1_oof_train, label)\n",
    "    x_test = combine_layer_oof_per_label(layer1_oof_test, label)\n",
    "    for j in range(x_train.shape[1]):\n",
    "        roc = roc_auc_score(train[label], x_train[:,j])\n",
    "        print(label, j, roc) # print out roc for meta feature on meta label (which is just the original train label)\n",
    "    \n",
    "    roc_scores_of_a_label = []\n",
    "    alphas = np.linspace(0,1,1001)\n",
    "    best_roc = 0\n",
    "    best_alpha = 0\n",
    "    for alpha in alphas:\n",
    "        roc = roc_auc_score(train[label], alpha*x_train[:,0] + (1-alpha)*x_train[:,1])\n",
    "        if roc > best_roc:\n",
    "            best_roc = roc\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(label, best_roc, best_alpha)\n",
    "    result[:,i] = best_alpha*x_test[:,0] + (1-best_alpha)*x_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "submission[label_cols] = result\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./StackPreds/mixtwo_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "# load the saved repo. IMPORTANT: set load_from_file to True! or you will overwrite the saved repo\n",
    "base_layer_results_repo = BaseLayerResultsRepo(load_from_file=True, filepath='obj/WithPreprocessedFile/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888\tModelName.NBLOGREG_tfidf_word_(1, 1)_30000_1_1.0\n",
      "0.9777\tModelName.LOGREG_tfidf_word_(1, 1)_30000_1_1.0\n",
      "0.9666\tModelName.LOGREG_PERLABEL_tfidf_word_(1, 1)_30000_1_1.0\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# two ways to choose model_data from the repo: set a threashold, or give a list to ones you want\n",
    "# a list can be sth like: \n",
    "# chosen = ['ModelName.NBLOGREG_tfidf_word_(1, 1)_30000_1_1.0'\n",
    "#          'ModelName.LOGREG_tfidf_word_(1, 1)_30000_1_1.0']\n",
    "layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('./xgb_search.csv', 'a')\n",
    "# header = 'time,id,th,amt,csbt,lr,md,ss,ga,a,rounds,folds,tops,sps,ops,\\\n",
    "# thps,inps,ihps,tobr,sbr,obr,thbr,inbr,ihbr,sth1,sth2,sth3,sth4,sth5,\\\n",
    "# to_auc,s_auc,o_auc,th_auc,in_auc,ih_auc,avg_auc\\n'\n",
    "header = 'time,id,threshold,num_models,colsample_bytree,lr,max_depth,subsample,\\\n",
    "gamma,alpha,cv_num_round,cv_nfolds,toxic_pos_scale,severe_toxic_pos_scale,obscene_pos_scale,\\\n",
    "threat_pos_scale,insult_pos_scale,identity_hate_pos_scale,sth1,sth2,sth3,sth4,sth5,\\\n",
    "toxic_best_round,severe_toxic_best_round,obscene_best_round,threat_best_round,\\\n",
    "insult_best_round,identity_hate_best_round,toxic_auc,severe_toxic_auc,obscene_auc,\\\n",
    "threat_auc,insult_auc,identity_hate_auc,avg_auc\\n'\n",
    "f.write(header)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "    \n",
    "    return est.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 00:25:50, id: 1519968350, th: 0.980300, num_models: 5, colsample_bytree: 0.700000, lr: 0.0152982,     max_depth: 4, subsample: 0.690000, gamma: 2, alpha: 0, cv_num_round: 633, cv_nfolds: 4,    to_pw: 6, s_pw: 74, ob_pw: 14, th_pw: 139, in_pw: 10, ih_pw: 128            \n",
      "time: 2018-03-02 00:34:34, id: 1519968874, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0105918,     max_depth: 4, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 729, cv_nfolds: 4,    to_pw: 7, s_pw: 121, ob_pw: 23, th_pw: 292, in_pw: 21, ih_pw: 42            \n",
      "time: 2018-03-02 00:40:43, id: 1519969243, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0027763,     max_depth: 4, subsample: 0.810000, gamma: 0, alpha: 1, cv_num_round: 471, cv_nfolds: 3,    to_pw: 7, s_pw: 52, ob_pw: 24, th_pw: 251, in_pw: 13, ih_pw: 118            \n",
      "time: 2018-03-02 00:44:33, id: 1519969473, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0010538,     max_depth: 7, subsample: 0.560000, gamma: 1, alpha: 1, cv_num_round: 481, cv_nfolds: 3,    to_pw: 13, s_pw: 26, ob_pw: 3, th_pw: 105, in_pw: 13, ih_pw: 66            \n",
      "time: 2018-03-02 00:47:37, id: 1519969657, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0018798,     max_depth: 3, subsample: 0.810000, gamma: 1, alpha: 0, cv_num_round: 834, cv_nfolds: 4,    to_pw: 12, s_pw: 43, ob_pw: 13, th_pw: 207, in_pw: 5, ih_pw: 137            \n",
      "time: 2018-03-02 00:54:48, id: 1519970088, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0966631,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 607, cv_nfolds: 4,    to_pw: 10, s_pw: 103, ob_pw: 18, th_pw: 120, in_pw: 8, ih_pw: 136            \n",
      "time: 2018-03-02 00:58:52, id: 1519970332, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0038114,     max_depth: 6, subsample: 0.890000, gamma: 0, alpha: 1, cv_num_round: 618, cv_nfolds: 3,    to_pw: 3, s_pw: 34, ob_pw: 11, th_pw: 110, in_pw: 13, ih_pw: 55            \n",
      "time: 2018-03-02 01:04:15, id: 1519970655, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0100228,     max_depth: 5, subsample: 0.560000, gamma: 1, alpha: 1, cv_num_round: 637, cv_nfolds: 3,    to_pw: 6, s_pw: 31, ob_pw: 9, th_pw: 218, in_pw: 24, ih_pw: 125            \n",
      "time: 2018-03-02 01:11:03, id: 1519971063, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0017185,     max_depth: 2, subsample: 0.990000, gamma: 1, alpha: 0, cv_num_round: 989, cv_nfolds: 3,    to_pw: 12, s_pw: 67, ob_pw: 23, th_pw: 144, in_pw: 10, ih_pw: 91            \n",
      "time: 2018-03-02 01:18:06, id: 1519971486, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0027707,     max_depth: 5, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 708, cv_nfolds: 3,    to_pw: 7, s_pw: 96, ob_pw: 22, th_pw: 279, in_pw: 24, ih_pw: 85            \n",
      "time: 2018-03-02 01:22:14, id: 1519971734, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0231520,     max_depth: 4, subsample: 0.620000, gamma: 2, alpha: 1, cv_num_round: 625, cv_nfolds: 3,    to_pw: 8, s_pw: 51, ob_pw: 6, th_pw: 102, in_pw: 13, ih_pw: 87            \n",
      "time: 2018-03-02 01:28:41, id: 1519972121, th: 0.976800, num_models: 10, colsample_bytree: 0.500000, lr: 0.0329555,     max_depth: 6, subsample: 0.910000, gamma: 2, alpha: 0, cv_num_round: 896, cv_nfolds: 3,    to_pw: 9, s_pw: 59, ob_pw: 20, th_pw: 339, in_pw: 13, ih_pw: 78            \n",
      "time: 2018-03-02 01:34:19, id: 1519972459, th: 0.980300, num_models: 5, colsample_bytree: 0.500000, lr: 0.0071469,     max_depth: 4, subsample: 0.750000, gamma: 2, alpha: 0, cv_num_round: 614, cv_nfolds: 4,    to_pw: 8, s_pw: 23, ob_pw: 3, th_pw: 185, in_pw: 12, ih_pw: 138            \n",
      "time: 2018-03-02 01:41:34, id: 1519972894, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0266129,     max_depth: 2, subsample: 0.560000, gamma: 0, alpha: 0, cv_num_round: 405, cv_nfolds: 4,    to_pw: 5, s_pw: 36, ob_pw: 24, th_pw: 188, in_pw: 7, ih_pw: 65            \n",
      "time: 2018-03-02 01:50:22, id: 1519973422, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0035740,     max_depth: 5, subsample: 0.700000, gamma: 1, alpha: 0, cv_num_round: 425, cv_nfolds: 3,    to_pw: 13, s_pw: 110, ob_pw: 3, th_pw: 64, in_pw: 19, ih_pw: 121            \n",
      "time: 2018-03-02 01:55:49, id: 1519973749, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0818469,     max_depth: 2, subsample: 0.730000, gamma: 1, alpha: 1, cv_num_round: 570, cv_nfolds: 3,    to_pw: 12, s_pw: 71, ob_pw: 8, th_pw: 124, in_pw: 17, ih_pw: 68            \n",
      "time: 2018-03-02 01:59:46, id: 1519973986, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0615287,     max_depth: 2, subsample: 0.580000, gamma: 1, alpha: 1, cv_num_round: 519, cv_nfolds: 4,    to_pw: 14, s_pw: 122, ob_pw: 15, th_pw: 110, in_pw: 19, ih_pw: 72            \n",
      "time: 2018-03-02 02:07:07, id: 1519974427, th: 0.978600, num_models: 8, colsample_bytree: 0.900000, lr: 0.0034895,     max_depth: 6, subsample: 0.970000, gamma: 1, alpha: 0, cv_num_round: 693, cv_nfolds: 4,    to_pw: 12, s_pw: 76, ob_pw: 16, th_pw: 99, in_pw: 23, ih_pw: 41            \n",
      "time: 2018-03-02 02:15:58, id: 1519974958, th: 0.980300, num_models: 5, colsample_bytree: 0.800000, lr: 0.0031782,     max_depth: 3, subsample: 0.630000, gamma: 2, alpha: 0, cv_num_round: 641, cv_nfolds: 4,    to_pw: 12, s_pw: 22, ob_pw: 21, th_pw: 198, in_pw: 21, ih_pw: 46            \n",
      "time: 2018-03-02 02:22:08, id: 1519975328, th: 0.980300, num_models: 5, colsample_bytree: 0.700000, lr: 0.0228572,     max_depth: 4, subsample: 0.580000, gamma: 2, alpha: 0, cv_num_round: 482, cv_nfolds: 4,    to_pw: 2, s_pw: 46, ob_pw: 9, th_pw: 76, in_pw: 23, ih_pw: 96            \n",
      "time: 2018-03-02 02:31:13, id: 1519975873, th: 0.981500, num_models: 4, colsample_bytree: 0.700000, lr: 0.0137914,     max_depth: 6, subsample: 0.920000, gamma: 2, alpha: 0, cv_num_round: 787, cv_nfolds: 4,    to_pw: 12, s_pw: 37, ob_pw: 11, th_pw: 171, in_pw: 11, ih_pw: 57            \n",
      "time: 2018-03-02 02:39:26, id: 1519976366, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0858544,     max_depth: 5, subsample: 0.580000, gamma: 0, alpha: 1, cv_num_round: 876, cv_nfolds: 3,    to_pw: 4, s_pw: 46, ob_pw: 16, th_pw: 100, in_pw: 20, ih_pw: 116            \n",
      "time: 2018-03-02 02:42:07, id: 1519976527, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0616493,     max_depth: 4, subsample: 0.710000, gamma: 0, alpha: 0, cv_num_round: 689, cv_nfolds: 4,    to_pw: 14, s_pw: 88, ob_pw: 19, th_pw: 343, in_pw: 23, ih_pw: 120            \n",
      "time: 2018-03-02 02:47:13, id: 1519976833, th: 0.977400, num_models: 9, colsample_bytree: 0.600000, lr: 0.0070077,     max_depth: 5, subsample: 0.600000, gamma: 1, alpha: 1, cv_num_round: 849, cv_nfolds: 3,    to_pw: 3, s_pw: 119, ob_pw: 3, th_pw: 107, in_pw: 13, ih_pw: 111            \n",
      "time: 2018-03-02 02:52:27, id: 1519977147, th: 0.981500, num_models: 4, colsample_bytree: 0.600000, lr: 0.0983639,     max_depth: 2, subsample: 0.500000, gamma: 1, alpha: 1, cv_num_round: 734, cv_nfolds: 3,    to_pw: 11, s_pw: 27, ob_pw: 22, th_pw: 227, in_pw: 9, ih_pw: 73            \n",
      "time: 2018-03-02 02:55:45, id: 1519977345, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0018059,     max_depth: 7, subsample: 0.560000, gamma: 2, alpha: 0, cv_num_round: 895, cv_nfolds: 3,    to_pw: 6, s_pw: 53, ob_pw: 14, th_pw: 168, in_pw: 4, ih_pw: 131            \n",
      "time: 2018-03-02 02:59:22, id: 1519977562, th: 0.981500, num_models: 4, colsample_bytree: 0.900000, lr: 0.0227284,     max_depth: 6, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 949, cv_nfolds: 4,    to_pw: 10, s_pw: 73, ob_pw: 19, th_pw: 168, in_pw: 6, ih_pw: 135            \n",
      "time: 2018-03-02 03:04:34, id: 1519977874, th: 0.976800, num_models: 10, colsample_bytree: 0.800000, lr: 0.0017340,     max_depth: 7, subsample: 0.850000, gamma: 2, alpha: 1, cv_num_round: 451, cv_nfolds: 4,    to_pw: 5, s_pw: 20, ob_pw: 22, th_pw: 312, in_pw: 20, ih_pw: 53            \n",
      "time: 2018-03-02 03:12:52, id: 1519978372, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0015783,     max_depth: 3, subsample: 0.880000, gamma: 1, alpha: 0, cv_num_round: 742, cv_nfolds: 4,    to_pw: 9, s_pw: 122, ob_pw: 22, th_pw: 318, in_pw: 15, ih_pw: 39            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 03:22:59, id: 1519978979, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0685323,     max_depth: 2, subsample: 0.880000, gamma: 2, alpha: 1, cv_num_round: 984, cv_nfolds: 4,    to_pw: 8, s_pw: 87, ob_pw: 11, th_pw: 93, in_pw: 10, ih_pw: 109            \n",
      "time: 2018-03-02 03:31:54, id: 1519979514, th: 0.976800, num_models: 10, colsample_bytree: 0.900000, lr: 0.0392708,     max_depth: 5, subsample: 0.630000, gamma: 0, alpha: 0, cv_num_round: 498, cv_nfolds: 4,    to_pw: 11, s_pw: 121, ob_pw: 23, th_pw: 351, in_pw: 23, ih_pw: 57            \n",
      "time: 2018-03-02 03:37:12, id: 1519979832, th: 0.977400, num_models: 9, colsample_bytree: 0.700000, lr: 0.0030536,     max_depth: 6, subsample: 0.700000, gamma: 0, alpha: 1, cv_num_round: 873, cv_nfolds: 4,    to_pw: 4, s_pw: 114, ob_pw: 7, th_pw: 341, in_pw: 4, ih_pw: 118            \n",
      "time: 2018-03-02 03:45:08, id: 1519980308, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0649149,     max_depth: 7, subsample: 0.660000, gamma: 2, alpha: 1, cv_num_round: 895, cv_nfolds: 4,    to_pw: 7, s_pw: 97, ob_pw: 16, th_pw: 68, in_pw: 11, ih_pw: 101            \n",
      "time: 2018-03-02 03:50:00, id: 1519980600, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0491951,     max_depth: 7, subsample: 0.630000, gamma: 0, alpha: 1, cv_num_round: 843, cv_nfolds: 4,    to_pw: 4, s_pw: 25, ob_pw: 14, th_pw: 184, in_pw: 24, ih_pw: 41            \n",
      "time: 2018-03-02 03:54:44, id: 1519980884, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0140443,     max_depth: 5, subsample: 0.800000, gamma: 2, alpha: 0, cv_num_round: 704, cv_nfolds: 4,    to_pw: 6, s_pw: 69, ob_pw: 5, th_pw: 109, in_pw: 10, ih_pw: 54            \n",
      "time: 2018-03-02 04:06:50, id: 1519981610, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0191208,     max_depth: 7, subsample: 0.570000, gamma: 0, alpha: 1, cv_num_round: 927, cv_nfolds: 4,    to_pw: 11, s_pw: 111, ob_pw: 22, th_pw: 135, in_pw: 13, ih_pw: 115            \n",
      "time: 2018-03-02 04:15:21, id: 1519982121, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0030881,     max_depth: 7, subsample: 0.780000, gamma: 1, alpha: 0, cv_num_round: 996, cv_nfolds: 3,    to_pw: 13, s_pw: 125, ob_pw: 8, th_pw: 166, in_pw: 23, ih_pw: 88            \n",
      "time: 2018-03-02 04:20:01, id: 1519982401, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0424453,     max_depth: 2, subsample: 0.830000, gamma: 0, alpha: 0, cv_num_round: 548, cv_nfolds: 4,    to_pw: 12, s_pw: 84, ob_pw: 3, th_pw: 326, in_pw: 8, ih_pw: 99            \n",
      "time: 2018-03-02 04:28:01, id: 1519982881, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0268777,     max_depth: 2, subsample: 0.850000, gamma: 2, alpha: 1, cv_num_round: 669, cv_nfolds: 4,    to_pw: 2, s_pw: 85, ob_pw: 19, th_pw: 264, in_pw: 24, ih_pw: 69            \n",
      "time: 2018-03-02 04:39:06, id: 1519983546, th: 0.977400, num_models: 9, colsample_bytree: 0.500000, lr: 0.0082537,     max_depth: 4, subsample: 0.540000, gamma: 1, alpha: 1, cv_num_round: 997, cv_nfolds: 3,    to_pw: 14, s_pw: 92, ob_pw: 3, th_pw: 303, in_pw: 10, ih_pw: 31            \n",
      "time: 2018-03-02 04:50:55, id: 1519984255, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0024649,     max_depth: 2, subsample: 0.950000, gamma: 2, alpha: 1, cv_num_round: 811, cv_nfolds: 4,    to_pw: 2, s_pw: 20, ob_pw: 16, th_pw: 267, in_pw: 11, ih_pw: 76            \n",
      "time: 2018-03-02 05:04:24, id: 1519985064, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0058243,     max_depth: 2, subsample: 0.720000, gamma: 1, alpha: 1, cv_num_round: 484, cv_nfolds: 3,    to_pw: 12, s_pw: 74, ob_pw: 11, th_pw: 323, in_pw: 16, ih_pw: 104            \n",
      "time: 2018-03-02 05:11:20, id: 1519985480, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0681723,     max_depth: 7, subsample: 0.890000, gamma: 2, alpha: 0, cv_num_round: 441, cv_nfolds: 3,    to_pw: 7, s_pw: 29, ob_pw: 15, th_pw: 261, in_pw: 7, ih_pw: 71            \n",
      "time: 2018-03-02 05:14:48, id: 1519985688, th: 0.976800, num_models: 10, colsample_bytree: 0.500000, lr: 0.0133457,     max_depth: 2, subsample: 0.970000, gamma: 0, alpha: 1, cv_num_round: 688, cv_nfolds: 4,    to_pw: 10, s_pw: 33, ob_pw: 19, th_pw: 149, in_pw: 17, ih_pw: 92            \n",
      "time: 2018-03-02 05:26:12, id: 1519986372, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0256713,     max_depth: 5, subsample: 0.760000, gamma: 1, alpha: 0, cv_num_round: 999, cv_nfolds: 3,    to_pw: 13, s_pw: 31, ob_pw: 21, th_pw: 344, in_pw: 9, ih_pw: 69            \n",
      "time: 2018-03-02 05:32:05, id: 1519986725, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0030848,     max_depth: 7, subsample: 0.620000, gamma: 0, alpha: 0, cv_num_round: 748, cv_nfolds: 4,    to_pw: 14, s_pw: 95, ob_pw: 13, th_pw: 66, in_pw: 24, ih_pw: 132            \n",
      "time: 2018-03-02 05:40:19, id: 1519987219, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0178147,     max_depth: 4, subsample: 0.910000, gamma: 1, alpha: 1, cv_num_round: 668, cv_nfolds: 4,    to_pw: 10, s_pw: 81, ob_pw: 23, th_pw: 133, in_pw: 9, ih_pw: 78            \n",
      "time: 2018-03-02 05:54:17, id: 1519988057, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0141059,     max_depth: 4, subsample: 0.740000, gamma: 0, alpha: 0, cv_num_round: 526, cv_nfolds: 3,    to_pw: 14, s_pw: 38, ob_pw: 9, th_pw: 85, in_pw: 23, ih_pw: 101            \n",
      "time: 2018-03-02 06:00:17, id: 1519988417, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0227382,     max_depth: 5, subsample: 0.800000, gamma: 1, alpha: 1, cv_num_round: 935, cv_nfolds: 3,    to_pw: 14, s_pw: 27, ob_pw: 4, th_pw: 305, in_pw: 8, ih_pw: 48            \n",
      "time: 2018-03-02 06:07:11, id: 1519988831, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0010075,     max_depth: 6, subsample: 0.580000, gamma: 2, alpha: 1, cv_num_round: 838, cv_nfolds: 4,    to_pw: 3, s_pw: 59, ob_pw: 24, th_pw: 271, in_pw: 11, ih_pw: 67            \n",
      "time: 2018-03-02 06:13:43, id: 1519989223, th: 0.981500, num_models: 4, colsample_bytree: 0.900000, lr: 0.0068811,     max_depth: 6, subsample: 0.760000, gamma: 1, alpha: 1, cv_num_round: 787, cv_nfolds: 3,    to_pw: 11, s_pw: 113, ob_pw: 5, th_pw: 197, in_pw: 10, ih_pw: 59            \n",
      "time: 2018-03-02 06:20:00, id: 1519989600, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0017916,     max_depth: 2, subsample: 0.510000, gamma: 2, alpha: 1, cv_num_round: 576, cv_nfolds: 4,    to_pw: 10, s_pw: 48, ob_pw: 7, th_pw: 227, in_pw: 7, ih_pw: 115            \n",
      "time: 2018-03-02 06:27:06, id: 1519990026, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0240391,     max_depth: 5, subsample: 0.600000, gamma: 1, alpha: 0, cv_num_round: 736, cv_nfolds: 4,    to_pw: 10, s_pw: 99, ob_pw: 12, th_pw: 348, in_pw: 12, ih_pw: 107            \n",
      "time: 2018-03-02 06:35:47, id: 1519990547, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0121182,     max_depth: 6, subsample: 0.930000, gamma: 1, alpha: 1, cv_num_round: 566, cv_nfolds: 3,    to_pw: 8, s_pw: 116, ob_pw: 24, th_pw: 181, in_pw: 20, ih_pw: 57            \n",
      "time: 2018-03-02 06:44:48, id: 1519991088, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0372951,     max_depth: 4, subsample: 0.590000, gamma: 0, alpha: 0, cv_num_round: 893, cv_nfolds: 4,    to_pw: 10, s_pw: 29, ob_pw: 12, th_pw: 185, in_pw: 8, ih_pw: 81            \n",
      "time: 2018-03-02 06:52:03, id: 1519991523, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0431916,     max_depth: 3, subsample: 0.990000, gamma: 0, alpha: 0, cv_num_round: 629, cv_nfolds: 4,    to_pw: 2, s_pw: 33, ob_pw: 8, th_pw: 102, in_pw: 20, ih_pw: 88            \n",
      "time: 2018-03-02 06:59:56, id: 1519991996, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0202178,     max_depth: 5, subsample: 0.870000, gamma: 0, alpha: 0, cv_num_round: 914, cv_nfolds: 4,    to_pw: 8, s_pw: 49, ob_pw: 4, th_pw: 240, in_pw: 15, ih_pw: 91            \n",
      "time: 2018-03-02 07:09:02, id: 1519992542, th: 0.978600, num_models: 8, colsample_bytree: 0.700000, lr: 0.0229555,     max_depth: 2, subsample: 0.990000, gamma: 0, alpha: 1, cv_num_round: 444, cv_nfolds: 3,    to_pw: 5, s_pw: 113, ob_pw: 19, th_pw: 324, in_pw: 9, ih_pw: 78            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 07:15:18, id: 1519992918, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0010395,     max_depth: 7, subsample: 0.840000, gamma: 1, alpha: 1, cv_num_round: 695, cv_nfolds: 4,    to_pw: 4, s_pw: 85, ob_pw: 23, th_pw: 341, in_pw: 9, ih_pw: 105            \n",
      "time: 2018-03-02 07:21:05, id: 1519993265, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0032795,     max_depth: 2, subsample: 0.860000, gamma: 1, alpha: 1, cv_num_round: 544, cv_nfolds: 3,    to_pw: 9, s_pw: 43, ob_pw: 9, th_pw: 69, in_pw: 19, ih_pw: 61            \n",
      "time: 2018-03-02 07:28:58, id: 1519993738, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0188473,     max_depth: 3, subsample: 0.940000, gamma: 2, alpha: 0, cv_num_round: 405, cv_nfolds: 3,    to_pw: 14, s_pw: 114, ob_pw: 17, th_pw: 344, in_pw: 24, ih_pw: 30            \n",
      "time: 2018-03-02 07:35:41, id: 1519994141, th: 0.977400, num_models: 9, colsample_bytree: 0.800000, lr: 0.0688231,     max_depth: 2, subsample: 0.650000, gamma: 2, alpha: 1, cv_num_round: 456, cv_nfolds: 3,    to_pw: 13, s_pw: 71, ob_pw: 5, th_pw: 294, in_pw: 13, ih_pw: 84            \n",
      "time: 2018-03-02 07:40:04, id: 1519994404, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0164190,     max_depth: 2, subsample: 0.770000, gamma: 0, alpha: 1, cv_num_round: 578, cv_nfolds: 4,    to_pw: 9, s_pw: 124, ob_pw: 15, th_pw: 116, in_pw: 21, ih_pw: 30            \n",
      "time: 2018-03-02 07:49:39, id: 1519994979, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0102957,     max_depth: 3, subsample: 0.740000, gamma: 0, alpha: 1, cv_num_round: 797, cv_nfolds: 4,    to_pw: 10, s_pw: 83, ob_pw: 9, th_pw: 150, in_pw: 7, ih_pw: 111            \n",
      "time: 2018-03-02 08:00:15, id: 1519995615, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0053757,     max_depth: 6, subsample: 0.670000, gamma: 0, alpha: 1, cv_num_round: 678, cv_nfolds: 4,    to_pw: 5, s_pw: 66, ob_pw: 5, th_pw: 214, in_pw: 9, ih_pw: 53            \n",
      "time: 2018-03-02 08:09:14, id: 1519996154, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0354215,     max_depth: 6, subsample: 0.520000, gamma: 1, alpha: 1, cv_num_round: 839, cv_nfolds: 3,    to_pw: 14, s_pw: 39, ob_pw: 18, th_pw: 364, in_pw: 13, ih_pw: 89            \n",
      "time: 2018-03-02 08:14:30, id: 1519996470, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0027659,     max_depth: 5, subsample: 0.690000, gamma: 0, alpha: 0, cv_num_round: 543, cv_nfolds: 4,    to_pw: 13, s_pw: 102, ob_pw: 10, th_pw: 350, in_pw: 13, ih_pw: 57            \n",
      "time: 2018-03-02 08:21:41, id: 1519996901, th: 0.977400, num_models: 9, colsample_bytree: 0.700000, lr: 0.0465120,     max_depth: 3, subsample: 0.690000, gamma: 0, alpha: 1, cv_num_round: 892, cv_nfolds: 4,    to_pw: 11, s_pw: 61, ob_pw: 5, th_pw: 364, in_pw: 21, ih_pw: 61            \n",
      "time: 2018-03-02 08:28:42, id: 1519997322, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0124011,     max_depth: 7, subsample: 0.710000, gamma: 0, alpha: 1, cv_num_round: 440, cv_nfolds: 4,    to_pw: 2, s_pw: 108, ob_pw: 7, th_pw: 212, in_pw: 24, ih_pw: 30            \n",
      "time: 2018-03-02 08:36:45, id: 1519997805, th: 0.976800, num_models: 10, colsample_bytree: 0.700000, lr: 0.0310058,     max_depth: 4, subsample: 0.900000, gamma: 1, alpha: 0, cv_num_round: 831, cv_nfolds: 3,    to_pw: 8, s_pw: 72, ob_pw: 12, th_pw: 200, in_pw: 5, ih_pw: 67            \n",
      "time: 2018-03-02 08:43:18, id: 1519998198, th: 0.978600, num_models: 8, colsample_bytree: 0.800000, lr: 0.0168915,     max_depth: 7, subsample: 0.670000, gamma: 2, alpha: 1, cv_num_round: 570, cv_nfolds: 4,    to_pw: 2, s_pw: 69, ob_pw: 21, th_pw: 307, in_pw: 6, ih_pw: 133            \n",
      "time: 2018-03-02 08:53:25, id: 1519998805, th: 0.977400, num_models: 9, colsample_bytree: 0.500000, lr: 0.0405281,     max_depth: 7, subsample: 0.860000, gamma: 1, alpha: 0, cv_num_round: 934, cv_nfolds: 3,    to_pw: 7, s_pw: 51, ob_pw: 22, th_pw: 107, in_pw: 16, ih_pw: 136            \n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(130):\n",
    "    now = get_time()\n",
    "    search_id = int(time.time())\n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    model_threshold = np.random.choice(ths)#[0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "        \n",
    "    xgb_colsample_bytree = np.random.randint(5, 10)/10\n",
    "    xgb_learning_rate = 1e-2 * (0.1 ** (np.random.rand() * 2 - 1.0)) # 0.001 to 0.0997\n",
    "    xgb_max_depth = np.random.randint(2, 8)\n",
    "    xgb_subsample = np.random.randint(50, 100)/100\n",
    "    xgb_gamma = np.random.randint(0, 3)\n",
    "    xgb_alpha = np.random.randint(0, 2)\n",
    "\n",
    "    xgb_cv_seed = 0\n",
    "    xgb_cv_num_round = np.random.randint(400, 1000)\n",
    "    xgb_cv_nfolds = np.random.randint(3,5)\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = np.random.randint(2, 15) #10\n",
    "    scale_pos_weights['severe_toxic'] = np.random.randint(20, 130) # 100\n",
    "    scale_pos_weights['obscene'] = np.random.randint(3, 25) # 17\n",
    "    scale_pos_weights['threat'] = np.random.randint(60, 380) # 333\n",
    "    scale_pos_weights['insult'] = np.random.randint(4, 25) # 20\n",
    "    scale_pos_weights['identity_hate'] = np.random.randint(30, 140) #112\n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    num_models = len(layer1_oof_train_loaded['toxic'])\n",
    "    #print('Stacking {} models'.format(num_models)) # number of models that will be stacked\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate']))\n",
    "    \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {} # all labels\n",
    "    best_nrounds = {}  # all labels\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "    #     clf = XGBClassifier()\n",
    "    #     #scores = cross_val_score(clf, x_train, train[label], cv=3, scoring='roc_auc')\n",
    "    #     #print(scores)\n",
    "    #     #print(\"Stacking-CV: ROC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    #     clf.fit(x_train, train[label])\n",
    "    #     result[:, i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds_for_current_label = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "\n",
    "        #print('Ensemble-CV: {}: {}+{}'.format(label, cv_mean, cv_std))\n",
    "        metric_dict[label] = cv_mean\n",
    "        best_nrounds[label] = best_nrounds_for_current_label\n",
    "        #metric_dict[label]['cv_mean'] = cv_mean\n",
    "        #metric_dict[label]['cv_std'] = cv_std\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds_for_current_label)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "          \n",
    "    res = '%s,%d,%f,%d,%f,%.7f,%d,%f,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f\\n\\\n",
    "            '%(now,search_id,model_threshold,num_models,xgb_colsample_bytree,\\\n",
    "               xgb_learning_rate,xgb_max_depth,xgb_subsample,xgb_gamma,xgb_alpha,\\\n",
    "               xgb_cv_num_round,xgb_cv_nfolds,scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "               scale_pos_weights['obscene'],scale_pos_weights['threat'],scale_pos_weights['insult'],\\\n",
    "               scale_pos_weights['identity_hate'],-99,-99,-99,-99,-99,best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "               best_nrounds['obscene'],best_nrounds['threat'],best_nrounds['insult'],\\\n",
    "               best_nrounds['identity_hate'],metric_dict['toxic'],metric_dict['severe_toxic'],\\\n",
    "               metric_dict['obscene'],metric_dict['threat'],metric_dict['insult'],\\\n",
    "               metric_dict['identity_hate'],avg_auc)\n",
    "\n",
    "    f = open('./xgb_search.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()\n",
    "\n",
    "#     sub_tile = 'stacking_test_'\n",
    "#     submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "#     submission[label_cols] = result\n",
    "#     submission.to_csv('./StackPreds/' + sub_tile + str(search_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only to get best rounnds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-07 09:27:15, id: 1520432835, th: 0.979300, num_models: 8, colsample_bytree: 0.800000, lr: 0.0800000,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 500, cv_nfolds: 4,    to_pw: 10, s_pw: 100, ob_pw: 17, th_pw: 333, in_pw: 20, ih_pw: 112            \n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(1):\n",
    "    now = get_time()\n",
    "    search_id = int(time.time())\n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    model_threshold = 0.9793\n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "        \n",
    "    xgb_colsample_bytree = 0.8\n",
    "    xgb_learning_rate = 0.08\n",
    "    xgb_max_depth = 3\n",
    "    xgb_subsample = 0.9\n",
    "    xgb_gamma = 2\n",
    "    xgb_alpha = 0\n",
    "\n",
    "    xgb_cv_seed = 0\n",
    "    xgb_cv_num_round = 500\n",
    "    xgb_cv_nfolds = 4\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = 10 #1\n",
    "    scale_pos_weights['severe_toxic'] = 100 #1\n",
    "    scale_pos_weights['obscene'] = 17\n",
    "    scale_pos_weights['threat'] = 333\n",
    "    scale_pos_weights['insult'] = 20\n",
    "    scale_pos_weights['identity_hate'] = 112\n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    num_models = len(layer1_oof_train_loaded['toxic'])\n",
    "    #print('Stacking {} models'.format(num_models)) # number of models that will be stacked\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate']))\n",
    "    \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {} # all labels\n",
    "    best_nrounds = {}  # all labels\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "    #     clf = XGBClassifier()\n",
    "    #     #scores = cross_val_score(clf, x_train, train[label], cv=3, scoring='roc_auc')\n",
    "    #     #print(scores)\n",
    "    #     #print(\"Stacking-CV: ROC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    #     clf.fit(x_train, train[label])\n",
    "    #     result[:, i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds_for_current_label = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "\n",
    "        #print('Ensemble-CV: {}: {}+{}'.format(label, cv_mean, cv_std))\n",
    "        metric_dict[label] = cv_mean\n",
    "        best_nrounds[label] = best_nrounds_for_current_label\n",
    "        #metric_dict[label]['cv_mean'] = cv_mean\n",
    "        #metric_dict[label]['cv_std'] = cv_std\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds_for_current_label)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "          \n",
    "    res = '%s,%d,%f,%d,%f,%.7f,%d,%f,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f\\n\\\n",
    "            '%(now,search_id,model_threshold,num_models,xgb_colsample_bytree,\\\n",
    "               xgb_learning_rate,xgb_max_depth,xgb_subsample,xgb_gamma,xgb_alpha,\\\n",
    "               xgb_cv_num_round,xgb_cv_nfolds,scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "               scale_pos_weights['obscene'],scale_pos_weights['threat'],scale_pos_weights['insult'],\\\n",
    "               scale_pos_weights['identity_hate'],-99,-99,-99,-99,-99,best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "               best_nrounds['obscene'],best_nrounds['threat'],best_nrounds['insult'],\\\n",
    "               best_nrounds['identity_hate'],metric_dict['toxic'],metric_dict['severe_toxic'],\\\n",
    "               metric_dict['obscene'],metric_dict['threat'],metric_dict['insult'],\\\n",
    "               metric_dict['identity_hate'],avg_auc)\n",
    "\n",
    "    f = open('./xgb_search.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb random search top N training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>threat_best_round</th>\n",
       "      <th>insult_best_round</th>\n",
       "      <th>identity_hate_best_round</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-02 00:54:48</td>\n",
       "      <td>1519970088</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.096663</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>74</td>\n",
       "      <td>98</td>\n",
       "      <td>0.987579</td>\n",
       "      <td>0.991802</td>\n",
       "      <td>0.995448</td>\n",
       "      <td>0.994074</td>\n",
       "      <td>0.990130</td>\n",
       "      <td>0.991359</td>\n",
       "      <td>0.991732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2018-03-02 08:36:45</td>\n",
       "      <td>1519997805</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.031006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>273</td>\n",
       "      <td>163</td>\n",
       "      <td>195</td>\n",
       "      <td>0.987640</td>\n",
       "      <td>0.991029</td>\n",
       "      <td>0.995461</td>\n",
       "      <td>0.993971</td>\n",
       "      <td>0.990103</td>\n",
       "      <td>0.991239</td>\n",
       "      <td>0.991574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2018-03-02 07:35:41</td>\n",
       "      <td>1519994141</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.068823</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>104</td>\n",
       "      <td>0.987659</td>\n",
       "      <td>0.991810</td>\n",
       "      <td>0.995460</td>\n",
       "      <td>0.992785</td>\n",
       "      <td>0.990203</td>\n",
       "      <td>0.991231</td>\n",
       "      <td>0.991525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               time          id  threshold  num_models  \\\n",
       "9               2018-03-02 00:54:48  1519970088     0.9794           6   \n",
       "73              2018-03-02 08:36:45  1519997805     0.9768          10   \n",
       "65              2018-03-02 07:35:41  1519994141     0.9774           9   \n",
       "\n",
       "    colsample_bytree        lr  max_depth  subsample  gamma  alpha    ...     \\\n",
       "9                0.9  0.096663          3       0.90      2      0    ...      \n",
       "73               0.7  0.031006          4       0.90      1      0    ...      \n",
       "65               0.8  0.068823          2       0.65      2      1    ...      \n",
       "\n",
       "    threat_best_round  insult_best_round  identity_hate_best_round  toxic_auc  \\\n",
       "9                 119                 74                        98   0.987579   \n",
       "73                273                163                       195   0.987640   \n",
       "65                100                 96                       104   0.987659   \n",
       "\n",
       "    severe_toxic_auc  obscene_auc  threat_auc  insult_auc  identity_hate_auc  \\\n",
       "9           0.991802     0.995448    0.994074    0.990130           0.991359   \n",
       "73          0.991029     0.995461    0.993971    0.990103           0.991239   \n",
       "65          0.991810     0.995460    0.992785    0.990203           0.991231   \n",
       "\n",
       "     avg_auc  \n",
       "9   0.991732  \n",
       "73  0.991574  \n",
       "65  0.991525  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search = pd.read_csv('~/data/kaggle/toxic/sc/stacking/xgb_search.csv').sort_values(by='avg_auc', ascending=False)\n",
    "\n",
    "xgb_search.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-12 12:28:44, id: 1520431835, th: 0.979300, num_models: 8, colsample_bytree: 0.800000, lr: 0.0800000,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 500, cv_nfolds: 4,    to_pw: 10, s_pw: 100, ob_pw: 17, th_pw: 333, in_pw: 20, ih_pw: 112, to_br: 231, s_br: 114,    ob_br: 103, th_br: 98, in_br: 101, ih_br: 116            \n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "    now = get_time()\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "    # per label based params \n",
    "    best_nrounds = {}\n",
    "    best_nrounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "    best_nrounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "    best_nrounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "    best_nrounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "    best_nrounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "    best_nrounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "#     scale_pos_weights['toxic'] = xgb_search['toxic_pos_scale'].values[i]\n",
    "#     scale_pos_weights['severe_toxic'] = xgb_search['severe_toxic_pos_scale'].values[i]\n",
    "#     scale_pos_weights['obscene'] = xgb_search['obscene_pos_scale'].values[i]\n",
    "#     scale_pos_weights['threat'] = xgb_search['threat_pos_scale'].values[i]\n",
    "#     scale_pos_weights['insult'] = xgb_search['insult_pos_scale'].values[i]\n",
    "#     scale_pos_weights['identity_hate'] = xgb_search['identity_hate_pos_scale'].values[i]\n",
    "    scale_pos_weights['toxic'] = 10 #1\n",
    "    scale_pos_weights['severe_toxic'] = 100 #1\n",
    "    scale_pos_weights['obscene'] = 17\n",
    "    scale_pos_weights['threat'] = 333\n",
    "    scale_pos_weights['insult'] = 20\n",
    "    scale_pos_weights['identity_hate'] = 112\n",
    "    \n",
    "    metric_dict_fromcsv = {}\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "    \n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d, to_br: %d, s_br: %d,\\\n",
    "    ob_br: %d, th_br: %d, in_br: %d, ih_br: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate'],\\\n",
    "              best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "              best_nrounds['obscene'],best_nrounds['threat'],\\\n",
    "              best_nrounds['insult'],best_nrounds['identity_hate']))\n",
    "\n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {}\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "        \n",
    "        # add engineered features to layer 2\n",
    "        x_train = np.hstack([F_train[features].as_matrix(), x_train])\n",
    "        x_test = np.hstack([F_test[features].as_matrix(), x_test])  \n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "        \n",
    "        xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds[label])\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1] # if using xgboost sklearn wrapper\n",
    "\n",
    "    sub_title = 'xgb_topn_w_addedfeatures_posweighted_'\n",
    "    submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "    submission[label_cols] = result\n",
    "    submission.to_csv('./StackPreds/TopN_XGB/{}_{}_{}.csv'.format(sub_title,metric_dict_fromcsv['avg_auc'],search_id), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_stacker_params = {\n",
    "    'max_depth':3, \n",
    "    'metric':\"auc\", \n",
    "    'n_estimators':125, \n",
    "    'num_leaves':10, \n",
    "    'boosting_type':\"gbdt\", \n",
    "    'learning_rate':0.1, \n",
    "    'feature_fraction':0.85,  #0.45 for only two added features\n",
    "    'colsample_bytree':0.45, \n",
    "    'bagging_fraction':0.8, \n",
    "    'bagging_freq':5, \n",
    "    'reg_lambda':0.2\n",
    "}\n",
    "\n",
    "lgb_stacker = LightgbmBLE(None, None, params=lgb_stacker_params, nb=False, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "XGBoostBase is initialized\n"
     ]
    }
   ],
   "source": [
    "xgb_stacker = XGBoostBLE(None, None, params={}, nb=False, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rf_stacker = SklearnBLE(RandomForestClassifier, params={}, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_stacker = SklearnBLE(ExtraTreesClassifier, params={}, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_stacker = SklearnBLE(LogisticRegression, params={}, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9796\tModelName.LOGREG_wordtfidf_word_(1, 1)_100000_1_1.0_char_(2, 5)_200000_1_1.0\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9765\tModelName.NBLGB_wordtfidf_word_(1, 1)_100000_1_1.0_char_(2, 5)_200000_1_1.0\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9745\tModelName.LOGREG_wordtfidf_word_(1, 1)_100000_1_1.0_char_(0, 0)_100000_1_1.0\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9662\tModelName.NBLGB_wordtfidf_word_(1, 1)_100000_1_1.0_char_(0, 0)_100000_1_1.0\n",
      "0.09854\tModelName.XGB_layer2\n",
      "0.09854\tModelName.LOGREG_layer2\n",
      "0.09853\tModelName.XGB_layer2_usingthebadhalf\n",
      "0.09852\tModelName.LOGREG_1sthalf16_layer2\n",
      "0.09851\tModelName.XGB_layer2_UsingAll14\n",
      "0.09851\tModelName.LOGREG_all16_layer2\n",
      "0.09851\tModelName.LOGREG_layer2_GoodHalfOfXGB\n",
      "0.09851\tModelName.LOGREG_2ndhalf16_layer2\n",
      "0.09839\tModelName.XGB_w_globalfeatures_1stHalf14_layer2\n",
      "0.09827\tModelName.LGB_layer2\n",
      "0.09825\tModelName.RNN_rnn_data_001\n",
      "0\tModelName.RNN_rnn_data_fasttext_200_300\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "selected = []\n",
    "for items in base_layer_results_repo.show_scores():\n",
    "    if items[1] >= 0.976:\n",
    "        selected.append(items[0])\n",
    "print(len(selected))\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(1001)\n",
    "shuffle(selected)\n",
    "#print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "layer2_inputs = {}\n",
    "\n",
    "n1 = '_w_features_1stHalf16_seed1001'\n",
    "n2 = '_w_features_2ndHalf16_seed1001'\n",
    "n3 = '_w_features_All16_seed1001'\n",
    "xgbmodel1 = str(ModelName.XGB)+n1\n",
    "xgbmodel2 = str(ModelName.XGB)+n2\n",
    "xgbmodel3 = str(ModelName.XGB)+n3\n",
    "model_pool[xgbmodel1] = xgb_stacker\n",
    "model_pool[xgbmodel2] = xgb_stacker\n",
    "model_pool[xgbmodel3] = xgb_stacker\n",
    "layer2_inputs[xgbmodel1] = base_layer_results_repo.get_results(chosen_ones=selected[:int(len(selected)/2)])\n",
    "layer2_inputs[xgbmodel2] = base_layer_results_repo.get_results(chosen_ones=selected[int(len(selected)/2):])\n",
    "layer2_inputs[xgbmodel3] = base_layer_results_repo.get_results(chosen_ones=selected)\n",
    "\n",
    "# logregmodel1 = str(ModelName.LOGREG)+n1\n",
    "# logregmodel2 = str(ModelName.LOGREG)+n2\n",
    "# logregmodel3 = str(ModelName.LOGREG)+n3\n",
    "# model_pool[logregmodel1] = logreg_stacker\n",
    "# model_pool[logregmodel2] = logreg_stacker\n",
    "# model_pool[logregmodel3] = logreg_stacker\n",
    "# layer2_inputs[logregmodel1] = base_layer_results_repo.get_results(chosen_ones=selected[:int(len(selected)/2)])\n",
    "# layer2_inputs[logregmodel2] = base_layer_results_repo.get_results(chosen_ones=selected[int(len(selected)/2):])\n",
    "# layer2_inputs[logregmodel3] = base_layer_results_repo.get_results(chosen_ones=selected)\n",
    "\n",
    "# lgbmodel1 = str(ModelName.LGB)+n1\n",
    "# lgbmodel2 = str(ModelName.LGB)+n2\n",
    "# lgbmodel3 = str(ModelName.LGB)+n3\n",
    "# model_pool[lgbmodel1] = lgb_stacker\n",
    "# model_pool[lgbmodel2] = lgb_stacker\n",
    "# model_pool[lgbmodel3] = lgb_stacker\n",
    "# layer2_inputs[lgbmodel1] = base_layer_results_repo.get_results(chosen_ones=selected[:int(len(selected)/2)])\n",
    "# layer2_inputs[lgbmodel2] = base_layer_results_repo.get_results(chosen_ones=selected[int(len(selected)/2):])\n",
    "# layer2_inputs[lgbmodel3] = base_layer_results_repo.get_results(chosen_ones=selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelName.XGB_w_features_1stHalf16_seed1001': <base_layer_utils.XGBoostBLE at 0x7f627e5ab0b8>,\n",
       " 'ModelName.XGB_w_features_2ndHalf16_seed1001': <base_layer_utils.XGBoostBLE at 0x7f627e5ab0b8>,\n",
       " 'ModelName.XGB_w_features_All16_seed1001': <base_layer_utils.XGBoostBLE at 0x7f627e5ab0b8>}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selected[int(len(selected)/2):] # the good half (xgb 9854) # 1stHalf14\n",
    "# selected = ['ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "#  'ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "#  'ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
    "#  'ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "#  'ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "#  'ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
    "#  'ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selected[:int(len(selected)/2)] # the bad half (xgb 9853)\n",
    "# ['ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "#  'ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w',\n",
    "#  'ModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
    "#  'ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "#  'ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
    "#  'ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "#  'ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_more_features = pd.read_csv('/home/kai/data/wei/Toxic/dataset/shiyi_0313_features_train.csv')\n",
    "\n",
    "global_train_features = train_more_features[['cleaned_word_count', 'word_count', 'unique_word_count', \n",
    "                    'cleaned_unique_word_count', 'ellipsis', 'exclamation_marks',\n",
    "                    'question_marks', 'polarity_cleaned', 'polarity_ori']].to_dense()\n",
    "\n",
    "global_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_more_features = pd.read_csv('/home/kai/data/wei/Toxic/dataset/shiyi_0313_features_test.csv')\n",
    "\n",
    "global_test_features = test_more_features[['cleaned_word_count', 'word_count', 'unique_word_count', \n",
    "                    'cleaned_unique_word_count', 'ellipsis', 'exclamation_marks',\n",
    "                    'question_marks', 'polarity_cleaned', 'polarity_ori']].to_dense()\n",
    "\n",
    "global_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4056"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Layer2 model ModelName.XGB_w_features_1stHalf16_seed1001 OOF\n",
      "XGB model chosen, setting params for toxic\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for severe_toxic\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for obscene\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for threat\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for insult\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for identity_hate\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "Generating Layer2 model ModelName.XGB_w_features_All16_seed1001 OOF\n",
      "XGB model chosen, setting params for toxic\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for severe_toxic\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for obscene\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for threat\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for insult\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for identity_hate\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "Generating Layer2 model ModelName.XGB_w_features_2ndHalf16_seed1001 OOF\n",
      "XGB model chosen, setting params for toxic\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for severe_toxic\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for obscene\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for threat\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for insult\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "XGB model chosen, setting params for identity_hate\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "# import lightgbm as lgb\n",
    "# stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, \n",
    "#                              boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, \n",
    "#                              colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "\n",
    "layer2_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer2_oof_train = {}\n",
    "layer2_oof_test = {}\n",
    "\n",
    "layer2_model_list = []\n",
    "\n",
    "# result = np.empty((test.shape[0],len(label_cols)))\n",
    "for model_name in model_pool.keys():\n",
    "    print('Generating Layer2 model {} OOF'.format(model_name))\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "\n",
    "        model = model_pool[model_name]\n",
    "        if str(ModelName.XGB) in model_name: # Done some grid search on xgb, so it has per label based best params\n",
    "            print('XGB model chosen, setting params for {}'.format(label))\n",
    "            model.set_params(get_best_xgb_params(label=label))\n",
    "\n",
    "        layer1_oof_train_loaded, layer1_oof_test_loaded, _ = layer2_inputs[model_name]\n",
    "        \n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "        # add engineered features to layer 2\n",
    "#         x_train = np.hstack([F_train[features].as_matrix(), global_train_features, x_train])\n",
    "#         x_test = np.hstack([F_test[features].as_matrix(), global_test_features, x_test])  \n",
    "        x_train = np.hstack([F_train[features].as_matrix(), x_train])\n",
    "        x_test = np.hstack([F_test[features].as_matrix(), x_test])  \n",
    "\n",
    "        SEED = 1001\n",
    "        NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "\n",
    "        oof_train, oof_test = get_oof(model,  x_train, train[label], x_test, NFOLDS, SEED)\n",
    "\n",
    "        if label not in layer2_oof_train:\n",
    "            layer2_oof_train[label] = []\n",
    "            layer2_oof_test[label] = []\n",
    "        layer2_oof_train[label].append(oof_train)\n",
    "        layer2_oof_test[label].append(oof_test)\n",
    "\n",
    "    #     stacker.fit(x_train, train[label])\n",
    "    #     result[:,i] = stacker.predict_proba(x_test)[:,1]\n",
    "        model_id = '{}_{}'.format(model_name, 'layer2')\n",
    "        model.train(x_train, train[label])\n",
    "        est_preds = model.predict(x_test)\n",
    "\n",
    "        if model_id not in layer2_est_preds:\n",
    "            layer2_est_preds[model_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "            layer2_model_list.append(model_id)\n",
    "        layer2_est_preds[model_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00873907, -0.02068911,  0.03144957, -0.09154736,  1.03971318,\n",
       "         0.23963355,  0.56386026, -1.4400043 ,  1.48109455,  5.55383302,\n",
       "         1.05722585]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clf.coef_ # w features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer2_oof_train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-796de001991a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayer2_oof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "layer2_oof_train['toxic'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.XGB_w_features_2ndHalf16_seed1001_layer2',\n",
       " 'ModelName.XGB_w_features_1stHalf16_seed1001_layer2',\n",
       " 'ModelName.XGB_w_features_All16_seed1001_layer2']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer2_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_est_preds[list(layer2_est_preds.keys())[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo.add(layer2_oof_train, layer2_oof_test, layer2_est_preds, layer2_model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9796\tModelName.LOGREG_wordtfidf_word_(1, 1)_100000_1_1.0_char_(2, 5)_200000_1_1.0\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9765\tModelName.NBLGB_wordtfidf_word_(1, 1)_100000_1_1.0_char_(2, 5)_200000_1_1.0\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9745\tModelName.LOGREG_wordtfidf_word_(1, 1)_100000_1_1.0_char_(0, 0)_100000_1_1.0\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9662\tModelName.NBLGB_wordtfidf_word_(1, 1)_100000_1_1.0_char_(0, 0)_100000_1_1.0\n",
      "0.09854\tModelName.XGB_layer2\n",
      "0.09854\tModelName.LOGREG_layer2\n",
      "0.09853\tModelName.XGB_layer2_usingthebadhalf\n",
      "0.09852\tModelName.LOGREG_1sthalf16_layer2\n",
      "0.09851\tModelName.XGB_layer2_UsingAll14\n",
      "0.09851\tModelName.LOGREG_all16_layer2\n",
      "0.09851\tModelName.LOGREG_layer2_GoodHalfOfXGB\n",
      "0.09851\tModelName.LOGREG_2ndhalf16_layer2\n",
      "0.09839\tModelName.XGB_w_globalfeatures_1stHalf14_layer2\n",
      "0.09827\tModelName.LGB_layer2\n",
      "0.09825\tModelName.RNN_rnn_data_001\n",
      "0.009839\tModelName.LGB_w_features_All16_seed1001_layer2\n",
      "0.009835\tModelName.LGB_w_features_1stHalf16_seed1001_layer2\n",
      "0.009833\tModelName.LOGREG_w_features_All16_seed1001_layer2\n",
      "0.009827\tModelName.LOGREG_w_features_2ndHalf16_seed1001_layer2\n",
      "0.009821\tModelName.LGB_w_features_2ndHalf16_seed1001_layer2\n",
      "0.009818\tModelName.LOGREG_w_features_1stHalf16_seed1001_layer2\n",
      "0\tModelName.XGB_w_features_1stHalf16_seed1001_layer2\n",
      "0\tModelName.RNN_rnn_data_fasttext_200_300\n",
      "0\tModelName.XGB_w_features_2ndHalf16_seed1001_layer2\n",
      "0\tModelName.XGB_w_features_All16_seed1001_layer2\n"
     ]
    }
   ],
   "source": [
    "_ = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.LOGREG_w_features_All16_seed1001_layer2 already existed in the repo. score: 0 update to 0.009833\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo.add_score('ModelName.LOGREG_w_features_All16_seed1001_layer2', 0.009833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sub_title = 'xgb_topn_w_addedfeatures_'\n",
    "submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "submission[label_cols] = layer2_est_preds['ModelName.XGB_layer2']\n",
    "tempid = int(time.time())\n",
    "print(tempid)\n",
    "submission.to_csv('./StackPreds/TopN_XGB/{}_{}.csv'.format(sub_title, tempid), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_base_layer_est_preds(base_layer_est_preds):\n",
    "    for key in base_layer_est_preds:\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "        submission[label_cols] = base_layer_est_preds[key]\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./BaseEstPreds/' + key + '_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521040515\n",
      "1521040516\n",
      "1521040518\n"
     ]
    }
   ],
   "source": [
    "generate_base_layer_est_preds(layer2_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_xgb_params(xgb_grid_search_res_path='~/data/kaggle/toxic/sc/stacking/xgb_search.csv', idx = 0, label='toxic'):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        xgb_grid_search_res_path: (str).\n",
    "        idx: load the nth best params. e.g. idx=0, load the best. idx=1, load the 2nd best\n",
    "    Returns:\n",
    "        (dict) the best xgb grid search params.\n",
    "    \"\"\"       \n",
    "    xgb_search = pd.read_csv(xgb_grid_search_res_path).sort_values(by='avg_auc', ascending=False)\n",
    "    \n",
    "    print('xgb grid search file loaded. shape:{}'.format(xgb_search.shape))\n",
    "    \n",
    "    i=idx\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "    # per label based params \n",
    "    best_nrounds = {}\n",
    "    best_nrounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "    best_nrounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "    best_nrounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "    best_nrounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "    best_nrounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "    best_nrounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = xgb_search['toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['severe_toxic'] = xgb_search['severe_toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['obscene'] = xgb_search['obscene_pos_scale'].values[i]\n",
    "    scale_pos_weights['threat'] = xgb_search['threat_pos_scale'].values[i]\n",
    "    scale_pos_weights['insult'] = xgb_search['insult_pos_scale'].values[i]\n",
    "    scale_pos_weights['identity_hate'] = xgb_search['identity_hate_pos_scale'].values[i]\n",
    "\n",
    "    metric_dict_fromcsv = {}\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "    \n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "    xgb_params['num_boost_round'] = best_nrounds[label]\n",
    "    \n",
    "    return xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb grid search file loaded. shape:(81, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0,\n",
       " 'colsample_bytree': 0.9,\n",
       " 'eval_metric': 'auc',\n",
       " 'gamma': 2,\n",
       " 'learning_rate': 0.0966631,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'nthread': 7,\n",
       " 'num_boost_round': 119,\n",
       " 'objective': 'binary:logistic',\n",
       " 'scale_pos_weight': 120,\n",
       " 'seed': 0,\n",
       " 'silent': 1,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_xgb_params(label='threat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance({}, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def get_oof(clf, x_train, y_train, x_test, nfolds, stratified=False, shuffle=True, seed=1001):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits=nfolds, shuffle=shuffle, random_state=seed)\n",
    "    else:\n",
    "        kf = KFold(n_splits=nfolds, shuffle=shuffle, random_state=seed)\n",
    "\n",
    "    for i, (tr_index, te_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_tr, x_te = x_train[tr_index], x_train[te_index]\n",
    "        y_tr, y_te = y_train.iloc[tr_index], y_train.iloc[te_index]\n",
    "        \n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[te_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
