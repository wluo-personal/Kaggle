{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory. \n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'adfasfad'#'/home/kai/data/resources/glove/glove.840B.300d.txt' #glove.twitter.27B.200d.txt\n",
    "PATH = '~/data/toxic/data/'\n",
    "# train = pd.read_csv(PATH + 'train.csv')\n",
    "# test = pd.read_csv(PATH + 'test.csv')\n",
    "# train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "# test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "train = pd.read_csv(PATH + 'train_preprocessed.csv')\n",
    "test = pd.read_csv(PATH + 'test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>set</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text                id  \\\n",
       "0  explanation why the edits made under my userna...  0000997932d777bf   \n",
       "\n",
       "   identity_hate  insult  obscene    set  severe_toxic  threat  toxic  \\\n",
       "0            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "\n",
       "   toxicity  \n",
       "0       0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2Vec(source):\n",
    "    embed_size = 300\n",
    "    if source.lower() == 'ft-common':\n",
    "        file = '/home/kai/data/resources/FastText/crawl-300d-2M.vec'\n",
    "    elif source.lower() == 'ft-wiki':\n",
    "        file = '/home/kai/data/resources/FastText/wiki.en.vec'\n",
    "    elif source.lower() == 'lex':\n",
    "        file = '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors'\n",
    "    elif source.lower() == 'gl-common':\n",
    "        file = '/home/kai/data/resources/glove/glove.840B.300d.txt'\n",
    "    elif source.lower() == 'gl-twitter':\n",
    "        file = '/home/kai/data/resources/glove/glove.twitter.27B.200d.txt'\n",
    "        embed_size = 200\n",
    "    def get_coefs(word,*arr): \n",
    "        try:\n",
    "            return word, np.asarray(arr, dtype='float32') \n",
    "        except ValueError:\n",
    "            return 'nnnnnnnaaaaaaa@@!',np.zeros(embed_size)\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(file, encoding='utf8'))\n",
    "    return embeddings_index, embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index_lex, embed_size = word2Vec('lex')\n",
    "embeddings_index_glc, embed_size = word2Vec('gl-common')\n",
    "embeddings_index_glt, embed_size = word2Vec('gl-twitter')\n",
    "embeddings_index_ftc, embed_size = word2Vec('ft-common')\n",
    "embeddings_index_ftw, embed_size = word2Vec('ft-wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_col = 'comment_text' # 'comment_text_cleaned' \n",
    "\n",
    "X_train = train[comment_col].str.lower().fillna('something') # something is a word of neutral sentiment\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[comment_col].str.lower().fillna('something')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "\n",
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_word_count = sorted(tok.word_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorted_word_count#[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48862\n",
      "0.8143666666666667\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for w,w_cnt in sorted_word_count[:60000]:\n",
    "#     if w in embeddings_index_glt:\n",
    "#         count+=1\n",
    "# print(count)\n",
    "# print(count/60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(embeddings_index_glt['hesitates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9787\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = embeddings_index_lex\n",
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "not_found_word = {}\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    try: \n",
    "        embedding_vector = embeddings_index[word] # w2v_model['/en/'+ word] #w2v_model[word]#\n",
    "    except KeyError:\n",
    "        embedding_vector = embeddings_index.get(bad_word_dict.get(word), None)\n",
    "#         embedding_vector = None #np.zeros(embed_size)\n",
    "        if embedding_vector is None:\n",
    "            not_found_word[word] = tok.word_counts[word]#i\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(len(not_found_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index_pool = [\n",
    "    (embeddings_index_lex, 300, 'lex'),\n",
    "    (embeddings_index_glc, 300, 'glc'),\n",
    "    (embeddings_index_glt, 200, 'glt'),\n",
    "    (embeddings_index_ftc, 300, 'ftc'), \n",
    "    (embeddings_index_ftw, 300, 'ftw')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_epoch_patience_pool = [\n",
    "    (1024, 10, 10),\n",
    "    (1024, 3, 3),\n",
    "    (1024, 3, 3),\n",
    "    (1024, 3, 3),\n",
    "    (512, 8, 8),\n",
    "    (512, 2, 2),\n",
    "    (512, 2, 2),\n",
    "    (512, 2, 2),\n",
    "    (128, 5, 5),\n",
    "    (128, 2, 2),\n",
    "    (128, 2, 2),\n",
    "    (128, 1, 1),\n",
    "    (32, 3, 3),\n",
    "    (32, 2, 2),\n",
    "    (32, 1, 1),\n",
    "    (32, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9787\n",
      "1024 10 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_w_conv_lex_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9658\n",
      " ROC-AUC - epoch: 1 - score: 0.956641\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98068, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 78s 541us/step - loss: 0.1126 - acc: 0.9658 - val_loss: 0.0533 - val_acc: 0.9807\n",
      "Epoch 2/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9811\n",
      " ROC-AUC - epoch: 2 - score: 0.979269\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98068 to 0.98231, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 495us/step - loss: 0.0518 - acc: 0.9811 - val_loss: 0.0469 - val_acc: 0.9823\n",
      "Epoch 3/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9821\n",
      " ROC-AUC - epoch: 3 - score: 0.982808\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98231 to 0.98243, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 496us/step - loss: 0.0476 - acc: 0.9821 - val_loss: 0.0468 - val_acc: 0.9824\n",
      "Epoch 4/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9828\n",
      " ROC-AUC - epoch: 4 - score: 0.984982\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98243 to 0.98326, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 72s 501us/step - loss: 0.0452 - acc: 0.9828 - val_loss: 0.0434 - val_acc: 0.9833\n",
      "Epoch 5/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9832\n",
      " ROC-AUC - epoch: 5 - score: 0.986756\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98326 to 0.98328, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 73s 508us/step - loss: 0.0437 - acc: 0.9832 - val_loss: 0.0430 - val_acc: 0.9833\n",
      "Epoch 6/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9837\n",
      " ROC-AUC - epoch: 6 - score: 0.988157\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98328 to 0.98371, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 72s 502us/step - loss: 0.0423 - acc: 0.9837 - val_loss: 0.0416 - val_acc: 0.9837\n",
      "Epoch 7/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9840\n",
      " ROC-AUC - epoch: 7 - score: 0.988372\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98371 to 0.98372, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 72s 498us/step - loss: 0.0412 - acc: 0.9840 - val_loss: 0.0414 - val_acc: 0.9837\n",
      "Epoch 8/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9843\n",
      " ROC-AUC - epoch: 8 - score: 0.988748\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "143613/143613 [==============================] - 72s 500us/step - loss: 0.0404 - acc: 0.9843 - val_loss: 0.0419 - val_acc: 0.9836\n",
      "Epoch 9/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9846\n",
      " ROC-AUC - epoch: 9 - score: 0.989179\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.98372 to 0.98409, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 72s 500us/step - loss: 0.0395 - acc: 0.9846 - val_loss: 0.0406 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9849\n",
      " ROC-AUC - epoch: 10 - score: 0.989441\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "143613/143613 [==============================] - 72s 498us/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0414 - val_acc: 0.9836\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 79us/step\n",
      "1521299173\n",
      "1024 3 3\n",
      "rnn_w_conv_lex_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9846\n",
      " ROC-AUC - epoch: 1 - score: 0.989464\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98531, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 78s 545us/step - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0378 - val_acc: 0.9853\n",
      "Epoch 2/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9848\n",
      " ROC-AUC - epoch: 2 - score: 0.990041\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 497us/step - loss: 0.0388 - acc: 0.9848 - val_loss: 0.0375 - val_acc: 0.9849\n",
      "Epoch 3/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852\n",
      " ROC-AUC - epoch: 3 - score: 0.989878\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 73s 506us/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0388 - val_acc: 0.9842\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521299415\n",
      "1024 3 3\n",
      "rnn_w_conv_lex_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9848\n",
      " ROC-AUC - epoch: 1 - score: 0.991123\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98531, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 81s 561us/step - loss: 0.0388 - acc: 0.9848 - val_loss: 0.0371 - val_acc: 0.9853\n",
      "Epoch 2/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9851\n",
      " ROC-AUC - epoch: 2 - score: 0.991003\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 498us/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0375 - val_acc: 0.9850\n",
      "Epoch 3/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9853\n",
      " ROC-AUC - epoch: 3 - score: 0.990746\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98531 to 0.98568, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 72s 501us/step - loss: 0.0372 - acc: 0.9853 - val_loss: 0.0370 - val_acc: 0.9857\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521299660\n",
      "1024 3 3\n",
      "rnn_w_conv_lex_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9855\n",
      " ROC-AUC - epoch: 1 - score: 0.992078\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98587, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 80s 557us/step - loss: 0.0370 - acc: 0.9855 - val_loss: 0.0358 - val_acc: 0.9859\n",
      "Epoch 2/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9857\n",
      " ROC-AUC - epoch: 2 - score: 0.992071\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98587 to 0.98618, saving model to ./NewRnnModels/rnn_w_conv_lex_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 496us/step - loss: 0.0364 - acc: 0.9857 - val_loss: 0.0351 - val_acc: 0.9862\n",
      "Epoch 3/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9859\n",
      " ROC-AUC - epoch: 3 - score: 0.992002\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 493us/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0358 - val_acc: 0.9859\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 70us/step\n",
      "1521299905\n",
      "512 8 8\n",
      "rnn_w_conv_lex_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0844 - acc: 0.9729\n",
      " ROC-AUC - epoch: 1 - score: 0.970724\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98213, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143613/143613 [==============================] - 119s 827us/step - loss: 0.0844 - acc: 0.9730 - val_loss: 0.0484 - val_acc: 0.9821\n",
      "Epoch 2/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9818\n",
      " ROC-AUC - epoch: 2 - score: 0.979491\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98213 to 0.98296, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 111s 771us/step - loss: 0.0484 - acc: 0.9818 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "Epoch 3/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9825\n",
      " ROC-AUC - epoch: 3 - score: 0.983550\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98296 to 0.98351, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 109s 759us/step - loss: 0.0453 - acc: 0.9825 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "Epoch 4/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9834\n",
      " ROC-AUC - epoch: 4 - score: 0.985439\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98351 to 0.98379, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 109s 761us/step - loss: 0.0430 - acc: 0.9834 - val_loss: 0.0415 - val_acc: 0.9838\n",
      "Epoch 5/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9839\n",
      " ROC-AUC - epoch: 5 - score: 0.987420\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98379 to 0.98403, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 109s 758us/step - loss: 0.0412 - acc: 0.9839 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9844\n",
      " ROC-AUC - epoch: 6 - score: 0.987781\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98403 to 0.98432, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 111s 770us/step - loss: 0.0400 - acc: 0.9844 - val_loss: 0.0406 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9846\n",
      " ROC-AUC - epoch: 7 - score: 0.987795\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "143613/143613 [==============================] - 109s 759us/step - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0422 - val_acc: 0.9837\n",
      "Epoch 8/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9849\n",
      " ROC-AUC - epoch: 8 - score: 0.987971\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "143613/143613 [==============================] - 109s 759us/step - loss: 0.0384 - acc: 0.9849 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 70us/step\n",
      "1521300812\n",
      "512 2 2\n",
      "rnn_w_conv_lex_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9842\n",
      " ROC-AUC - epoch: 1 - score: 0.989999\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98592, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 119s 832us/step - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0365 - val_acc: 0.9859\n",
      "Epoch 2/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9849\n",
      " ROC-AUC - epoch: 2 - score: 0.989074\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 111s 775us/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0366 - val_acc: 0.9857\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 70us/step\n",
      "1521301067\n",
      "512 2 2\n",
      "rnn_w_conv_lex_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849\n",
      " ROC-AUC - epoch: 1 - score: 0.992387\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98498, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 120s 838us/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0372 - val_acc: 0.9850\n",
      "Epoch 2/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9851\n",
      " ROC-AUC - epoch: 2 - score: 0.992035\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98498 to 0.98511, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 109s 762us/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0377 - val_acc: 0.9851\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 79us/step\n",
      "1521301320\n",
      "512 2 2\n",
      "rnn_w_conv_lex_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9854\n",
      " ROC-AUC - epoch: 1 - score: 0.992615\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98660, saving model to ./NewRnnModels/rnn_w_conv_lex_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 121s 845us/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0340 - val_acc: 0.9866\n",
      "Epoch 2/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9857\n",
      " ROC-AUC - epoch: 2 - score: 0.992361\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 109s 757us/step - loss: 0.0360 - acc: 0.9857 - val_loss: 0.0346 - val_acc: 0.9864\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 69us/step\n",
      "1521301573\n",
      "128 5 5\n",
      "rnn_w_conv_lex_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9789\n",
      " ROC-AUC - epoch: 1 - score: 0.982063\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98286, saving model to ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 356s 2ms/step - loss: 0.0598 - acc: 0.9789 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9830\n",
      " ROC-AUC - epoch: 2 - score: 0.986113\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 343s 2ms/step - loss: 0.0441 - acc: 0.9830 - val_loss: 0.0465 - val_acc: 0.9824\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838\n",
      " ROC-AUC - epoch: 3 - score: 0.987588\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98286 to 0.98382, saving model to ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 344s 2ms/step - loss: 0.0416 - acc: 0.9838 - val_loss: 0.0423 - val_acc: 0.9838\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9844\n",
      " ROC-AUC - epoch: 4 - score: 0.987058\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 345s 2ms/step - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9848\n",
      " ROC-AUC - epoch: 5 - score: 0.987261\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "143613/143613 [==============================] - 344s 2ms/step - loss: 0.0383 - acc: 0.9848 - val_loss: 0.0429 - val_acc: 0.9837\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521303326\n",
      "128 2 2\n",
      "rnn_w_conv_lex_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9842\n",
      " ROC-AUC - epoch: 1 - score: 0.989910\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98497, saving model to ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 353s 2ms/step - loss: 0.0404 - acc: 0.9842 - val_loss: 0.0387 - val_acc: 0.9850\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9848\n",
      " ROC-AUC - epoch: 2 - score: 0.990203\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98497 to 0.98516, saving model to ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 343s 2ms/step - loss: 0.0388 - acc: 0.9848 - val_loss: 0.0383 - val_acc: 0.9852\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 78us/step\n",
      "1521304049\n",
      "128 2 2\n",
      "rnn_w_conv_lex_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9853\n",
      " ROC-AUC - epoch: 1 - score: 0.991808\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98628, saving model to ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 355s 2ms/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0358 - val_acc: 0.9863\n",
      "Epoch 2/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9858\n",
      " ROC-AUC - epoch: 2 - score: 0.991894\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 343s 2ms/step - loss: 0.0360 - acc: 0.9858 - val_loss: 0.0370 - val_acc: 0.9855\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521304772\n",
      "128 1 1\n",
      "rnn_w_conv_lex_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9858\n",
      " ROC-AUC - epoch: 1 - score: 0.992200\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98519, saving model to ./NewRnnModels/rnn_w_conv_lex_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 355s 2ms/step - loss: 0.0357 - acc: 0.9858 - val_loss: 0.0368 - val_acc: 0.9852\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 79us/step\n",
      "1521305155\n",
      "32 3 3\n",
      "rnn_w_conv_lex_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9808\n",
      " ROC-AUC - epoch: 1 - score: 0.987059\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98390, saving model to ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1280s 9ms/step - loss: 0.0527 - acc: 0.9808 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 2/3\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9832\n",
      " ROC-AUC - epoch: 2 - score: 0.988917\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 1276s 9ms/step - loss: 0.0434 - acc: 0.9832 - val_loss: 0.0418 - val_acc: 0.9834\n",
      "Epoch 3/3\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9841\n",
      " ROC-AUC - epoch: 3 - score: 0.988296\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98390 to 0.98409, saving model to ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1278s 9ms/step - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0417 - val_acc: 0.9841\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 78us/step\n",
      "1521309012\n",
      "32 2 2\n",
      "rnn_w_conv_lex_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9849\n",
      " ROC-AUC - epoch: 1 - score: 0.990882\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98354, saving model to ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1279s 9ms/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0411 - val_acc: 0.9835\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855\n",
      " ROC-AUC - epoch: 2 - score: 0.990928\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98354 to 0.98482, saving model to ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1267s 9ms/step - loss: 0.0367 - acc: 0.9855 - val_loss: 0.0395 - val_acc: 0.9848\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 70us/step\n",
      "1521311584\n",
      "32 1 1\n",
      "rnn_w_conv_lex_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9860\n",
      " ROC-AUC - epoch: 1 - score: 0.993025\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98704, saving model to ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1275s 9ms/step - loss: 0.0358 - acc: 0.9860 - val_loss: 0.0324 - val_acc: 0.9870\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 70us/step\n",
      "1521312886\n",
      "32 1 1\n",
      "rnn_w_conv_lex_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9865\n",
      " ROC-AUC - epoch: 1 - score: 0.994796\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98762, saving model to ./NewRnnModels/rnn_w_conv_lex_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1283s 9ms/step - loss: 0.0341 - acc: 0.9865 - val_loss: 0.0312 - val_acc: 0.9876\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 72us/step\n",
      "1521314195\n",
      "21631\n",
      "1024 10 10\n",
      "rnn_w_conv_glc_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9706\n",
      " ROC-AUC - epoch: 1 - score: 0.976741\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98260, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 85s 591us/step - loss: 0.0915 - acc: 0.9707 - val_loss: 0.0464 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9823\n",
      " ROC-AUC - epoch: 2 - score: 0.982665\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 70s 489us/step - loss: 0.0470 - acc: 0.9823 - val_loss: 0.0455 - val_acc: 0.9825\n",
      "Epoch 3/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9830\n",
      " ROC-AUC - epoch: 3 - score: 0.985690\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98260 to 0.98393, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 495us/step - loss: 0.0441 - acc: 0.9830 - val_loss: 0.0420 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9837\n",
      " ROC-AUC - epoch: 4 - score: 0.987032\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98393 to 0.98427, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 495us/step - loss: 0.0417 - acc: 0.9837 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 5/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9841\n",
      " ROC-AUC - epoch: 5 - score: 0.987652\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98427 to 0.98454, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 494us/step - loss: 0.0404 - acc: 0.9841 - val_loss: 0.0397 - val_acc: 0.9845\n",
      "Epoch 6/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9846\n",
      " ROC-AUC - epoch: 6 - score: 0.988033\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98454 to 0.98476, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 497us/step - loss: 0.0389 - acc: 0.9846 - val_loss: 0.0392 - val_acc: 0.9848\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849\n",
      " ROC-AUC - epoch: 7 - score: 0.988709\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98476 to 0.98477, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 493us/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0385 - val_acc: 0.9848\n",
      "Epoch 8/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9853\n",
      " ROC-AUC - epoch: 8 - score: 0.988945\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 495us/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0386 - val_acc: 0.9847\n",
      "Epoch 9/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9856\n",
      " ROC-AUC - epoch: 9 - score: 0.989029\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "143613/143613 [==============================] - 72s 504us/step - loss: 0.0361 - acc: 0.9856 - val_loss: 0.0431 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9859\n",
      " ROC-AUC - epoch: 10 - score: 0.988802\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 498us/step - loss: 0.0353 - acc: 0.9859 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521314944\n",
      "1024 3 3\n",
      "rnn_w_conv_glc_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9851\n",
      " ROC-AUC - epoch: 1 - score: 0.991766\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98611, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 84s 585us/step - loss: 0.0378 - acc: 0.9850 - val_loss: 0.0351 - val_acc: 0.9861\n",
      "Epoch 2/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853\n",
      " ROC-AUC - epoch: 2 - score: 0.991546\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 72s 499us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0355 - val_acc: 0.9860\n",
      "Epoch 3/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9857\n",
      " ROC-AUC - epoch: 3 - score: 0.991573\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98611 to 0.98612, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 71s 498us/step - loss: 0.0357 - acc: 0.9857 - val_loss: 0.0354 - val_acc: 0.9861\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 79us/step\n",
      "1521315200\n",
      "1024 3 3\n",
      "rnn_w_conv_glc_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9860\n",
      " ROC-AUC - epoch: 1 - score: 0.994240\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98690, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 85s 593us/step - loss: 0.0353 - acc: 0.9860 - val_loss: 0.0326 - val_acc: 0.9869\n",
      "Epoch 2/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9864\n",
      " ROC-AUC - epoch: 2 - score: 0.994003\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 494us/step - loss: 0.0340 - acc: 0.9864 - val_loss: 0.0331 - val_acc: 0.9867\n",
      "Epoch 3/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867\n",
      " ROC-AUC - epoch: 3 - score: 0.993907\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 497us/step - loss: 0.0331 - acc: 0.9867 - val_loss: 0.0331 - val_acc: 0.9863\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521315453\n",
      "1024 3 3\n",
      "rnn_w_conv_glc_1024_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9862\n",
      " ROC-AUC - epoch: 1 - score: 0.994158\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98822, saving model to ./NewRnnModels/rnn_w_conv_glc_1024_w_dict.hdf5\n",
      "143613/143613 [==============================] - 85s 592us/step - loss: 0.0345 - acc: 0.9862 - val_loss: 0.0304 - val_acc: 0.9882\n",
      "Epoch 2/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9866\n",
      " ROC-AUC - epoch: 2 - score: 0.994085\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 71s 496us/step - loss: 0.0333 - acc: 0.9866 - val_loss: 0.0344 - val_acc: 0.9860\n",
      "Epoch 3/3\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9872\n",
      " ROC-AUC - epoch: 3 - score: 0.993952\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 73s 507us/step - loss: 0.0323 - acc: 0.9872 - val_loss: 0.0311 - val_acc: 0.9878\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 72us/step\n",
      "1521315710\n",
      "512 8 8\n",
      "rnn_w_conv_glc_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9752\n",
      " ROC-AUC - epoch: 1 - score: 0.981647\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98243, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 124s 865us/step - loss: 0.0745 - acc: 0.9752 - val_loss: 0.0456 - val_acc: 0.9824\n",
      "Epoch 2/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9827\n",
      " ROC-AUC - epoch: 2 - score: 0.987644\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98243 to 0.98362, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 110s 765us/step - loss: 0.0450 - acc: 0.9827 - val_loss: 0.0421 - val_acc: 0.9836\n",
      "Epoch 3/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9836\n",
      " ROC-AUC - epoch: 3 - score: 0.989835\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98362 to 0.98428, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 111s 775us/step - loss: 0.0420 - acc: 0.9836 - val_loss: 0.0401 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9843\n",
      " ROC-AUC - epoch: 4 - score: 0.990382\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 110s 765us/step - loss: 0.0401 - acc: 0.9843 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 5/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9847\n",
      " ROC-AUC - epoch: 5 - score: 0.990738\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "143613/143613 [==============================] - 109s 761us/step - loss: 0.0385 - acc: 0.9847 - val_loss: 0.0406 - val_acc: 0.9836\n",
      "Epoch 6/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9853\n",
      " ROC-AUC - epoch: 6 - score: 0.990720\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "143613/143613 [==============================] - 110s 763us/step - loss: 0.0370 - acc: 0.9853 - val_loss: 0.0401 - val_acc: 0.9840\n",
      "Epoch 7/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9856\n",
      " ROC-AUC - epoch: 7 - score: 0.990917\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98428 to 0.98439, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 111s 770us/step - loss: 0.0361 - acc: 0.9856 - val_loss: 0.0397 - val_acc: 0.9844\n",
      "Epoch 8/8\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9861\n",
      " ROC-AUC - epoch: 8 - score: 0.990648\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "143613/143613 [==============================] - 110s 765us/step - loss: 0.0349 - acc: 0.9861 - val_loss: 0.0409 - val_acc: 0.9837\n",
      "Predicting using the best model/epoch so far....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 11s 73us/step\n",
      "1521316628\n",
      "512 2 2\n",
      "rnn_w_conv_glc_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9858\n",
      " ROC-AUC - epoch: 1 - score: 0.993488\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98654, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 124s 862us/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0328 - val_acc: 0.9865\n",
      "Epoch 2/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9864\n",
      " ROC-AUC - epoch: 2 - score: 0.993185\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 111s 774us/step - loss: 0.0341 - acc: 0.9864 - val_loss: 0.0336 - val_acc: 0.9863\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 72us/step\n",
      "1521316892\n",
      "512 2 2\n",
      "rnn_w_conv_glc_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9862\n",
      " ROC-AUC - epoch: 1 - score: 0.994252\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98771, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 124s 862us/step - loss: 0.0345 - acc: 0.9862 - val_loss: 0.0310 - val_acc: 0.9877\n",
      "Epoch 2/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9869\n",
      " ROC-AUC - epoch: 2 - score: 0.994071\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 112s 781us/step - loss: 0.0331 - acc: 0.9868 - val_loss: 0.0323 - val_acc: 0.9871\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521317157\n",
      "512 2 2\n",
      "rnn_w_conv_glc_512_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867\n",
      " ROC-AUC - epoch: 1 - score: 0.994686\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98725, saving model to ./NewRnnModels/rnn_w_conv_glc_512_w_dict.hdf5\n",
      "143613/143613 [==============================] - 123s 858us/step - loss: 0.0332 - acc: 0.9868 - val_loss: 0.0318 - val_acc: 0.9872\n",
      "Epoch 2/2\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9873\n",
      " ROC-AUC - epoch: 2 - score: 0.994426\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 109s 760us/step - loss: 0.0317 - acc: 0.9873 - val_loss: 0.0313 - val_acc: 0.9872\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 80us/step\n",
      "1521317419\n",
      "128 5 5\n",
      "rnn_w_conv_glc_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9806\n",
      " ROC-AUC - epoch: 1 - score: 0.985271\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98268, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 361s 3ms/step - loss: 0.0548 - acc: 0.9806 - val_loss: 0.0447 - val_acc: 0.9827\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9834\n",
      " ROC-AUC - epoch: 2 - score: 0.990141\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98268 to 0.98461, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 344s 2ms/step - loss: 0.0429 - acc: 0.9834 - val_loss: 0.0390 - val_acc: 0.9846\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9843\n",
      " ROC-AUC - epoch: 3 - score: 0.990490\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98461 to 0.98481, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 344s 2ms/step - loss: 0.0400 - acc: 0.9843 - val_loss: 0.0388 - val_acc: 0.9848\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850\n",
      " ROC-AUC - epoch: 4 - score: 0.990640\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98481 to 0.98520, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 347s 2ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0378 - val_acc: 0.9852\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9856\n",
      " ROC-AUC - epoch: 5 - score: 0.990944\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98520 to 0.98551, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 345s 2ms/step - loss: 0.0361 - acc: 0.9856 - val_loss: 0.0379 - val_acc: 0.9855\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 72us/step\n",
      "1521319185\n",
      "128 2 2\n",
      "rnn_w_conv_glc_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9863\n",
      " ROC-AUC - epoch: 1 - score: 0.993443\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98654, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 356s 2ms/step - loss: 0.0347 - acc: 0.9863 - val_loss: 0.0338 - val_acc: 0.9865\n",
      "Epoch 2/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9869\n",
      " ROC-AUC - epoch: 2 - score: 0.993207\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98654 to 0.98674, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 342s 2ms/step - loss: 0.0329 - acc: 0.9869 - val_loss: 0.0338 - val_acc: 0.9867\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 72us/step\n",
      "1521319912\n",
      "128 2 2\n",
      "rnn_w_conv_glc_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9876\n",
      " ROC-AUC - epoch: 1 - score: 0.995508\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98774, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 358s 2ms/step - loss: 0.0313 - acc: 0.9876 - val_loss: 0.0301 - val_acc: 0.9877\n",
      "Epoch 2/2\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9881\n",
      " ROC-AUC - epoch: 2 - score: 0.994951\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 346s 2ms/step - loss: 0.0295 - acc: 0.9881 - val_loss: 0.0304 - val_acc: 0.9877\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521320645\n",
      "128 1 1\n",
      "rnn_w_conv_glc_128_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9880\n",
      " ROC-AUC - epoch: 1 - score: 0.995977\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98904, saving model to ./NewRnnModels/rnn_w_conv_glc_128_w_dict.hdf5\n",
      "143613/143613 [==============================] - 362s 3ms/step - loss: 0.0299 - acc: 0.9880 - val_loss: 0.0273 - val_acc: 0.9890\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 12s 81us/step\n",
      "1521321037\n",
      "32 3 3\n",
      "rnn_w_conv_glc_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9813\n",
      " ROC-AUC - epoch: 1 - score: 0.987813\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98356, saving model to ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1286s 9ms/step - loss: 0.0507 - acc: 0.9813 - val_loss: 0.0428 - val_acc: 0.9836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9836\n",
      " ROC-AUC - epoch: 2 - score: 0.990148\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98356 to 0.98399, saving model to ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1269s 9ms/step - loss: 0.0422 - acc: 0.9836 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "Epoch 3/3\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9845\n",
      " ROC-AUC - epoch: 3 - score: 0.989710\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 1270s 9ms/step - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0459 - val_acc: 0.9818\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 72us/step\n",
      "1521324887\n",
      "32 2 2\n",
      "rnn_w_conv_glc_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9842\n",
      " ROC-AUC - epoch: 1 - score: 0.991951\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98560, saving model to ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1282s 9ms/step - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0365 - val_acc: 0.9856\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852\n",
      " ROC-AUC - epoch: 2 - score: 0.991678\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 1270s 9ms/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0378 - val_acc: 0.9847\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521327470\n",
      "32 1 1\n",
      "rnn_w_conv_glc_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9851\n",
      " ROC-AUC - epoch: 1 - score: 0.991226\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98531, saving model to ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "143613/143613 [==============================] - 1288s 9ms/step - loss: 0.0378 - acc: 0.9851 - val_loss: 0.0363 - val_acc: 0.9853\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 71us/step\n",
      "1521328788\n",
      "32 1 1\n",
      "rnn_w_conv_glc_32_w_dict\n",
      "load model: ./NewRnnModels/rnn_w_conv_glc_32_w_dict.hdf5\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      " 56000/143613 [==========>...................] - ETA: 12:28 - loss: 0.0356 - acc: 0.9859"
     ]
    }
   ],
   "source": [
    "for embeddings_index, embed_size, embedding_name in embeddings_index_pool:\n",
    "    \n",
    "    word_index = tok.word_index\n",
    "    #prepare embedding matrix\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embed_size))\n",
    "    not_found_word = {}\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        try: \n",
    "            embedding_vector = embeddings_index[word] # w2v_model['/en/'+ word] #w2v_model[word]#\n",
    "        except KeyError:\n",
    "            embedding_vector = embeddings_index.get(bad_word_dict.get(word), None)\n",
    "    #         embedding_vector = None #np.zeros(embed_size)\n",
    "            if embedding_vector is None:\n",
    "                not_found_word[word] = tok.word_counts[word]#i\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(len(not_found_word))\n",
    "    \n",
    "    for batch_size, epochs, patience in batch_epoch_patience_pool:\n",
    "        print(batch_size, epochs, patience)\n",
    "    \n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9)#, random_state=233)\n",
    "\n",
    "        sequence_input = Input(shape=(maxlen, ))\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "        x = SpatialDropout1D(0.2)(x)\n",
    "        x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "        x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "        avg_pool = GlobalAveragePooling1D()(x) \n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        x = concatenate([avg_pool, max_pool]) \n",
    "        # x = Dense(128, activation='relu')(x)\n",
    "        # x = Dropout(0.1)(x)\n",
    "        preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "        model = Model(sequence_input, preds)\n",
    "\n",
    "\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "\n",
    "        run_name = \"rnn_w_conv_{}_{}_w_dict\".format(embedding_name, batch_size)\n",
    "        print(run_name)\n",
    "\n",
    "        filepath= './NewRnnModels/' + run_name + '.hdf5'\n",
    "        \n",
    "        try: \n",
    "            print('load model: ' + str(filepath))\n",
    "            model.load_weights(filepath)\n",
    "        except:\n",
    "            print('no model found')\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=patience)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
    "        #Loading model weights\n",
    "        model.load_weights(filepath)\n",
    "        print('Predicting using the best model/epoch so far....')\n",
    "        y_pred = model.predict(x_test,batch_size=1024,verbose=1)\n",
    "\n",
    "        y_pred.shape\n",
    "\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "        submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "        import time\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./NewRnnPreds/' + run_name + '_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 150, 300)     30000000    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 150, 300)     0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 150, 256)     329472      spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 148, 64)      49216       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            774         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,379,462\n",
      "Trainable params: 379,462\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_word_dict = {\n",
    "    \"'ass\": 'ass',\n",
    "     \"'shit\": 'shit',\n",
    "     \"'stupid\": 'stupid',\n",
    "     'asse': 'ass',\n",
    "     'asspie': 'ass',\n",
    "     \"bitch'\": 'bitch',\n",
    "     'bitchbot': 'bitch',\n",
    "     'bith': 'bitch',\n",
    "     'bithc': 'bitch',\n",
    "     'bithces': 'bitch',\n",
    "     'choked': 'choke',\n",
    "     'cocain': 'cocaine',\n",
    "     'cuck': 'cock',\n",
    "     'cucks': 'cocks',\n",
    "     'decease': 'deceased',\n",
    "     'deth': 'death',\n",
    "     'diedres': 'crap',\n",
    "     'dik': 'dick',\n",
    "     'donkeysex': 'dick',\n",
    "     'faggt': 'faggot',\n",
    "     \"fool'\": 'fool',\n",
    "     \"fuck'\": 'fuck',\n",
    "     'fuckerucker': 'fucker',\n",
    "     'fuckn': 'fuck',\n",
    "     'fuock': 'fuck',\n",
    "     'gayy': 'gay',\n",
    "     'headsdick': 'dick',\n",
    "     'homopetersymonds': 'homo',\n",
    "     'horsecock': 'cock',\n",
    "     'maoth': 'mouth',\n",
    "     \"mother's\": 'mother',\n",
    "     'motherfuc': 'motherfucker',\n",
    "     'mothjer': 'mother',\n",
    "     'niggetr': 'nigger',\n",
    "     'niggors': 'nigger',\n",
    "     'nonense': 'nonsense',\n",
    "     'nonsesnse': 'nonsense',\n",
    "     'peenus': 'penis',\n",
    "     'peni': 'penis',\n",
    "     \"penis'\": 'penis',\n",
    "     'pneis': 'penis',\n",
    "     'pornn': 'porn',\n",
    "     'semite': 'semitic',\n",
    "     'sexe': 'sex',\n",
    "     'sexsex': 'sex',\n",
    "     'sexualit': 'sexuality',\n",
    "     'sexuall': 'sexual',\n",
    "     'sockpuppet': 'alias',\n",
    "     'sockpuppetry': 'alias',\n",
    "     'vagpenis': 'penis',\n",
    "     'valentin': 'valentine',\n",
    "     'youfuck': 'fuck',\n",
    "     'zdick': 'dick'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
