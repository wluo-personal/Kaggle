{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'adfasfad'#'/home/kai/data/resources/glove/glove.840B.300d.txt' #glove.twitter.27B.200d.txt\n",
    "PATH = '~/data/toxic/data/'\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "test = pd.read_csv(PATH + 'test.csv')\n",
    "#train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "#test = pd.read_csv(PATH + 'cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_col = 'comment_text' # 'comment_text_cleaned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train[comment_col].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[comment_col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300\n",
    "if 'twitter' in EMBEDDING_FILE:\n",
    "    embed_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# file = '/home/kai/data/resources/google/freebase-vectors-skipgram1000-en.bin'#GoogleNews-vectors-negative300.bin'\n",
    "# w2v_model = gensim.models.KeyedVectors.load_word2vec_format(file, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "#     for line in f:\n",
    "#         values = line.rstrip().rsplit(' ')\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')\n",
    "#         embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32') \n",
    "#file = '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors'\n",
    "file = '/home/kai/data/resources/dependency/bow5.words'#deps.words' # don't include .bz2\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(file, encoding='utf8'))\n",
    "#embeddings_index.pop('2000000')  # only needed when using lexvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "not_found_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    try: \n",
    "        embedding_vector = embeddings_index.get(word) # w2v_model['/en/'+ word] #w2v_model[word]#\n",
    "    except KeyError:\n",
    "        embedding_vector = None #np.zeros(embed_size)\n",
    "        not_found_count += 1\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(not_found_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(maxlen, ))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x) \n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "# filepath=\"lex_300d_weights_b512_ep6.hdf5\"\n",
    "# print('load model: ' + str(filepath))\n",
    "# model.load_weights(filepath)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath= \"dep_300d_weights_b128_ep6.hdf5\" #\"twit_weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "callbacks_list = [ra_val,checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/6\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9761\n",
      " ROC-AUC - epoch: 1 - score: 0.970835\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97814, saving model to dep_300d_weights_b128_ep6.hdf5\n",
      "143613/143613 [==============================] - 316s 2ms/step - loss: 0.0745 - acc: 0.9761 - val_loss: 0.0635 - val_acc: 0.9781\n",
      "Epoch 2/6\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9806\n",
      " ROC-AUC - epoch: 2 - score: 0.979198\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97814 to 0.98171, saving model to dep_300d_weights_b128_ep6.hdf5\n",
      "143613/143613 [==============================] - 317s 2ms/step - loss: 0.0545 - acc: 0.9806 - val_loss: 0.0511 - val_acc: 0.9817\n",
      "Epoch 3/6\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9813\n",
      " ROC-AUC - epoch: 3 - score: 0.982574\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 325s 2ms/step - loss: 0.0510 - acc: 0.9813 - val_loss: 0.0518 - val_acc: 0.9814\n",
      "Epoch 4/6\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9820\n",
      " ROC-AUC - epoch: 4 - score: 0.984063\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98171 to 0.98235, saving model to dep_300d_weights_b128_ep6.hdf5\n",
      "143613/143613 [==============================] - 319s 2ms/step - loss: 0.0485 - acc: 0.9820 - val_loss: 0.0487 - val_acc: 0.9823\n",
      "Epoch 5/6\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9824\n",
      " ROC-AUC - epoch: 5 - score: 0.984080\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "143613/143613 [==============================] - 330s 2ms/step - loss: 0.0471 - acc: 0.9824 - val_loss: 0.0511 - val_acc: 0.9816\n",
      "Epoch 6/6\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9829\n",
      " ROC-AUC - epoch: 6 - score: 0.984728\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98235 to 0.98293, saving model to dep_300d_weights_b128_ep6.hdf5\n",
      "143613/143613 [==============================] - 321s 2ms/step - loss: 0.0456 - acc: 0.9829 - val_loss: 0.0471 - val_acc: 0.9829\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 70us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
    "#Loading model weights\n",
    "model.load_weights(filepath)\n",
    "print('Predicting using the best model/epoch so far....')\n",
    "y_pred = model.predict(x_test,batch_size=1024,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520614488\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "import time\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./BaseEstPreds/' + 'rnn_w_conv_dep_d300_b128_ep6' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 150, 300)     30000000    input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_9 (SpatialDro (None, 150, 300)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 150, 256)     329472      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 148, 64)      49216       bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 64)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 64)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 6)            774         concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 30,379,462\n",
      "Trainable params: 379,462\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 150, 300)     30000000    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_5 (SpatialDro (None, 150, 300)     0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 150, 256)     329472      spatial_dropout1d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 148, 64)      49216       bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            774         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,379,462\n",
      "Trainable params: 379,462\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
