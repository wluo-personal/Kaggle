{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory. \n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'adfasfad'#'/home/kai/data/resources/glove/glove.840B.300d.txt' #glove.twitter.27B.200d.txt\n",
    "PATH = '~/data/toxic/data/'\n",
    "# train = pd.read_csv(PATH + 'train.csv')\n",
    "# test = pd.read_csv(PATH + 'test.csv')\n",
    "# train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "# test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "train = pd.read_csv(PATH + 'train_preprocessed.csv')\n",
    "test = pd.read_csv(PATH + 'test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>set</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text                id  \\\n",
       "0  explanation why the edits made under my userna...  0000997932d777bf   \n",
       "\n",
       "   identity_hate  insult  obscene    set  severe_toxic  threat  toxic  \\\n",
       "0            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "\n",
       "   toxicity  \n",
       "0       0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2Vec(source):\n",
    "    embed_size = 300\n",
    "    if source.lower() == 'ft-common':\n",
    "        file = '/home/kai/data/resources/FastText/crawl-300d-2M.vec'\n",
    "    elif source.lower() == 'ft-wiki':\n",
    "        file = '/home/kai/data/resources/FastText/wiki.en.vec'\n",
    "    elif source.lower() == 'lex':\n",
    "        file = '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors'\n",
    "    elif source.lower() == 'gl-common':\n",
    "        file = '/home/kai/data/resources/glove/glove.840B.300d.txt'\n",
    "    elif source.lower() == 'gl-twitter':\n",
    "        file = '/home/kai/data/resources/glove/glove.twitter.27B.200d.txt'\n",
    "        embed_size = 200\n",
    "    def get_coefs(word,*arr): \n",
    "        try:\n",
    "            return word, np.asarray(arr, dtype='float32') \n",
    "        except ValueError:\n",
    "            return 'nnnnnnnaaaaaaa@@!',np.zeros(embed_size)\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(file, encoding='utf8'))\n",
    "    return embeddings_index, embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index_lex, embed_size = word2Vec('lex')\n",
    "# embeddings_index_glc, embed_size = word2Vec('gl-common')\n",
    "embeddings_index_glt, embed_size = word2Vec('gl-twitter')\n",
    "# embeddings_index_ftc, embed_size = word2Vec('ft-common')\n",
    "# embeddings_index_ftw, embed_size = word2Vec('ft-wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings_index_glt['didn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_col = 'comment_text' # 'comment_text_cleaned' \n",
    "\n",
    "X_train = train[comment_col].str.lower().fillna('something') # something is a word of neutral sentiment\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[comment_col].str.lower().fillna('something')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "\n",
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_word_count = sorted(tok.word_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorted_word_count#[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for w,w_cnt in sorted_word_count[:60000]:\n",
    "#     if w in embeddings_index_glt:\n",
    "#         count+=1\n",
    "# print(count)\n",
    "# print(count/60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type(embeddings_index_glt['hesitates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json as js\n",
    "with open('/home/kai/data/kaggle/toxic/wl/models/RNN/rnn/dirty_word_dict.json', 'r') as file:\n",
    "    bad_word_dict = js.load(file)\n",
    "\n",
    "len(bad_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index = embeddings_index_lex\n",
    "# word_index = tok.word_index\n",
    "# #prepare embedding matrix\n",
    "# num_words = min(max_features, len(word_index) + 1)\n",
    "# embedding_matrix = np.zeros((num_words, embed_size))\n",
    "# not_found_word = {}\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features:\n",
    "#         continue\n",
    "#     try: \n",
    "#         embedding_vector = embeddings_index[word] # w2v_model['/en/'+ word] #w2v_model[word]#\n",
    "#     except KeyError:\n",
    "#         embedding_vector = embeddings_index.get(bad_word_dict.get(word), None)\n",
    "# #         embedding_vector = None #np.zeros(embed_size)\n",
    "#         if embedding_vector is None:\n",
    "#             not_found_word[word] = tok.word_counts[word]#i\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "# print(len(not_found_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embeddings_index, embed_size, max_features, tokenizer, bad_word_dict):\n",
    "    word_index = tokenizer.word_index\n",
    "    #prepare embedding matrix\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embed_size))\n",
    "    not_found_word = {}\n",
    "    replaced_word = {}\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        try: \n",
    "            embedding_vector = embeddings_index[word] # w2v_model['/en/'+ word] #w2v_model[word]#\n",
    "        except KeyError:\n",
    "            replacement = bad_word_dict.get(word)\n",
    "            embedding_vector = embeddings_index.get(replacement, None)\n",
    "    #         embedding_vector = None #np.zeros(embed_size)\n",
    "            if embedding_vector is None:\n",
    "                not_found_word[word] = tokenizer.word_counts[word]#i\n",
    "            else:\n",
    "                if word not in replaced_word:\n",
    "                    replaced_word[word] = replacement\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('{} words not found in embedding file'.format(len(not_found_word)))\n",
    "    print('{} words are replaced:'.format(len(replaced_word)))\n",
    "    print(replaced_word)\n",
    "    return embedding_matrix, not_found_word, replaced_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(maxlen, max_features, embedding_matrix, embed_size, output_size):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "\n",
    "    conv1d_1 = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool_1 = GlobalAveragePooling1D()(conv1d_1) \n",
    "    max_pool_1 = GlobalMaxPooling1D()(conv1d_1)\n",
    "\n",
    "    x = concatenate([avg_pool_1, max_pool_1])\n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(output_size, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index_pool = [\n",
    "#     (embeddings_index_lex, 300, 'lex'),\n",
    "#     (embeddings_index_glc, 300, 'glc'),\n",
    "    (embeddings_index_glt, 200, 'glt')#,\n",
    "#     (embeddings_index_ftc, 300, 'ftc'), \n",
    "#     (embeddings_index_ftw, 300, 'ftw')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_epoch_patience_pool = [\n",
    "    (1024, 20, 5)\n",
    "#     (1024, 10, 10),\n",
    "#     (1024, 3, 3),\n",
    "#     (1024, 3, 3),\n",
    "#     (1024, 3, 3),\n",
    "#     (512, 8, 8),\n",
    "#     (512, 2, 2),\n",
    "#     (512, 2, 2),\n",
    "#     (512, 2, 2),\n",
    "#     (128, 5, 5),\n",
    "#     (128, 2, 2),\n",
    "#     (128, 2, 2),\n",
    "#     (128, 1, 1),\n",
    "#     (32, 3, 3),\n",
    "#     (32, 2, 2),\n",
    "#     (32, 1, 1),\n",
    "#     (32, 1, 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_index = np.load('train_index.npy')\n",
    "\n",
    "# test_index = np.load('test_index.npy')\n",
    "\n",
    "# print(train_index, test_index)\n",
    "\n",
    "# real_train_index = [x for x in list(train_index) if x < len(train)]\n",
    "# real_val_index = [x for x in list(test_index) if x < len(train)]\n",
    "\n",
    "# assert len(real_train_index) + len(real_val_index) == len(train)\n",
    "\n",
    "# X_tra, X_val = x_train[real_train_index, :], x_train[real_val_index, :]\n",
    "# y_tra, y_val = y_train[real_train_index, :], y_train[real_val_index, :]\n",
    "\n",
    "# print(X_tra.shape, X_val.shape, x_train.shape)\n",
    "# print(y_tra.shape, y_val.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 150), (159571, 6), (153164, 150))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adversarial_validation(x_train, y_train, x_test, embeddings_index, embed_size):\n",
    "    \"\"\"\n",
    "    x_train, y_train, x_test: tokenized, indexed and padded data\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xdat = np.vstack([x_train, x_test])\n",
    "    print(xdat.shape)\n",
    "    ydat = np.vstack([np.ones([len(x_train),1]), np.zeros([len(x_test),1])]) # train=>1, test=>0\n",
    "    print(ydat.shape)\n",
    "    \n",
    "    y_val_oof_preds = np.ones([len(ydat), 1]) * 999 # we will check if there is still any 999 after the whole thing is done\n",
    "    \n",
    "    embedding_matrix, _, _ = get_embedding_matrix(embeddings_index, embed_size, max_features, tok, bad_word_dict)\n",
    "    \n",
    "    nfolds = 5\n",
    "    xseed = 1001\n",
    "    batch_size = 1024\n",
    "    epochs = 10\n",
    "    patience = 3\n",
    "    # stratified split\n",
    "    skf = StratifiedKFold(n_splits= nfolds, random_state= xseed)\n",
    "    score_vec = np.zeros((nfolds,1))\n",
    "\n",
    "    index_list = {}\n",
    "    for (f, (train_index, test_index)) in enumerate(skf.split(xdat, ydat[:,0])):\n",
    "        # split \n",
    "        X_tra, X_val = xdat[train_index], xdat[test_index]\n",
    "        y_tra, y_val = ydat[train_index,0], ydat[test_index,0] \n",
    "\n",
    "        model = get_model(maxlen, max_features, embedding_matrix, embed_size, 1)\n",
    "        \n",
    "        run_name = 'cnn_nfolds_{}_fond_{}'.format(nfolds, f)\n",
    "        print(run_name)\n",
    "        filepath= './NewRnnModels/AdversarialValidation/' + run_name + '.hdf5'\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        #ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [checkpoint, early]\n",
    "\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
    "        #Loading model weights\n",
    "        model.load_weights(filepath)\n",
    "        print('Predicting using the best model/epoch so far....')\n",
    "        y_val_pred = model.predict(X_val,batch_size=1024,verbose=1)\n",
    "        \n",
    "        y_val_oof_preds[test_index] = y_val_pred\n",
    "\n",
    "        roc_auc = roc_auc_score(y_val,y_val_pred)\n",
    "        print(roc_auc)\n",
    "        score_vec[f,:] = roc_auc\n",
    "    \n",
    "    return np.hstack([y_val_oof_preds, ydat]), score_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 150)\n",
      "(312735, 1)\n",
      "32208 words not found in embedding file\n",
      "51 words are replaced:\n",
      "{'busterd': 'bustard', 'britfags': 'faggot', 'fagsgod': 'faggot', 'dickus': 'dick', 'vagpenis': 'penis', 'bustards': 'bustard', 'pspsex': 'sex', 'homeostatic': 'homosexual', 'horsecock': 'cock', 'zdick': 'dick', 'dscc': 'dick', 'fuckerucker': 'fucker', \"mother's\": 'mother', 'niggors': 'nigger', 'semite': 'semitic', 'dickishness': 'dick', 'tdick': 'dick', \"father's\": 'father', 'terorists': 'terrorist', 'mothjer': 'mother', 'rapeing': 'rape', 'homopetersymonds': 'homo', 'cucks': 'cocks', 'burglaring': 'burglar', 'nonsesnse': 'nonsense', 'asslicker': 'ass', 'sexvbutt': 'sex', 'donkeysex': 'dick', 'fuock': 'fuck', 'niggetr': 'nigger', 'abuseful': 'abuse', 'faggt': 'faggot', 'fuckervckers': 'fuck', 'sockpuppetter': 'idiot', 'assraped': 'ass', 'headsdick': 'dick', 'peenus': 'penis', 'nonense': 'nonsense', 'diedres': 'crap', 'sockpuppetry': 'alias', 'noobish': 'noob', 'sockpuppet': 'alias', 'asspie': 'ass', 'sexbot': 'sex', 'maoth': 'mouth', 'fagcjcurrie': 'faggot', 'faggeorge': 'faggot', 'idotic': 'idiot', 'fuckeruckerfuck': 'fuck', 'sexsex': 'sex', 'terrosist': 'terrorist'}\n",
      "cnn_nfolds_5_fond_0\n",
      "Train on 250187 samples, validate on 62548 samples\n",
      "Epoch 1/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.5854\n",
      "Epoch 00001: val_loss improved from inf to 0.64199, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_0.hdf5\n",
      "250187/250187 [==============================] - 72s 289us/step - loss: 0.6611 - acc: 0.5854 - val_loss: 0.6420 - val_acc: 0.5989\n",
      "Epoch 2/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.6347 - acc: 0.6118\n",
      "Epoch 00002: val_loss improved from 0.64199 to 0.62604, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_0.hdf5\n",
      "250187/250187 [==============================] - 71s 285us/step - loss: 0.6347 - acc: 0.6118 - val_loss: 0.6260 - val_acc: 0.6210\n",
      "Epoch 3/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.6215\n",
      "Epoch 00003: val_loss improved from 0.62604 to 0.62084, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_0.hdf5\n",
      "250187/250187 [==============================] - 71s 284us/step - loss: 0.6264 - acc: 0.6215 - val_loss: 0.6208 - val_acc: 0.6264\n",
      "Epoch 4/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.6411\n",
      "Epoch 00006: val_loss improved from 0.61663 to 0.61511, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_0.hdf5\n",
      "250187/250187 [==============================] - 73s 290us/step - loss: 0.6103 - acc: 0.6411 - val_loss: 0.6151 - val_acc: 0.6324\n",
      "Epoch 7/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.6466\n",
      "Epoch 00007: val_loss did not improve\n",
      "250187/250187 [==============================] - 72s 286us/step - loss: 0.6056 - acc: 0.6467 - val_loss: 0.6182 - val_acc: 0.6318\n",
      "Epoch 8/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.6520\n",
      "Epoch 00008: val_loss improved from 0.61511 to 0.61389, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_0.hdf5\n",
      "250187/250187 [==============================] - 71s 285us/step - loss: 0.6011 - acc: 0.6520 - val_loss: 0.6139 - val_acc: 0.6357\n",
      "Epoch 9/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.6573\n",
      "Epoch 00009: val_loss did not improve\n",
      "250187/250187 [==============================] - 71s 285us/step - loss: 0.5959 - acc: 0.6572 - val_loss: 0.6144 - val_acc: 0.6347\n",
      "Epoch 10/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.5915 - acc: 0.6623\n",
      "Epoch 00010: val_loss did not improve\n",
      "250187/250187 [==============================] - 71s 286us/step - loss: 0.5915 - acc: 0.6623 - val_loss: 0.6157 - val_acc: 0.6348\n",
      "Epoch 11/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.5864 - acc: 0.6684\n",
      "Epoch 00011: val_loss did not improve\n",
      "250187/250187 [==============================] - 72s 286us/step - loss: 0.5864 - acc: 0.6684 - val_loss: 0.6190 - val_acc: 0.6333\n",
      "Epoch 12/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.6734\n",
      "Epoch 00012: val_loss did not improve\n",
      "250187/250187 [==============================] - 71s 285us/step - loss: 0.5820 - acc: 0.6734 - val_loss: 0.6185 - val_acc: 0.6364\n",
      "Epoch 13/20\n",
      "249856/250187 [============================>.] - ETA: 0s - loss: 0.5772 - acc: 0.6779\n",
      "Epoch 00013: val_loss did not improve\n",
      "250187/250187 [==============================] - 72s 286us/step - loss: 0.5771 - acc: 0.6779 - val_loss: 0.6211 - val_acc: 0.6345\n",
      "Predicting using the best model/epoch so far....\n",
      "62548/62548 [==============================] - 6s 91us/step\n",
      "0.704184722359264\n",
      "cnn_nfolds_5_fond_1\n",
      "Train on 250188 samples, validate on 62547 samples\n",
      "Epoch 1/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6604 - acc: 0.5865\n",
      "Epoch 00001: val_loss improved from inf to 0.63615, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 73s 290us/step - loss: 0.6604 - acc: 0.5864 - val_loss: 0.6361 - val_acc: 0.6129\n",
      "Epoch 2/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6345 - acc: 0.6124\n",
      "Epoch 00002: val_loss improved from 0.63615 to 0.63069, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6345 - acc: 0.6124 - val_loss: 0.6307 - val_acc: 0.6142\n",
      "Epoch 3/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.6222\n",
      "Epoch 00003: val_loss improved from 0.63069 to 0.62911, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6258 - acc: 0.6222 - val_loss: 0.6291 - val_acc: 0.6172\n",
      "Epoch 4/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6207 - acc: 0.6291\n",
      "Epoch 00004: val_loss improved from 0.62911 to 0.62296, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 286us/step - loss: 0.6207 - acc: 0.6291 - val_loss: 0.6230 - val_acc: 0.6221\n",
      "Epoch 5/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6147 - acc: 0.6353\n",
      "Epoch 00005: val_loss improved from 0.62296 to 0.62031, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 284us/step - loss: 0.6147 - acc: 0.6354 - val_loss: 0.6203 - val_acc: 0.6280\n",
      "Epoch 6/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6099 - acc: 0.6425\n",
      "Epoch 00006: val_loss improved from 0.62031 to 0.61902, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 284us/step - loss: 0.6099 - acc: 0.6426 - val_loss: 0.6190 - val_acc: 0.6297\n",
      "Epoch 7/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6051 - acc: 0.6486\n",
      "Epoch 00007: val_loss did not improve\n",
      "250188/250188 [==============================] - 72s 289us/step - loss: 0.6051 - acc: 0.6487 - val_loss: 0.6228 - val_acc: 0.6267\n",
      "Epoch 8/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6000 - acc: 0.6549\n",
      "Epoch 00008: val_loss improved from 0.61902 to 0.61738, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6000 - acc: 0.6549 - val_loss: 0.6174 - val_acc: 0.6336\n",
      "Epoch 9/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.6584\n",
      "Epoch 00009: val_loss improved from 0.61738 to 0.61732, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_1.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.5960 - acc: 0.6584 - val_loss: 0.6173 - val_acc: 0.6333\n",
      "Epoch 10/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5904 - acc: 0.6644\n",
      "Epoch 00010: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.5905 - acc: 0.6644 - val_loss: 0.6182 - val_acc: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.6698\n",
      "Epoch 00011: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 284us/step - loss: 0.5857 - acc: 0.6697 - val_loss: 0.6194 - val_acc: 0.6353\n",
      "Epoch 12/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5816 - acc: 0.6750\n",
      "Epoch 00012: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.5816 - acc: 0.6750 - val_loss: 0.6214 - val_acc: 0.6331\n",
      "Epoch 13/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5773 - acc: 0.6791\n",
      "Epoch 00013: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.5773 - acc: 0.6790 - val_loss: 0.6236 - val_acc: 0.6318\n",
      "Epoch 14/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5723 - acc: 0.6844\n",
      "Epoch 00014: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 286us/step - loss: 0.5723 - acc: 0.6844 - val_loss: 0.6280 - val_acc: 0.6307\n",
      "Predicting using the best model/epoch so far....\n",
      "62547/62547 [==============================] - 6s 93us/step\n",
      "0.7015954057895463\n",
      "cnn_nfolds_5_fond_2\n",
      "Train on 250188 samples, validate on 62547 samples\n",
      "Epoch 1/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6597 - acc: 0.5884\n",
      "Epoch 00001: val_loss improved from inf to 0.63576, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_2.hdf5\n",
      "250188/250188 [==============================] - 73s 290us/step - loss: 0.6597 - acc: 0.5884 - val_loss: 0.6358 - val_acc: 0.6087\n",
      "Epoch 2/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6343 - acc: 0.6132\n",
      "Epoch 00002: val_loss improved from 0.63576 to 0.62680, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_2.hdf5\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.6343 - acc: 0.6132 - val_loss: 0.6268 - val_acc: 0.6212\n",
      "Epoch 3/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6251 - acc: 0.6243\n",
      "Epoch 00003: val_loss improved from 0.62680 to 0.62487, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_2.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6251 - acc: 0.6242 - val_loss: 0.6249 - val_acc: 0.6203\n",
      "Epoch 4/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6190 - acc: 0.6310\n",
      "Epoch 00004: val_loss improved from 0.62487 to 0.62060, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_2.hdf5\n",
      "250188/250188 [==============================] - 71s 284us/step - loss: 0.6190 - acc: 0.6310 - val_loss: 0.6206 - val_acc: 0.6262\n",
      "Epoch 5/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6132 - acc: 0.6372\n",
      "Epoch 00005: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.6132 - acc: 0.6372 - val_loss: 0.6212 - val_acc: 0.6265\n",
      "Epoch 6/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6089 - acc: 0.6426\n",
      "Epoch 00006: val_loss improved from 0.62060 to 0.61870, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_2.hdf5\n",
      "250188/250188 [==============================] - 71s 284us/step - loss: 0.6089 - acc: 0.6425 - val_loss: 0.6187 - val_acc: 0.6274\n",
      "Epoch 7/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6031 - acc: 0.6503\n",
      "Epoch 00007: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.6031 - acc: 0.6503 - val_loss: 0.6197 - val_acc: 0.6294\n",
      "Epoch 8/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5988 - acc: 0.6543\n",
      "Epoch 00008: val_loss did not improve\n",
      "250188/250188 [==============================] - 72s 288us/step - loss: 0.5988 - acc: 0.6543 - val_loss: 0.6209 - val_acc: 0.6288\n",
      "Epoch 9/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5945 - acc: 0.6607\n",
      "Epoch 00009: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.5945 - acc: 0.6607 - val_loss: 0.6218 - val_acc: 0.6285\n",
      "Epoch 10/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5888 - acc: 0.6665\n",
      "Epoch 00010: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.5888 - acc: 0.6665 - val_loss: 0.6199 - val_acc: 0.6287\n",
      "Epoch 11/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5849 - acc: 0.6701\n",
      "Epoch 00011: val_loss did not improve\n",
      "250188/250188 [==============================] - 70s 279us/step - loss: 0.5849 - acc: 0.6701 - val_loss: 0.6228 - val_acc: 0.6278\n",
      "Predicting using the best model/epoch so far....\n",
      "62547/62547 [==============================] - 4s 71us/step\n",
      "0.6950431756127735\n",
      "cnn_nfolds_5_fond_3\n",
      "Train on 250188 samples, validate on 62547 samples\n",
      "Epoch 1/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6648 - acc: 0.5837\n",
      "Epoch 00001: val_loss improved from inf to 0.63996, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_3.hdf5\n",
      "250188/250188 [==============================] - 75s 298us/step - loss: 0.6648 - acc: 0.5837 - val_loss: 0.6400 - val_acc: 0.6026\n",
      "Epoch 2/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6359 - acc: 0.6120\n",
      "Epoch 00002: val_loss improved from 0.63996 to 0.63881, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_3.hdf5\n",
      "250188/250188 [==============================] - 73s 291us/step - loss: 0.6360 - acc: 0.6120 - val_loss: 0.6388 - val_acc: 0.6035\n",
      "Epoch 3/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6264 - acc: 0.6219\n",
      "Epoch 00003: val_loss improved from 0.63881 to 0.62289, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_3.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6263 - acc: 0.6219 - val_loss: 0.6229 - val_acc: 0.6220\n",
      "Epoch 4/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6202 - acc: 0.6294\n",
      "Epoch 00004: val_loss improved from 0.62289 to 0.62052, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_3.hdf5\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.6202 - acc: 0.6294 - val_loss: 0.6205 - val_acc: 0.6256\n",
      "Epoch 5/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6142 - acc: 0.6378\n",
      "Epoch 00005: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6142 - acc: 0.6377 - val_loss: 0.6257 - val_acc: 0.6171\n",
      "Epoch 6/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6094 - acc: 0.6425\n",
      "Epoch 00006: val_loss improved from 0.62052 to 0.61718, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_3.hdf5\n",
      "250188/250188 [==============================] - 71s 286us/step - loss: 0.6094 - acc: 0.6425 - val_loss: 0.6172 - val_acc: 0.6295\n",
      "Epoch 7/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6049 - acc: 0.6481\n",
      "Epoch 00007: val_loss improved from 0.61718 to 0.61688, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_3.hdf5\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.6050 - acc: 0.6480 - val_loss: 0.6169 - val_acc: 0.6306\n",
      "Epoch 8/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.6009 - acc: 0.6534\n",
      "Epoch 00008: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 284us/step - loss: 0.6009 - acc: 0.6535 - val_loss: 0.6202 - val_acc: 0.6270\n",
      "Epoch 9/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5960 - acc: 0.6582\n",
      "Epoch 00009: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.5960 - acc: 0.6583 - val_loss: 0.6206 - val_acc: 0.6278\n",
      "Epoch 10/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.6637\n",
      "Epoch 00010: val_loss did not improve\n",
      "250188/250188 [==============================] - 73s 290us/step - loss: 0.5919 - acc: 0.6637 - val_loss: 0.6307 - val_acc: 0.6221\n",
      "Epoch 11/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.6691\n",
      "Epoch 00011: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 285us/step - loss: 0.5872 - acc: 0.6691 - val_loss: 0.6197 - val_acc: 0.6299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "249856/250188 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.6734\n",
      "Epoch 00012: val_loss did not improve\n",
      "250188/250188 [==============================] - 71s 283us/step - loss: 0.5820 - acc: 0.6734 - val_loss: 0.6243 - val_acc: 0.6276\n",
      "Predicting using the best model/epoch so far....\n",
      "62547/62547 [==============================] - 5s 73us/step\n",
      "0.6984662343198196\n",
      "cnn_nfolds_5_fond_4\n",
      "Train on 250189 samples, validate on 62546 samples\n",
      "Epoch 1/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.5884\n",
      "Epoch 00001: val_loss improved from inf to 0.63724, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_4.hdf5\n",
      "250189/250189 [==============================] - 75s 299us/step - loss: 0.6579 - acc: 0.5884 - val_loss: 0.6372 - val_acc: 0.6075\n",
      "Epoch 2/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6333 - acc: 0.6134\n",
      "Epoch 00002: val_loss improved from 0.63724 to 0.62723, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_4.hdf5\n",
      "250189/250189 [==============================] - 72s 288us/step - loss: 0.6333 - acc: 0.6134 - val_loss: 0.6272 - val_acc: 0.6184\n",
      "Epoch 3/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6254 - acc: 0.6238\n",
      "Epoch 00003: val_loss improved from 0.62723 to 0.62226, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_4.hdf5\n",
      "250189/250189 [==============================] - 74s 295us/step - loss: 0.6253 - acc: 0.6237 - val_loss: 0.6223 - val_acc: 0.6237\n",
      "Epoch 4/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6201 - acc: 0.6278\n",
      "Epoch 00004: val_loss improved from 0.62226 to 0.62045, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_4.hdf5\n",
      "250189/250189 [==============================] - 72s 289us/step - loss: 0.6202 - acc: 0.6277 - val_loss: 0.6204 - val_acc: 0.6244\n",
      "Epoch 5/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6138 - acc: 0.6367\n",
      "Epoch 00005: val_loss did not improve\n",
      "250189/250189 [==============================] - 72s 288us/step - loss: 0.6139 - acc: 0.6366 - val_loss: 0.6206 - val_acc: 0.6284\n",
      "Epoch 6/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6097 - acc: 0.6422\n",
      "Epoch 00006: val_loss improved from 0.62045 to 0.61943, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_4.hdf5\n",
      "250189/250189 [==============================] - 72s 288us/step - loss: 0.6097 - acc: 0.6421 - val_loss: 0.6194 - val_acc: 0.6295\n",
      "Epoch 7/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6055 - acc: 0.6460\n",
      "Epoch 00007: val_loss improved from 0.61943 to 0.61756, saving model to ./NewRnnModels/AdversarialValidation/cnn_nfolds_5_fond_4.hdf5\n",
      "250189/250189 [==============================] - 72s 289us/step - loss: 0.6055 - acc: 0.6461 - val_loss: 0.6176 - val_acc: 0.6312\n",
      "Epoch 8/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.6005 - acc: 0.6526\n",
      "Epoch 00008: val_loss did not improve\n",
      "250189/250189 [==============================] - 72s 288us/step - loss: 0.6005 - acc: 0.6527 - val_loss: 0.6189 - val_acc: 0.6307\n",
      "Epoch 9/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.5961 - acc: 0.6577\n",
      "Epoch 00009: val_loss did not improve\n",
      "250189/250189 [==============================] - 71s 286us/step - loss: 0.5961 - acc: 0.6578 - val_loss: 0.6178 - val_acc: 0.6316\n",
      "Epoch 10/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.5913 - acc: 0.6627\n",
      "Epoch 00010: val_loss did not improve\n",
      "250189/250189 [==============================] - 73s 294us/step - loss: 0.5913 - acc: 0.6628 - val_loss: 0.6177 - val_acc: 0.6305\n",
      "Epoch 11/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.5871 - acc: 0.6680\n",
      "Epoch 00011: val_loss did not improve\n",
      "250189/250189 [==============================] - 73s 290us/step - loss: 0.5871 - acc: 0.6680 - val_loss: 0.6195 - val_acc: 0.6335\n",
      "Epoch 12/20\n",
      "249856/250189 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.6732\n",
      "Epoch 00012: val_loss did not improve\n",
      "250189/250189 [==============================] - 72s 289us/step - loss: 0.5817 - acc: 0.6732 - val_loss: 0.6209 - val_acc: 0.6321\n",
      "Predicting using the best model/epoch so far....\n",
      "62546/62546 [==============================] - 5s 74us/step\n",
      "0.6985198711924168\n"
     ]
    }
   ],
   "source": [
    "y_val_oof_preds, score_vec = adversarial_validation(x_train, y_train, x_test, embeddings_index_glt, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_oof_preds = y_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_val_oof_preds', y_val_oof_preds) # was saved with name: 'y_oof_preds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_train_val_split(x_train, y_train, y_val_oof_preds, val_size=0.1):\n",
    "    try:\n",
    "        ordered_indexes = np.load('pick_val_from_the_start.npy')\n",
    "        print('ordered_indexes file loaded')\n",
    "    except OSError:\n",
    "        print('ordered_indexes file not found. computing...')\n",
    "        print(y_val_oof_preds.shape)\n",
    "        indexes = np.arange(len(y_val_oof_preds)).reshape(-1,1)\n",
    "        print(indexes.shape)\n",
    "        y_val_oof_preds_w_idx = np.hstack([indexes, y_val_oof_preds])\n",
    "        # 0.0 0.7034685015678406 1.0\n",
    "        # 1.0 0.994215190410614 1.0\n",
    "        # 2.0 0.7640196084976196 1.0\n",
    "        # 3.0 0.27269116044044495 1.0\n",
    "        # 4.0 0.5975226759910583 1.0\n",
    "        y_val_preds_dict = dict([(idx, y_val_pred) for idx, y_val_pred, y_val in y_val_oof_preds_w_idx if y_val == 1])\n",
    "        print(len(y_val_preds_dict))\n",
    "        print(list(y_val_preds_dict.items())[:3])\n",
    "        sorted_y_val_preds = sorted(y_val_preds_dict.items(), key=lambda x:x[1])\n",
    "        ordered_indexes = [int(idx) for idx in sorted(y_val_preds_dict, key=y_val_preds_dict.get)]\n",
    "        np.save('pick_val_from_the_start', ordered_indexes)\n",
    "        # ordered_indexes[:5] \n",
    "        # [(6833.0, 0.008921189233660698),\n",
    "        #  (44761.0, 0.012650093995034695),\n",
    "        #  (116571.0, 0.016914881765842438),\n",
    "        #  (117959.0, 0.0190033670514822),\n",
    "        #  (74465.0, 0.02322762832045555)......]\n",
    "\n",
    "    split_point = int(len(ordered_indexes)*val_size)\n",
    "    print('split point: {}. Before this point => val. After this point => train'.format(split_point))\n",
    "    val_indexes = ordered_indexes[:split_point]\n",
    "    train_indexes = ordered_indexes[split_point:]\n",
    "    assert len(val_indexes) + len(train_indexes) == len(ordered_indexes) ==len(x_train)\n",
    "\n",
    "    X_tra, X_val = x_train[train_indexes], x_train[val_indexes]\n",
    "    y_tra, y_val = y_train[train_indexes], y_train[val_indexes]\n",
    "\n",
    "    return X_tra, X_val, y_tra, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val_oof_preds = np.load('y_val_oof_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tra, X_val, y_tra, y_val = adversarial_train_val_split(x_train, y_train, y_oof_preds)#y_val_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32208 words not found in embedding file\n",
      "51 words are replaced:\n",
      "{'busterd': 'bustard', 'britfags': 'faggot', 'fagsgod': 'faggot', 'dickus': 'dick', 'vagpenis': 'penis', 'bustards': 'bustard', 'pspsex': 'sex', 'homeostatic': 'homosexual', 'horsecock': 'cock', 'zdick': 'dick', 'dscc': 'dick', 'fuckerucker': 'fucker', \"mother's\": 'mother', 'niggors': 'nigger', 'semite': 'semitic', 'dickishness': 'dick', 'tdick': 'dick', \"father's\": 'father', 'terorists': 'terrorist', 'mothjer': 'mother', 'rapeing': 'rape', 'homopetersymonds': 'homo', 'cucks': 'cocks', 'burglaring': 'burglar', 'nonsesnse': 'nonsense', 'asslicker': 'ass', 'sexvbutt': 'sex', 'donkeysex': 'dick', 'fuock': 'fuck', 'niggetr': 'nigger', 'abuseful': 'abuse', 'faggt': 'faggot', 'fuckervckers': 'fuck', 'sockpuppetter': 'idiot', 'assraped': 'ass', 'headsdick': 'dick', 'peenus': 'penis', 'nonense': 'nonsense', 'diedres': 'crap', 'sockpuppetry': 'alias', 'noobish': 'noob', 'sockpuppet': 'alias', 'asspie': 'ass', 'sexbot': 'sex', 'maoth': 'mouth', 'fagcjcurrie': 'faggot', 'faggeorge': 'faggot', 'idotic': 'idiot', 'fuckeruckerfuck': 'fuck', 'sexsex': 'sex', 'terrosist': 'terrorist'}\n",
      "1024 20 5\n",
      "rnn10_w_conv_glt_1024_w_new_dict\n",
      "load model: ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "no model found\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9688\n",
      " ROC-AUC - epoch: 1 - score: 0.968618\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05040, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 73s 511us/step - loss: 0.0987 - acc: 0.9688 - val_loss: 0.0504 - val_acc: 0.9818\n",
      "Epoch 2/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9815\n",
      " ROC-AUC - epoch: 2 - score: 0.978980\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05040 to 0.04578, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 69s 478us/step - loss: 0.0500 - acc: 0.9815 - val_loss: 0.0458 - val_acc: 0.9829\n",
      "Epoch 3/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9824\n",
      " ROC-AUC - epoch: 3 - score: 0.982153\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04578 to 0.04375, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 70s 484us/step - loss: 0.0465 - acc: 0.9824 - val_loss: 0.0438 - val_acc: 0.9833\n",
      "Epoch 4/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9831\n",
      " ROC-AUC - epoch: 4 - score: 0.984067\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04375 to 0.04326, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 69s 480us/step - loss: 0.0444 - acc: 0.9831 - val_loss: 0.0433 - val_acc: 0.9835\n",
      "Epoch 5/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9834\n",
      " ROC-AUC - epoch: 5 - score: 0.985614\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04326 to 0.04110, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 69s 481us/step - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0411 - val_acc: 0.9843\n",
      "Epoch 6/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9840\n",
      " ROC-AUC - epoch: 6 - score: 0.985510\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04110 to 0.04091, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 69s 479us/step - loss: 0.0415 - acc: 0.9840 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 7/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9843\n",
      " ROC-AUC - epoch: 7 - score: 0.985906\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 69s 482us/step - loss: 0.0403 - acc: 0.9843 - val_loss: 0.0409 - val_acc: 0.9845\n",
      "Epoch 8/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9846\n",
      " ROC-AUC - epoch: 8 - score: 0.986448\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04091 to 0.04030, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 69s 478us/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0403 - val_acc: 0.9843\n",
      "Epoch 9/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9850\n",
      " ROC-AUC - epoch: 9 - score: 0.986961\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.04030 to 0.04025, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 70s 485us/step - loss: 0.0383 - acc: 0.9850 - val_loss: 0.0403 - val_acc: 0.9845\n",
      "Epoch 10/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9851\n",
      " ROC-AUC - epoch: 10 - score: 0.986592\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.04025 to 0.03998, saving model to ./NewRnnModels/rnn10_w_conv_glt_1024_w_new_dict.hdf5\n",
      "143613/143613 [==============================] - 69s 481us/step - loss: 0.0380 - acc: 0.9851 - val_loss: 0.0400 - val_acc: 0.9846\n",
      "Epoch 11/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9855\n",
      " ROC-AUC - epoch: 11 - score: 0.987490\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "143613/143613 [==============================] - 69s 478us/step - loss: 0.0369 - acc: 0.9855 - val_loss: 0.0400 - val_acc: 0.9845\n",
      "Epoch 12/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9858\n",
      " ROC-AUC - epoch: 12 - score: 0.987093\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "143613/143613 [==============================] - 69s 480us/step - loss: 0.0361 - acc: 0.9858 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 13/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9860\n",
      " ROC-AUC - epoch: 13 - score: 0.986424\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "143613/143613 [==============================] - 70s 485us/step - loss: 0.0352 - acc: 0.9860 - val_loss: 0.0411 - val_acc: 0.9845\n",
      "Epoch 14/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9862\n",
      " ROC-AUC - epoch: 14 - score: 0.986847\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "143613/143613 [==============================] - 69s 481us/step - loss: 0.0346 - acc: 0.9862 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 15/20\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9867\n",
      " ROC-AUC - epoch: 15 - score: 0.986708\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "143613/143613 [==============================] - 69s 479us/step - loss: 0.0338 - acc: 0.9867 - val_loss: 0.0411 - val_acc: 0.9844\n",
      "Predicting using the best model/epoch so far....\n",
      "153164/153164 [==============================] - 11s 74us/step\n",
      "1521565637\n"
     ]
    }
   ],
   "source": [
    "for embeddings_index, embed_size, embedding_name in embeddings_index_pool:\n",
    "    \n",
    "    embedding_matrix, _, _ = get_embedding_matrix(embeddings_index, embed_size, max_features, tok, bad_word_dict)\n",
    "    \n",
    "    for batch_size, epochs, patience in batch_epoch_patience_pool:\n",
    "        print(batch_size, epochs, patience)\n",
    "        \n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, test_size=0.1)#, random_state=233)\n",
    "        #X_tra, X_val, y_tra, y_val = adversarial_train_val_split(x_train, y_train, y_val_oof_preds)\n",
    "\n",
    "        model = get_model(maxlen, max_features, embedding_matrix, embed_size, 6)\n",
    "\n",
    "        run_name = \"rnn10_w_conv_{}_{}_w_new_dict\".format(embedding_name, batch_size)\n",
    "        print(run_name)\n",
    "\n",
    "        filepath= './NewRnnModels/' + run_name + '.hdf5'\n",
    "        \n",
    "        try: \n",
    "            print('load model: ' + str(filepath))\n",
    "            model.load_weights(filepath)\n",
    "        except OSError: \n",
    "            print('no model found')\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n",
    "        #Loading model weights\n",
    "        model.load_weights(filepath)\n",
    "        print('Predicting using the best model/epoch so far....')\n",
    "        y_pred = model.predict(x_test,batch_size=1024,verbose=1)\n",
    "\n",
    "        y_pred.shape\n",
    "\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "        submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "        import time\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./NewRnnPreds/' + run_name + '_' + str(sub_id) + '.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_92 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_92 (Embedding)        (None, 150, 300)     30000000    input_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_92 (SpatialDr (None, 150, 300)     0           embedding_92[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_92 (Bidirectional (None, 150, 256)     329472      spatial_dropout1d_92[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 148, 64)      49216       bidirectional_92[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_92 (Gl (None, 64)           0           conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_92 (Global (None, 64)           0           conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_92 (Concatenate)    (None, 128)          0           global_average_pooling1d_92[0][0]\n",
      "                                                                 global_max_pooling1d_92[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 6)            774         concatenate_92[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 30,379,462\n",
      "Trainable params: 379,462\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_word_dict = {\n",
    "    \"'ass\": 'ass',\n",
    "     \"'shit\": 'shit',\n",
    "     \"'stupid\": 'stupid',\n",
    "     'asse': 'ass',\n",
    "     'asspie': 'ass',\n",
    "     \"bitch'\": 'bitch',\n",
    "     'bitchbot': 'bitch',\n",
    "     'bith': 'bitch',\n",
    "     'bithc': 'bitch',\n",
    "     'bithces': 'bitch',\n",
    "     'choked': 'choke',\n",
    "     'cocain': 'cocaine',\n",
    "     'cuck': 'cock',\n",
    "     'cucks': 'cocks',\n",
    "     'decease': 'deceased',\n",
    "     'deth': 'death',\n",
    "     'diedres': 'crap',\n",
    "     'dik': 'dick',\n",
    "     'donkeysex': 'dick',\n",
    "     'faggt': 'faggot',\n",
    "     \"fool'\": 'fool',\n",
    "     \"fuck'\": 'fuck',\n",
    "     'fuckerucker': 'fucker',\n",
    "     'fuckn': 'fuck',\n",
    "     'fuock': 'fuck',\n",
    "     'gayy': 'gay',\n",
    "     'headsdick': 'dick',\n",
    "     'homopetersymonds': 'homo',\n",
    "     'horsecock': 'cock',\n",
    "     'maoth': 'mouth',\n",
    "     \"mother's\": 'mother',\n",
    "     'motherfuc': 'motherfucker',\n",
    "     'mothjer': 'mother',\n",
    "     'niggetr': 'nigger',\n",
    "     'niggors': 'nigger',\n",
    "     'nonense': 'nonsense',\n",
    "     'nonsesnse': 'nonsense',\n",
    "     'peenus': 'penis',\n",
    "     'peni': 'penis',\n",
    "     \"penis'\": 'penis',\n",
    "     'pneis': 'penis',\n",
    "     'pornn': 'porn',\n",
    "     'semite': 'semitic',\n",
    "     'sexe': 'sex',\n",
    "     'sexsex': 'sex',\n",
    "     'sexualit': 'sexuality',\n",
    "     'sexuall': 'sexual',\n",
    "     'sockpuppet': 'alias',\n",
    "     'sockpuppetry': 'alias',\n",
    "     'vagpenis': 'penis',\n",
    "     'valentin': 'valentine',\n",
    "     'youfuck': 'fuck',\n",
    "     'zdick': 'dick'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
