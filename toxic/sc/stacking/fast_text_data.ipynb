{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastText import load_model\n",
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"\n",
    "    Given a text, cleans and normalizes it. Feel free to add your own stuff.\n",
    "    \"\"\"\n",
    "    #s = s.lower()\n",
    "    # Replace ips\n",
    "    #s = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', ' _ip_ ', s)\n",
    "    # Isolate punctuation\n",
    "    #s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    #s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Replace numbers and symbols with language\n",
    "    s = s.replace('&', ' and ')\n",
    "    s = s.replace('@', ' at ')\n",
    "    s = s.replace('0', ' zero ')\n",
    "    s = s.replace('1', ' one ')\n",
    "    s = s.replace('2', ' two ')\n",
    "    s = s.replace('3', ' three ')\n",
    "    s = s.replace('4', ' four ')\n",
    "    s = s.replace('5', ' five ')\n",
    "    s = s.replace('6', ' six ')\n",
    "    s = s.replace('7', ' seven ')\n",
    "    s = s.replace('8', ' eight ')\n",
    "    s = s.replace('9', ' nine ')\n",
    "    return s\n",
    "\n",
    "def text_to_vector(text, window_length, n_features, ft_model):\n",
    "    \"\"\"\n",
    "    Given a string, normalizes it, then splits it into words and finally converts\n",
    "    it to a sequence of word vectors.\n",
    "    \"\"\"\n",
    "    text = normalize(text)\n",
    "    words = text.split()\n",
    "    window = words[-window_length:]\n",
    "    \n",
    "    x = np.zeros((window_length, n_features))\n",
    "\n",
    "    for i, word in enumerate(window):\n",
    "        x[i, :] = ft_model.get_word_vector(word).astype('float32')\n",
    "\n",
    "    return x\n",
    "\n",
    "def df_to_data(df, window_length, n_features, ft_model):\n",
    "    \"\"\"\n",
    "    Convert a given dataframe to a dataset of inputs for the NN.\n",
    "    \"\"\"\n",
    "    x = np.zeros((len(df), window_length, n_features), dtype='float32')\n",
    "\n",
    "    for i, comment in enumerate(df['comment_text_cleaned'].values):\n",
    "        x[i, :] = text_to_vector(comment, window_length, n_features, ft_model)\n",
    "\n",
    "    return x\n",
    "\n",
    "def fasttext_data_process(window_length=200, shuffle_train=False, val_ratio=0, first_n_entries=-1):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        window_length: (int, 200 to 450) the max number of words you choose in a text\n",
    "        shuffe_train: (boolean) whether you want to shuffle training data\n",
    "        val_raio: (float, 0 to 1.0) the validation set portion sepereated from training set\n",
    "        first_n_entries: (int -1 to max length of training data) only choose the first n entries\n",
    "                        of the training and testing data, mainly for test purpose\n",
    "                                \n",
    "    Returns:\n",
    "        x_train, y_train, x_test, x_val, y_val\n",
    "    \"\"\"\n",
    "    print('\\nLoading data')\n",
    "    train = pd.read_csv('/home/kai/data/haoyan/ToxicClassificationCopy3/data/cleaned_train.csv')\n",
    "    test = pd.read_csv('/home/kai/data/haoyan/ToxicClassificationCopy3/data/cleaned_test.csv')\n",
    "    if first_n_entries != -1:\n",
    "        max_value = min(len(train), len(test))\n",
    "        if first_n_entries < max_value:\n",
    "            train = train[:first_n_entries]\n",
    "            test = test[:first_n_entries]\n",
    "        else:\n",
    "            raise ValueError('max value of first_n_entries is ' + str(max_value))\n",
    "    \n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    print('train shape: {}. test shape: {}'.format(train.shape, test.shape))\n",
    "    \n",
    "    print('\\nLoading FT model')\n",
    "    ft_model = load_model('/home/kai/data/resources/FastText/wiki.en.bin')\n",
    "    n_features = ft_model.get_dimension()\n",
    "\n",
    "    print(n_features)\n",
    "    #window_length = 200 # The amount of words we look at per example. Experiment with this.\n",
    "    print('window: {}. dimension(n_features): {}'.format(window_length, n_features))\n",
    "    \n",
    "    split_index = len(train)\n",
    "    if val_ratio > 0:\n",
    "        split_index = round(split_index * (1-val_ratio)) \n",
    "    if shuffle_train:\n",
    "        train = train.sample(frac=1)\n",
    "    \n",
    "    df_train = train.iloc[:split_index]\n",
    "    x_train = df_to_data(df_train, window_length, n_features, ft_model)\n",
    "    y_train = df_train[label_cols].values\n",
    "    \n",
    "    df_val = train.iloc[split_index:]\n",
    "    # Convert validation set to fixed array\n",
    "    x_val = df_to_data(df_val, window_length, n_features, ft_model)\n",
    "    y_val = df_val[label_cols].values\n",
    "    # Get test data ready\n",
    "    x_test = df_to_data(test, window_length, n_features, ft_model)\n",
    "    \n",
    "    del ft_model, train, test\n",
    "    \n",
    "    return x_train, y_train, x_test, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, _, x_test, _, _ = fasttext_data_process(first_n_entries=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
