{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from base_layer_utils import BaseLayerDataRepo, ModelName\n",
    "from base_layer_utils import LightgbmBLE, OneVSOneRegBLE, SklearnBLE, NbSvmBLE, XgbBLE, RnnBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fast_text_data import fasttext_data_process\n",
    "from onevsone_data import onevsone_data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=True)\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold # replace with model_selection?\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "#train = pd.read_csv('/home/kai/data/wei/Toxic/data/Shiyi_training.csv').fillna('na')\n",
    "\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "#train = train.head(1000)\n",
    "#test = test.head(1000)\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "text = pd.concat([train_sentence, test_sentence])\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bldr = BaseLayerDataRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    }
   ],
   "source": [
    "x_train_1v1, y_train_1v1, x_test_1v1, data_id_1v1 = onevsone_data_process() # should not be named as 1v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compatible_models= [ModelName.NBLSVC, ModelName.NBSVM]\n",
    "bldr.add_data(data_id_1v1, x_train_1v1, x_test_1v1, y_train_1v1, label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real \n",
      "\tx_train: (159571, 300000)\tx_test: (153164, 300000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBSVM: 4>, <ModelName.NBLSVC: 7>}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n",
      "train shape: (159571, 27). test shape: (153164, 21)\n",
      "\n",
      "Loading FT model\n",
      "300\n",
      "window: 200. dimension(n_features): 300\n"
     ]
    }
   ],
   "source": [
    "x_train_rnn, y_train_rnn, x_test_rnn, _, _ = fasttext_data_process()#first_n_entries=100)\n",
    "\n",
    "rnn_data_id = 'rnn_data_001'\n",
    "compatible_models= [ModelName.RNN]\n",
    "bldr.add_data(rnn_data_id, x_train_rnn, x_test_rnn, y_train_rnn, label_cols, compatible_models, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: rnn_data_001         \n",
      "\tx_train: (159571, 200, 300)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 6>} \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bldr = BaseLayerDataRepo()\n",
    "\n",
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,1),(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        #min_df = i\n",
    "        #word_ngram_range = (1,1)\n",
    "        #char_ngram_range = (1,5)\n",
    "        word_max_features = 200000\n",
    "        #char_max_features = 100000\n",
    "        token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "        data_id = 'tfidf_word_df%d_ng%s_wmf%s'%(min_df,str(word_ngram_range),str(word_max_features))\n",
    "\n",
    "        word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                  min_df=1,\n",
    "                                  ngram_range=word_ngram_range,\n",
    "                                  max_features=word_max_features,\n",
    "                                  token_pattern=token_pattern,\n",
    "                                  stop_words='english',\n",
    "                                  strip_accents='unicode',\n",
    "                                  sublinear_tf=True)\n",
    "        \n",
    "        train_term_doc = word_vec.fit_transform(train.comment_text)\n",
    "        test_term_doc = word_vec.transform(test.comment_text)\n",
    "        #pdb.set_trace()\n",
    "        #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_doc)\n",
    "        #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_doc)\n",
    "    \n",
    "        compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "        bldr.add_data(data_id, train_term_doc, test_term_doc, train[label_cols], label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        for char_max_df in [0.3]:\n",
    "            #min_df = i\n",
    "            #word_ngram_range = (1,1)\n",
    "            #char_ngram_range = (1,5)\n",
    "            word_max_features = 100000\n",
    "            char_max_features = 100000\n",
    "            token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "            data_id = 'tfidf_wordchar_charmaxdf%f_ng%s_wmf%s_cmf%s'%(char_max_df,str(word_ngram_range),str(word_max_features),str(char_max_features))\n",
    "\n",
    "            word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                      min_df=1,\n",
    "                                      ngram_range=word_ngram_range,\n",
    "                                      max_features=word_max_features,\n",
    "                                      token_pattern=token_pattern,\n",
    "                                      stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "\n",
    "            char_vec = TfidfVectorizer(analyzer='char',\n",
    "                                      min_df = 1,\n",
    "                                      max_df = char_max_df,\n",
    "                                      ngram_range=(2,7), \n",
    "                                      max_features=char_max_features, \n",
    "                                      #stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "            train_word_doc = word_vec.fit_transform(train.comment_text)\n",
    "            test_word_doc = word_vec.transform(test.comment_text)\n",
    "\n",
    "            train_char_doc = char_vec.fit_transform(train.comment_text)\n",
    "            test_char_doc = char_vec.transform(test.comment_text)\n",
    "\n",
    "            train_term_tfidf = hstack((train_word_doc, train_char_doc), format='csr')\n",
    "            test_term_tfidf = hstack((test_word_doc, test_char_doc), format='csr')\n",
    "\n",
    "            #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_tfidf)\n",
    "            #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_tfidf)\n",
    "\n",
    "            compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "            bldr.add_data(data_id, train_term_tfidf, test_term_tfidf, train[label_cols], label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(data_id_list_file, 'wb') as f:\n",
    "    pickle.dump(data_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(data_id_list_file, 'rb') as f:\n",
    "    data_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for data_id in data_ids:\n",
    "    bldr.add_data(data_id, np.load(DATA_PATH+data_id+'_x_train.npy'), np.load(DATA_PATH+data_id+'_x_test.npy'), train[label_cols], label_cols, ['logreg','gbm','rf','nbsvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: rnn_data_001         \n",
      "\tx_train: (159571, 200, 300)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 6>} data_id: tfidf_word_df2_ng(1, 2)_wmf200000 \n",
      "\tx_train: (159571, 200000)\tx_test: (153164, 200000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} data_id: tfidf_word_df2_ng(1, 1)_wmf200000 \n",
      "\tx_train: (159571, 184719)\tx_test: (153164, 184719)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} data_id: tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 \n",
      "\tx_train: (159571, 200000)\tx_test: (153164, 200000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>}\n",
      "(159571, 200000)\n",
      "{<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>}\n",
      "(159571, 184719)\n",
      "{<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>}\n",
      "(159571, 200000)\n"
     ]
    }
   ],
   "source": [
    "for data in bldr.get_data_by_compatible_model(ModelName.LGB):\n",
    "    print(data['compatible_model'])\n",
    "    print(data['x_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test, nfolds):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "    \n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_1v1(clf, x_train, y_train, x_test, nfolds, label):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr, label)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te, label)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test, label)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_rnn(clf, x_train, y_train, x_test, nfolds, number_labels):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,number_labels))\n",
    "    oof_test = np.zeros((ntest,number_labels))\n",
    "    oof_test_skf = np.empty((nfolds, ntest, number_labels))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        ################################################################ maybe shuffle train_index\n",
    "        x_tr = x_train[train_index]\n",
    "        #pdb.set_trace()\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_base_layer_est_preds(base_layer_est_preds):\n",
    "    for key in base_layer_est_preds:\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "        submission[label_cols] = base_layer_est_preds[key]\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./BaseEstPreds/' + key + '_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_rnn.shape[1], x_train_rnn.shape[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_ble = RnnBLE(x_train_rnn.shape[1], x_train_rnn.shape[2], label_cols, epochs=2)\n",
    "rnn_model_pool[ModelName.RNN] = rnn_ble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start generating oof_train, oof_test, baselayer estimater prediction and model data id list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.RNN rnn_data_001\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1556s 13ms/step - loss: 0.0507 - acc: 0.9814\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1552s 13ms/step - loss: 0.0440 - acc: 0.9832\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1618s 14ms/step - loss: 0.0412 - acc: 0.9838\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1600s 13ms/step - loss: 0.0387 - acc: 0.9846\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1570s 13ms/step - loss: 0.0383 - acc: 0.9848\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1569s 13ms/step - loss: 0.0362 - acc: 0.9853\n",
      "Epoch 1/2\n",
      "119679/119679 [==============================] - 1569s 13ms/step - loss: 0.0355 - acc: 0.9857\n",
      "Epoch 2/2\n",
      "119679/119679 [==============================] - 1565s 13ms/step - loss: 0.0337 - acc: 0.9862\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 2131s 13ms/step - loss: 0.0333 - acc: 0.9864\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 2092s 13ms/step - loss: 0.0321 - acc: 0.9869\n"
     ]
    }
   ],
   "source": [
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for model_name in rnn_model_pool.keys():\n",
    "    for data in bldr.get_data_by_compatible_model(model_name):\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "\n",
    "        SEED = 0 # for reproducibility\n",
    "        NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "        #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "\n",
    "        current_run = '{} {}'.format(model_name,data['data_id'])\n",
    "        print('Generating: '+current_run)\n",
    "\n",
    "        oof_train, oof_test = get_oof_rnn(rnn_model_pool[model_name], \\\n",
    "                                          x_train, y_train, x_test, NFOLDS, len(label_cols))\n",
    "        for i, label in enumerate(label_cols):\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train[:, i].reshape(-1,1)) # before reshape: (159571,) after: (159571, 1) => good for np.concatenate\n",
    "            layer1_oof_test[label].append(oof_test[:, i].reshape(-1,1))\n",
    "\n",
    "        model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "        model_data_id_list.append(model_data_id)\n",
    "        model = rnn_model_pool[model_name]\n",
    "        model.train(x_train, y_train) ################################ maybe shuffle x_train along with y_train?\n",
    "        est_preds = model.predict(x_test)\n",
    "\n",
    "        base_layer_est_preds[model_data_id] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519810384\n"
     ]
    }
   ],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_est_preds['ModelName.RNN_rnn_data_001'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1VS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsOne is using svc kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using svc kernel\n",
      "OneVsOne is using logistic kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using logistic kernel\n"
     ]
    }
   ],
   "source": [
    "onevsone_svc = OneVSOneReg(x_train_1v1, y_train_1v1, model='svc')\n",
    "onevsone_logreg = OneVSOneReg(x_train_1v1, y_train_1v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "model_pool[ModelName.ONESVC] = onevsone_svc\n",
    "model_pool[ModelName.ONELOGREG] = onevsone_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "########################## ONE VS ONE ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "            if model_name == ModelName.LGB:\n",
    "                model = LogisticRegression(solver='sag')\n",
    "                sfm = SelectFromModel(model, threshold='5*mean')\n",
    "                print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "                x_train = sfm.fit_transform(x_train, y_train)\n",
    "                x_test = sfm.transform(x_test)\n",
    "                print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof_1v1(model_pool[model_name],  x_train, y_train, x_test, NFOLDS, label) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train, label)\n",
    "            est_preds = model.predict(x_test, label)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# logreg_params = {\n",
    "#     'n_jobs': 3\n",
    "# }\n",
    "# logreg_ble = SklearnBLE(LogisticRegression, seed=SEED, params=logreg_params)\n",
    "# model_pool[ModelName.LOGREG] = logreg_ble\n",
    "\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={'n_jobs': 3})\n",
    "#model_pool[ModelName.RF] = rf_ble\n",
    "\n",
    "\n",
    "nblsvc_params = {\n",
    "    'C':0.02\n",
    "}\n",
    "nblsvc_ble = NbSvmBLE(mode=ModelName.NBLSVC, seed=SEED, params=nblsvc_params)\n",
    "model_pool[ModelName.NBLSVC] = nblsvc_ble\n",
    "\n",
    "\n",
    "nbsvm_params = {\n",
    "    'C':0.25,\n",
    "    #'dual':True,\n",
    "    'n_jobs':5\n",
    "}\n",
    "nbsvm_ble = NbSvmBLE(mode=ModelName.NBSVM, seed=SEED, params=nbsvm_params)\n",
    "model_pool[ModelName.NBSVM] = nbsvm_ble\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={})\n",
    "#xgb_ble = XgbBLE(params=xgb_params)\n",
    "\n",
    "# lgb_params = {\n",
    "#     'learning_rate': 0.2,\n",
    "#     'application': 'binary',\n",
    "#     'num_leaves': 31,\n",
    "#     'verbosity': -1,\n",
    "#     'metric': 'auc',\n",
    "#     'data_random_seed': 2,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'feature_fraction': 0.6,\n",
    "#     'nthread': 8,\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 1\n",
    "# }\n",
    "# lgb_ble = LightgbmBLE(params=lgb_params)\n",
    "# model_pool[ModelName.LGB] = lgb_ble\n",
    "\n",
    "\n",
    "# lg = SklearnBLE(clf=LogisticRegression, seed=SEED, params={'n_jobs': 1})\n",
    "\n",
    "# et = SklearnBLE(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "# ada = SklearnBLE(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "# gb = SklearnBLE(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "# svc = SklearnBLE(clf=SVC, seed=SEED, params=svc_params)\n",
    "\n",
    "#model_pool['rf'] = rf_ble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.NBSVM wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 5.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.NBLSVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Generating: ModelName.NBSVM wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 5.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.NBLSVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n"
     ]
    }
   ],
   "source": [
    "########################## NORMAL MODELS ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "            if model_name == ModelName.LGB:\n",
    "                model = LogisticRegression(solver='sag')\n",
    "                sfm = SelectFromModel(model, threshold='5*mean')\n",
    "                print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "                x_train = sfm.fit_transform(x_train, y_train)\n",
    "                x_test = sfm.transform(x_test)\n",
    "                print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof(model_pool[model_name],  x_train, y_train, x_test, NFOLDS) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train)\n",
    "            est_preds = model.predict(x_test)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(layer1_oof_train) # list keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(layer1_oof_train['toxic']) # number of models to stack (each model will predict one set of toxic, servere_toxic, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(layer1_oof_train['toxic'][0]) # examples in oof_train (meta features, x_train) (meta labels are in train[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(layer1_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(layer1_oof_test['toxic'][0]) # examples in oof_test (will be used by meta model (after validation) to predict the final prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before we choose which models to assemble, we can do:\n",
    "#### 1. scatter plot analysis to check the diversity\n",
    "#### 2. submit to check if the models have similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_layer_oof_per_label(layer1_oof_dict, label):\n",
    "    x = None\n",
    "    data_list = layer1_oof_dict[label]\n",
    "    for i in range(len(data_list)):\n",
    "        if i == 0:\n",
    "            x = data_list[0]\n",
    "        else:\n",
    "            x = np.concatenate((x, data_list[i]), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simple blend of two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "result = np.empty((test.shape[0],len(label_cols)))\n",
    "\n",
    "# mix the first two models\n",
    "for i, label in enumerate(label_cols):\n",
    "    x_train = combine_layer_oof_per_label(layer1_oof_train, label)\n",
    "    x_test = combine_layer_oof_per_label(layer1_oof_test, label)\n",
    "    for j in range(x_train.shape[1]):\n",
    "        roc = roc_auc_score(train[label], x_train[:,j])\n",
    "        print(label, j, roc) # print out roc for meta feature on meta label (which is just the original train label)\n",
    "    \n",
    "    roc_scores_of_a_label = []\n",
    "    alphas = np.linspace(0,1,1001)\n",
    "    best_roc = 0\n",
    "    best_alpha = 0\n",
    "    for alpha in alphas:\n",
    "        roc = roc_auc_score(train[label], alpha*x_train[:,0] + (1-alpha)*x_train[:,1])\n",
    "        if roc > best_roc:\n",
    "            best_roc = roc\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(label, best_roc, best_alpha)\n",
    "    result[:,i] = best_alpha*x_test[:,0] + (1-best_alpha)*x_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "submission[label_cols] = result\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./StackPreds/mixtwo_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insult', 'toxic', 'obscene', 'threat', 'severe_toxic', 'identity_hate']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()#load_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
       " 'ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo.add(layer1_oof_train, layer1_oof_test, base_layer_est_preds, model_data_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_threshold = np.random.choice([0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0 update to 0.9818\n"
     ]
    }
   ],
   "source": [
    "#base_layer_results_repo.add_score('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.9818)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_layer_results_repo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=0.9774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_train_loaded['toxic']) # number of models that will be stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(layer1_oof_train_temp[label_cols[5]]) == len(layer1_oof_test_temp[label_cols[5]]) == len(list(base_layer_est_preds_temp)) == 1\n",
    "                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 27)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40881912"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time.time()* 1000000 % 45234634)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('./xgb_search.csv', 'a')\n",
    "header = 'time,id,threshold,num_models,colsample_bytree,lr,max_depth,subsample,\\\n",
    "gamma,alpha,cv_num_round,cv_nfolds,toxic_best_round,severe_toxic_best_round,\\\n",
    "obscene_best_round,threat_best_round,insult_best_round,identity_hate_best_round,\\\n",
    "toxic_auc,severe_toxic_auc,obscene_auc,threat_auc,insult_auc,identity_hate_auc,avg_auc\\n'\n",
    "f.write(header)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-212-a449875801c1>(9)<module>()\n",
      "-> model_threshold = np.random.choice([0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
      "(Pdb) n\n",
      "> <ipython-input-212-a449875801c1>(10)<module>()\n",
      "-> layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
      "(Pdb) unt 19\n",
      "> <ipython-input-212-a449875801c1>(20)<module>()\n",
      "-> xgb_cv_seed = 0\n",
      "(Pdb) model_threshold\n",
      "0.9768\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-a449875801c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mxgb_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mxgb_cv_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mxgb_cv_num_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mxgb_cv_nfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-a449875801c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mxgb_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mxgb_cv_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mxgb_cv_num_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mxgb_cv_nfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import datetime, time, gc\n",
    "\n",
    "for i in range(100):\n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    model_threshold = np.random.choice([0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "        \n",
    "    xgb_colsample_bytree = np.random.randint(5, 10)/10\n",
    "    xgb_learning_rate = 1e-2 * (0.1 ** (np.random.rand() * 2 - 1.0)) # 0.001 to 0.0997\n",
    "    xgb_max_depth = np.random.randint(2, 8)\n",
    "    xgb_subsample = np.random.randint(50, 100)/100\n",
    "    xgb_gamma = np.random.randint(0, 3)\n",
    "    xgb_alpha = np.random.randint(0, 2)\n",
    "\n",
    "    xgb_cv_seed = 0\n",
    "    xgb_cv_num_round = np.random.randint(400, 1000)\n",
    "    xgb_cv_nfolds = np.random.randint(3,5)\n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    num_models = len(layer1_oof_train_loaded['toxic'])\n",
    "    #print('Stacking {} models'.format(num_models)) # number of models that will be stacked\n",
    "    \n",
    "    search_id = int(time.time())\n",
    "\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f,\\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds))\n",
    "    \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {} # all labels\n",
    "    best_rounds = {}  # all labels\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "    #     clf = XGBClassifier()\n",
    "\n",
    "    #     #scores = cross_val_score(clf, x_train, train[label], cv=3, scoring='roc_auc')\n",
    "\n",
    "    #     #print(scores)\n",
    "    #     #print(\"Stacking-CV: ROC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    #     clf.fit(x_train, train[label])\n",
    "\n",
    "    #     result[:, i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds_for_current_label = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "\n",
    "        #print('Ensemble-CV: {}: {}+{}'.format(label, cv_mean, cv_std))\n",
    "        metric_dict[label] = cv_mean\n",
    "        best_rounds[label] = best_nrounds_for_current_label\n",
    "        #metric_dict[label]['cv_mean'] = cv_mean\n",
    "        #metric_dict[label]['cv_std'] = cv_std\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds_for_current_label)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "          \n",
    "    res = '%s,%d,%f,%d,%f,%.7f,%d,%f,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f\\n\\\n",
    "            '%(now,search_id,model_threshold,num_models,xgb_colsample_bytree,\\\n",
    "               xgb_learning_rate,xgb_max_depth,xgb_subsample,xgb_gamma,xgb_alpha,\\\n",
    "               xgb_cv_num_round,xgb_cv_nfolds,best_rounds['toxic'],best_rounds['severe_toxic'],\\\n",
    "               best_rounds['obscene'],best_rounds['threat'],best_rounds['insult'],\\\n",
    "               best_rounds['identity_hate'],metric_dict['toxic'],metric_dict['severe_toxic'],\\\n",
    "               metric_dict['obscene'],metric_dict['threat'],metric_dict['insult'],\\\n",
    "               metric_dict['identity_hate'],avg_auc)\n",
    "\n",
    "    f = open('./xgb_search.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()\n",
    "\n",
    "#     sub_tile = 'stacking_test_'\n",
    "#     submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "#     submission[label_cols] = result\n",
    "#     submission.to_csv('./StackPreds/' + sub_tile + str(search_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb random search top N training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from base_layer_results_repo import BaseLayerResultsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ModelName.RNN_rnn_data_001', 0.9825),\n",
       " ('ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', 0.9819),\n",
       " ('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
       "  0.9818),\n",
       " ('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9815),\n",
       " ('ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9803),\n",
       " ('ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9794),\n",
       " ('ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9793),\n",
       " ('ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w', 0.9786),\n",
       " ('ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9774),\n",
       " ('ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9768),\n",
       " ('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765),\n",
       " ('ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9761),\n",
       " ('ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000', 0.976),\n",
       " ('ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9752),\n",
       " ('ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9726),\n",
       " ('ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9723)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_search = pd.read_csv('xgb_search_ori100.csv').sort_values(by='avg_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>cv_num_round</th>\n",
       "      <th>cv_nfolds</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 10:23:13</td>\n",
       "      <td>1519899793</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.092125</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987677</td>\n",
       "      <td>0.991848</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.994002</td>\n",
       "      <td>0.990178</td>\n",
       "      <td>0.991174</td>\n",
       "      <td>0.991723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 06:45:20</td>\n",
       "      <td>1519886720</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.097664</td>\n",
       "      <td>4</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987663</td>\n",
       "      <td>0.991693</td>\n",
       "      <td>0.995412</td>\n",
       "      <td>0.993793</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.991636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 08:14:47</td>\n",
       "      <td>1519892087</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.098587</td>\n",
       "      <td>3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>809</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987629</td>\n",
       "      <td>0.991796</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>0.991218</td>\n",
       "      <td>0.991635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time          id  threshold  num_models  colsample_bytree  \\\n",
       "0  2018-03-01 10:23:13  1519899793     0.9768          10               0.7   \n",
       "1  2018-03-01 06:45:20  1519886720     0.9794           6               0.8   \n",
       "2  2018-03-01 08:14:47  1519892087     0.9794           6               0.5   \n",
       "\n",
       "         lr  max_depth  subsample  gamma  alpha  cv_num_round  cv_nfolds  \\\n",
       "0  0.092125          2       0.85      2      0           962          4   \n",
       "1  0.097664          4       0.68      1      0           995          3   \n",
       "2  0.098587          3       0.77      1      0           809          4   \n",
       "\n",
       "   toxic_auc  severe_toxic_auc  obscene_auc  threat_auc  insult_auc  \\\n",
       "0   0.987677          0.991848     0.995462    0.994002    0.990178   \n",
       "1   0.987663          0.991693     0.995412    0.993793    0.990062   \n",
       "2   0.987629          0.991796     0.995422    0.993610    0.990133   \n",
       "\n",
       "   identity_hate_auc   avg_auc  \n",
       "0           0.991174  0.991723  \n",
       "1           0.991191  0.991636  \n",
       "2           0.991218  0.991635  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-01 21:12:32, id: 1519899793, th: 0.976800, num_models: 10, colsample_bytree: 0.700000, lr: 0.0921252,    max_depth: 2, subsample: 0.850000, gamma: 2, alpha: 0, cv_num_round: 962, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519899793). toxic: \t cv_mean:0.9876475 \t cv_mean_fromcsv 0.98767725 \t best nrounds: 222        \n",
      "XGB top N training (id: 1519899793). severe_toxic: \t cv_mean:0.99171125 \t cv_mean_fromcsv 0.991848 \t best nrounds: 85        \n",
      "XGB top N training (id: 1519899793). obscene: \t cv_mean:0.995424 \t cv_mean_fromcsv 0.9954620000000001 \t best nrounds: 129        \n",
      "XGB top N training (id: 1519899793). threat: \t cv_mean:0.99372925 \t cv_mean_fromcsv 0.994002 \t best nrounds: 63        \n",
      "XGB top N training (id: 1519899793). insult: \t cv_mean:0.9901455 \t cv_mean_fromcsv 0.99017775 \t best nrounds: 65        \n",
      "XGB top N training (id: 1519899793). identity_hate: \t cv_mean:0.99099575 \t cv_mean_fromcsv 0.9911735 \t best nrounds: 56        \n",
      "XGB top N training. avg_auc:0.991608875 \t avg_auc_fromcsv 0.99172342\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sub_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6d55d050df37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.head(1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./StackPreds/TopN_XGB/scaled_{}_{}_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearch_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_dict_fromcsv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg_auc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub_title' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "            \n",
    "#     best_rounds = {}\n",
    "#     best_rounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "#     best_rounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "#     best_rounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "#     best_rounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "#     best_rounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "#     best_rounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = 10\n",
    "    scale_pos_weights['severe_toxic'] = 100\n",
    "    scale_pos_weights['obscene'] = 18\n",
    "    scale_pos_weights['threat'] = 333\n",
    "    scale_pos_weights['insult'] = 20\n",
    "    scale_pos_weights['identity_hate'] = 112\n",
    "    \n",
    "    metric_dict_fromcsv = {}\n",
    "    metric_dict_fromcsv['toxic'] = xgb_search['toxic_auc'].values[i]\n",
    "    metric_dict_fromcsv['severe_toxic'] = xgb_search['severe_toxic_auc'].values[i]\n",
    "    metric_dict_fromcsv['obscene'] = xgb_search['obscene_auc'].values[i]\n",
    "    metric_dict_fromcsv['threat'] = xgb_search['threat_auc'].values[i]\n",
    "    metric_dict_fromcsv['insult'] = xgb_search['insult_auc'].values[i]\n",
    "    metric_dict_fromcsv['identity_hate'] = xgb_search['identity_hate_auc'].values[i]\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "\n",
    "    \n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f,\\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d\\\n",
    "        '%(now,search_id,model_threshold,num_models,\\\n",
    "          xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "          xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "          xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds))\n",
    "        \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {}\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "        \n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #########################     best_nrounds    #########################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "\n",
    "        \n",
    "        metric_dict[label] = cv_mean\n",
    "        print('XGB top N training (id: {}). {}: \\t cv_mean:{} \\t cv_mean_fromcsv {} \\t best nrounds: {}\\\n",
    "        '.format(search_id, label, cv_mean, metric_dict_fromcsv[label], best_nrounds))\n",
    "        \n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "    \n",
    "    print('XGB top N training. avg_auc:{} \\t avg_auc_fromcsv {}'.format(avg_auc, metric_dict_fromcsv['avg_auc']))\n",
    "\n",
    "\n",
    "    sub_title = 'xgb_topn_'\n",
    "    submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "    submission[label_cols] = result\n",
    "    submission.to_csv('./StackPreds/TopN_XGB/scaled_{}_{}_{}.csv'.format(sub_title,search_id,metric_dict_fromcsv['avg_auc']), index=False)\n",
    "        \n",
    "        \n",
    "#     val_auc = para['val_auc'].values[i]\n",
    "#     print('Model training done. Validation AUC: %.5f'%val_auc)\n",
    "\n",
    "    \n",
    "#     test_flow = dataGenerator.flow(test_embeddings + test_genre, [test_context], \\\n",
    "#             batch_size=16384, shuffle=False)\n",
    "#     test_pred = model.predict_generator(test_flow, test_flow.__len__(), workers=1)\n",
    "    \n",
    "#     test_sub = pd.DataFrame({'id': test_id, 'target': test_pred.ravel()})\n",
    "#     test_sub.to_csv('./temp_nn/nn_%.5f_%.5f_%d.csv'%(val_auc, train_loss, flag), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "preds_mats = []\n",
    "for label_idx, label in enumerate(label_cols):\n",
    "    preds_list = []\n",
    "    print(label)\n",
    "    for i, (key, value) in enumerate(base_layer_est_preds_loaded.items()):\n",
    "        preds_list.append(value[:,label_idx].reshape(-1,1))\n",
    "    preds_mats.append(np.hstack(preds_list))\n",
    "    \n",
    "    \n",
    "assert len(preds_mats) == len(label_cols)\n",
    "assert preds_mats[0].shape[1] == (len(base_layer_est_preds_loaded))\n",
    "\n",
    "# analyze the first label in scatter matrix\n",
    "temp = pd.DataFrame(preds_mats[0], columns=[key.split('_')[0] + re.sub('\\D','',key) for key in base_layer_est_preds_loaded.keys()])\n",
    "print(temp.head(3))\n",
    "\n",
    "scatter_matrix(temp, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparamstune(clf, x, y, label_cols, cv=5, scoring='roc_auc'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model_data_id_list = ['ModelName.RNN_rnn_data_001',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9774)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9760)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9723)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9752)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9726)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9761)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9803)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9793)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9794)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
