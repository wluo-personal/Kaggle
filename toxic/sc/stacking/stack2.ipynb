{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from base_layer_utils import BaseLayerDataRepo, BaseLayerResultsRepo, ModelName\n",
    "from base_layer_utils import LightgbmBLE, OneVSOneRegBLE, SklearnBLE, NbSvmBLE, XGBoostBLE, RnnBLE\n",
    "\n",
    "from fast_text_data import fasttext_data_process\n",
    "from tfidf_data import tfidf_data_process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold # replace with model_selection?\n",
    "import time\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=True)\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "#train = pd.read_csv('/home/kai/data/wei/Toxic/data/Shiyi_training.csv').fillna('na')\n",
    "\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "#train = train.head(1000)\n",
    "#test = test.head(1000)\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "text = pd.concat([train_sentence, test_sentence])\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bldr = BaseLayerDataRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, data_id = tfidf_data_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compatible_models= [ModelName.NBLSVC, ModelName.NBSVM]\n",
    "bldr.add_data(data_id, x_train, y_train, x_test, label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real \n",
      "\tx_train: (159571, 300000)\tx_test: (153164, 300000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBSVM: 4>, <ModelName.NBLSVC: 7>}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n",
      "train shape: (159571, 27). test shape: (153164, 21)\n",
      "\n",
      "Loading FT model\n",
      "300\n",
      "window: 200. dimension(n_features): 300\n"
     ]
    }
   ],
   "source": [
    "x_train_rnn, y_train_rnn, x_test_rnn, _, _ = fasttext_data_process()#first_n_entries=100)\n",
    "\n",
    "rnn_data_id = 'rnn_data_001'\n",
    "compatible_models= [ModelName.RNN]\n",
    "bldr.add_data(rnn_data_id, x_train_rnn, x_test_rnn, y_train_rnn, label_cols, compatible_models, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: rnn_data_001         \n",
      "\tx_train: (159571, 200, 300)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 6>} \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bldr = BaseLayerDataRepo()\n",
    "\n",
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,1),(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        #min_df = i\n",
    "        #word_ngram_range = (1,1)\n",
    "        #char_ngram_range = (1,5)\n",
    "        word_max_features = 200000\n",
    "        #char_max_features = 100000\n",
    "        token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "        data_id = 'tfidf_word_df%d_ng%s_wmf%s'%(min_df,str(word_ngram_range),str(word_max_features))\n",
    "\n",
    "        word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                  min_df=1,\n",
    "                                  ngram_range=word_ngram_range,\n",
    "                                  max_features=word_max_features,\n",
    "                                  token_pattern=token_pattern,\n",
    "                                  stop_words='english',\n",
    "                                  strip_accents='unicode',\n",
    "                                  sublinear_tf=True)\n",
    "        \n",
    "        train_term_doc = word_vec.fit_transform(train.comment_text)\n",
    "        test_term_doc = word_vec.transform(test.comment_text)\n",
    "        #pdb.set_trace()\n",
    "        #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_doc)\n",
    "        #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_doc)\n",
    "    \n",
    "        compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "        bldr.add_data(data_id, train_term_doc, test_term_doc, train[label_cols], label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        for char_max_df in [0.3]:\n",
    "            #min_df = i\n",
    "            #word_ngram_range = (1,1)\n",
    "            #char_ngram_range = (1,5)\n",
    "            word_max_features = 100000\n",
    "            char_max_features = 100000\n",
    "            token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "            data_id = 'tfidf_wordchar_charmaxdf%f_ng%s_wmf%s_cmf%s'%(char_max_df,str(word_ngram_range),str(word_max_features),str(char_max_features))\n",
    "\n",
    "            word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                      min_df=1,\n",
    "                                      ngram_range=word_ngram_range,\n",
    "                                      max_features=word_max_features,\n",
    "                                      token_pattern=token_pattern,\n",
    "                                      stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "\n",
    "            char_vec = TfidfVectorizer(analyzer='char',\n",
    "                                      min_df = 1,\n",
    "                                      max_df = char_max_df,\n",
    "                                      ngram_range=(2,7), \n",
    "                                      max_features=char_max_features, \n",
    "                                      #stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "            train_word_doc = word_vec.fit_transform(train.comment_text)\n",
    "            test_word_doc = word_vec.transform(test.comment_text)\n",
    "\n",
    "            train_char_doc = char_vec.fit_transform(train.comment_text)\n",
    "            test_char_doc = char_vec.transform(test.comment_text)\n",
    "\n",
    "            train_term_tfidf = hstack((train_word_doc, train_char_doc), format='csr')\n",
    "            test_term_tfidf = hstack((test_word_doc, test_char_doc), format='csr')\n",
    "\n",
    "            #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_tfidf)\n",
    "            #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_tfidf)\n",
    "\n",
    "            compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "            bldr.add_data(data_id, train_term_tfidf, test_term_tfidf, train[label_cols], label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(data_id_list_file, 'wb') as f:\n",
    "    pickle.dump(data_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(data_id_list_file, 'rb') as f:\n",
    "    data_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for data_id in data_ids:\n",
    "    bldr.add_data(data_id, np.load(DATA_PATH+data_id+'_x_train.npy'), np.load(DATA_PATH+data_id+'_x_test.npy'), train[label_cols], label_cols, ['logreg','gbm','rf','nbsvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: rnn_data_001         \n",
      "\tx_train: (159571, 200, 300)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 6>} data_id: tfidf_word_df2_ng(1, 2)_wmf200000 \n",
      "\tx_train: (159571, 200000)\tx_test: (153164, 200000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} data_id: tfidf_word_df2_ng(1, 1)_wmf200000 \n",
      "\tx_train: (159571, 184719)\tx_test: (153164, 184719)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} data_id: tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 \n",
      "\tx_train: (159571, 200000)\tx_test: (153164, 200000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bldr.add_compatible_model('wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', ModelName.LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ModelName.NBSVM: 4>, <ModelName.NBLSVC: 7>, <ModelName.LGB: 2>}\n",
      "(159571, 300000)\n"
     ]
    }
   ],
   "source": [
    "for data in bldr.get_data_by_compatible_model(ModelName.LGB):\n",
    "    print(data['compatible_model'])\n",
    "    print(data['x_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test, nfolds):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "    \n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_1v1(clf, x_train, y_train, x_test, nfolds, label):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr, label)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te, label)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test, label)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_rnn(clf, x_train, y_train, x_test, nfolds, number_labels):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,number_labels))\n",
    "    oof_test = np.zeros((ntest,number_labels))\n",
    "    oof_test_skf = np.empty((nfolds, ntest, number_labels))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        ################################################################ maybe shuffle train_index\n",
    "        x_tr = x_train[train_index]\n",
    "        #pdb.set_trace()\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_base_layer_est_preds(base_layer_est_preds):\n",
    "    for key in base_layer_est_preds:\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "        submission[label_cols] = base_layer_est_preds[key]\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./BaseEstPreds/' + key + '_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_rnn.shape[1], x_train_rnn.shape[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_ble = RnnBLE(x_train_rnn.shape[1], x_train_rnn.shape[2], label_cols, epochs=2)\n",
    "rnn_model_pool[ModelName.RNN] = rnn_ble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start generating oof_train, oof_test, baselayer estimater prediction and model data id list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.RNN rnn_data_001\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1556s 13ms/step - loss: 0.0507 - acc: 0.9814\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1552s 13ms/step - loss: 0.0440 - acc: 0.9832\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1618s 14ms/step - loss: 0.0412 - acc: 0.9838\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1600s 13ms/step - loss: 0.0387 - acc: 0.9846\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1570s 13ms/step - loss: 0.0383 - acc: 0.9848\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1569s 13ms/step - loss: 0.0362 - acc: 0.9853\n",
      "Epoch 1/2\n",
      "119679/119679 [==============================] - 1569s 13ms/step - loss: 0.0355 - acc: 0.9857\n",
      "Epoch 2/2\n",
      "119679/119679 [==============================] - 1565s 13ms/step - loss: 0.0337 - acc: 0.9862\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 2131s 13ms/step - loss: 0.0333 - acc: 0.9864\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 2092s 13ms/step - loss: 0.0321 - acc: 0.9869\n"
     ]
    }
   ],
   "source": [
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for model_name in rnn_model_pool.keys():\n",
    "    for data in bldr.get_data_by_compatible_model(model_name):\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "\n",
    "        SEED = 0 # for reproducibility\n",
    "        NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "        #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "\n",
    "        current_run = '{} {}'.format(model_name,data['data_id'])\n",
    "        print('Generating: '+current_run)\n",
    "\n",
    "        oof_train, oof_test = get_oof_rnn(rnn_model_pool[model_name], \\\n",
    "                                          x_train, y_train, x_test, NFOLDS, len(label_cols))\n",
    "        for i, label in enumerate(label_cols):\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train[:, i].reshape(-1,1)) # before reshape: (159571,) after: (159571, 1) => good for np.concatenate\n",
    "            layer1_oof_test[label].append(oof_test[:, i].reshape(-1,1))\n",
    "\n",
    "        model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "        model_data_id_list.append(model_data_id)\n",
    "        model = rnn_model_pool[model_name]\n",
    "        model.train(x_train, y_train) ################################ maybe shuffle x_train along with y_train?\n",
    "        est_preds = model.predict(x_test)\n",
    "\n",
    "        base_layer_est_preds[model_data_id] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519810384\n"
     ]
    }
   ],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_est_preds['ModelName.RNN_rnn_data_001'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1VS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsOne is using svc kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using svc kernel\n",
      "OneVsOne is using logistic kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using logistic kernel\n"
     ]
    }
   ],
   "source": [
    "onevsone_svc = OneVSOneReg(x_train_1v1, y_train_1v1, model='svc')\n",
    "onevsone_logreg = OneVSOneReg(x_train_1v1, y_train_1v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "model_pool[ModelName.ONESVC] = onevsone_svc\n",
    "model_pool[ModelName.ONELOGREG] = onevsone_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "########################## ONE VS ONE ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "            if model_name == ModelName.LGB:\n",
    "                model = LogisticRegression(solver='sag')\n",
    "                sfm = SelectFromModel(model, threshold='5*mean')\n",
    "                print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "                x_train = sfm.fit_transform(x_train, y_train)\n",
    "                x_test = sfm.transform(x_test)\n",
    "                print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof_1v1(model_pool[model_name],  x_train, y_train, x_test, NFOLDS, label) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train, label)\n",
    "            est_preds = model.predict(x_test, label)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "model_pool = {}\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# logreg_params = {\n",
    "#     'n_jobs': 3\n",
    "# }\n",
    "# logreg_ble = SklearnBLE(LogisticRegression, seed=SEED, params=logreg_params)\n",
    "# model_pool[ModelName.LOGREG] = logreg_ble\n",
    "\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={'n_jobs': 3})\n",
    "#model_pool[ModelName.RF] = rf_ble\n",
    "\n",
    "\n",
    "# nblsvc_params = {\n",
    "#     'C':0.02\n",
    "# }\n",
    "# nblsvc_ble = NbSvmBLE(mode=ModelName.NBLSVC, seed=SEED, params=nblsvc_params)\n",
    "# model_pool[ModelName.NBLSVC] = nblsvc_ble\n",
    "\n",
    "\n",
    "# nbsvm_params = {\n",
    "#     'C':0.25,\n",
    "#     #'dual':True,\n",
    "#     'n_jobs':5\n",
    "# }\n",
    "# nbsvm_ble = NbSvmBLE(mode=ModelName.NBSVM, seed=SEED, params=nbsvm_params)\n",
    "# model_pool[ModelName.NBSVM] = nbsvm_ble\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={})\n",
    "#xgb_ble = XgbBLE(params=xgb_params)\n",
    "\n",
    "lgb_params = {\n",
    "    #'learning_rate': 0.05,\n",
    "    'is_unbalance': True,\n",
    "    'early_stopping_round': 25,\n",
    "    'max_depth': -1,\n",
    "    'num_boost_round': 3000,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': 1,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 14\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 1\n",
    "}\n",
    "lgb_ble = LightgbmBLE(x_train_1v1, y_train_1v1, params=lgb_params, nb=False)\n",
    "model_pool[ModelName.LGB] = lgb_ble\n",
    "\n",
    "\n",
    "# lg = SklearnBLE(clf=LogisticRegression, seed=SEED, params={'n_jobs': 1})\n",
    "\n",
    "# et = SklearnBLE(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "# ada = SklearnBLE(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "# gb = SklearnBLE(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "# svc = SklearnBLE(clf=SVC, seed=SEED, params=svc_params)\n",
    "\n",
    "#model_pool['rf'] = rf_ble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.LGB wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds.\n",
      "[20]\tvalid_0's auc: 0.94673\n",
      "[40]\tvalid_0's auc: 0.9676\n",
      "[60]\tvalid_0's auc: 0.978568\n",
      "[80]\tvalid_0's auc: 0.984588\n",
      "[100]\tvalid_0's auc: 0.988221\n",
      "[120]\tvalid_0's auc: 0.990593\n",
      "[140]\tvalid_0's auc: 0.992301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9d2b9f2d6b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#                 print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNFOLDS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Logreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer1_oof_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mlayer1_oof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6de8033a6a47>\u001b[0m in \u001b[0;36mget_oof\u001b[0;34m(clf, x_train, y_train, x_test, nfolds)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/kaggle/toxic/sc/stacking/base_layer_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, label)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mlgb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mlgb_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################## NORMAL MODELS ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "#             if model_name == ModelName.LGB:\n",
    "#                 model = LogisticRegression(solver='sag')\n",
    "#                 sfm = SelectFromModel(model, threshold='5*mean')\n",
    "#                 print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "#                 x_train = sfm.fit_transform(x_train, y_train)\n",
    "#                 x_test = sfm.transform(x_test)\n",
    "#                 print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof(model_pool[model_name],  x_train, y_train, x_test, NFOLDS) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train)\n",
    "            est_preds = model.predict(x_test)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submit the base layer estimator predictions. If they look fine, save them to BaseLayerResultsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
       " 'ModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
       " 'ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'insult', 'obscene', 'threat', 'severe_toxic', 'identity_hate']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_train) # list keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_train['toxic']) # number of models to stack (each model will predict one set of toxic, servere_toxic, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_train['toxic'][0]) # examples in oof_train (meta features, x_train) (meta labels are in train[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'insult', 'obscene', 'threat', 'severe_toxic', 'identity_hate']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_test['toxic'][0]) # examples in oof_test (will be used by meta model (after validation) to predict the final prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()#load_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add(layer1_oof_train, layer1_oof_test, base_layer_est_preds, model_data_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', 0.9826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before we choose which models to assemble, we can do:\n",
    "#### 1. scatter plot analysis to check the diversity\n",
    "#### 2. submit to check if the models have similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_layer_oof_per_label(layer1_oof_dict, label):\n",
    "    x = None\n",
    "    data_list = layer1_oof_dict[label]\n",
    "    for i in range(len(data_list)):\n",
    "        if i == 0:\n",
    "            x = data_list[0]\n",
    "        else:\n",
    "            x = np.concatenate((x, data_list[i]), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simple blend of two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "result = np.empty((test.shape[0],len(label_cols)))\n",
    "\n",
    "# mix the first two models\n",
    "for i, label in enumerate(label_cols):\n",
    "    x_train = combine_layer_oof_per_label(layer1_oof_train, label)\n",
    "    x_test = combine_layer_oof_per_label(layer1_oof_test, label)\n",
    "    for j in range(x_train.shape[1]):\n",
    "        roc = roc_auc_score(train[label], x_train[:,j])\n",
    "        print(label, j, roc) # print out roc for meta feature on meta label (which is just the original train label)\n",
    "    \n",
    "    roc_scores_of_a_label = []\n",
    "    alphas = np.linspace(0,1,1001)\n",
    "    best_roc = 0\n",
    "    best_alpha = 0\n",
    "    for alpha in alphas:\n",
    "        roc = roc_auc_score(train[label], alpha*x_train[:,0] + (1-alpha)*x_train[:,1])\n",
    "        if roc > best_roc:\n",
    "            best_roc = roc\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(label, best_roc, best_alpha)\n",
    "    result[:,i] = best_alpha*x_test[:,0] + (1-best_alpha)*x_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "submission[label_cols] = result\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./StackPreds/mixtwo_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insult', 'toxic', 'obscene', 'threat', 'severe_toxic', 'identity_hate']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0.9826 update to 0.09826\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.09826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.09819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ths = [score for _, score in scores][3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9815, 0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0 update to 0.9818\n"
     ]
    }
   ],
   "source": [
    "#base_layer_results_repo.add_score('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.9818)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5e6a15ce28a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('./xgb_search.csv', 'a')\n",
    "# header = 'time,id,th,amt,csbt,lr,md,ss,ga,a,rounds,folds,tops,sps,ops,\\\n",
    "# thps,inps,ihps,tobr,sbr,obr,thbr,inbr,ihbr,sth1,sth2,sth3,sth4,sth5,\\\n",
    "# to_auc,s_auc,o_auc,th_auc,in_auc,ih_auc,avg_auc\\n'\n",
    "header = 'time,id,threshold,num_models,colsample_bytree,lr,max_depth,subsample,\\\n",
    "gamma,alpha,cv_num_round,cv_nfolds,toxic_pos_scale,severe_toxic_pos_scale,obscene_pos_scale,\\\n",
    "threat_pos_scale,insult_pos_scale,identity_hate_pos_scale,sth1,sth2,sth3,sth4,sth5,\\\n",
    "toxic_best_round,severe_toxic_best_round,obscene_best_round,threat_best_round,\\\n",
    "insult_best_round,identity_hate_best_round,toxic_auc,severe_toxic_auc,obscene_auc,\\\n",
    "threat_auc,insult_auc,identity_hate_auc,avg_auc\\n'\n",
    "f.write(header)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "    \n",
    "    return est.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 00:25:50, id: 1519968350, th: 0.980300, num_models: 5, colsample_bytree: 0.700000, lr: 0.0152982,     max_depth: 4, subsample: 0.690000, gamma: 2, alpha: 0, cv_num_round: 633, cv_nfolds: 4,    to_pw: 6, s_pw: 74, ob_pw: 14, th_pw: 139, in_pw: 10, ih_pw: 128            \n",
      "time: 2018-03-02 00:34:34, id: 1519968874, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0105918,     max_depth: 4, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 729, cv_nfolds: 4,    to_pw: 7, s_pw: 121, ob_pw: 23, th_pw: 292, in_pw: 21, ih_pw: 42            \n",
      "time: 2018-03-02 00:40:43, id: 1519969243, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0027763,     max_depth: 4, subsample: 0.810000, gamma: 0, alpha: 1, cv_num_round: 471, cv_nfolds: 3,    to_pw: 7, s_pw: 52, ob_pw: 24, th_pw: 251, in_pw: 13, ih_pw: 118            \n",
      "time: 2018-03-02 00:44:33, id: 1519969473, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0010538,     max_depth: 7, subsample: 0.560000, gamma: 1, alpha: 1, cv_num_round: 481, cv_nfolds: 3,    to_pw: 13, s_pw: 26, ob_pw: 3, th_pw: 105, in_pw: 13, ih_pw: 66            \n",
      "time: 2018-03-02 00:47:37, id: 1519969657, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0018798,     max_depth: 3, subsample: 0.810000, gamma: 1, alpha: 0, cv_num_round: 834, cv_nfolds: 4,    to_pw: 12, s_pw: 43, ob_pw: 13, th_pw: 207, in_pw: 5, ih_pw: 137            \n",
      "time: 2018-03-02 00:54:48, id: 1519970088, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0966631,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 607, cv_nfolds: 4,    to_pw: 10, s_pw: 103, ob_pw: 18, th_pw: 120, in_pw: 8, ih_pw: 136            \n",
      "time: 2018-03-02 00:58:52, id: 1519970332, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0038114,     max_depth: 6, subsample: 0.890000, gamma: 0, alpha: 1, cv_num_round: 618, cv_nfolds: 3,    to_pw: 3, s_pw: 34, ob_pw: 11, th_pw: 110, in_pw: 13, ih_pw: 55            \n",
      "time: 2018-03-02 01:04:15, id: 1519970655, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0100228,     max_depth: 5, subsample: 0.560000, gamma: 1, alpha: 1, cv_num_round: 637, cv_nfolds: 3,    to_pw: 6, s_pw: 31, ob_pw: 9, th_pw: 218, in_pw: 24, ih_pw: 125            \n",
      "time: 2018-03-02 01:11:03, id: 1519971063, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0017185,     max_depth: 2, subsample: 0.990000, gamma: 1, alpha: 0, cv_num_round: 989, cv_nfolds: 3,    to_pw: 12, s_pw: 67, ob_pw: 23, th_pw: 144, in_pw: 10, ih_pw: 91            \n",
      "time: 2018-03-02 01:18:06, id: 1519971486, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0027707,     max_depth: 5, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 708, cv_nfolds: 3,    to_pw: 7, s_pw: 96, ob_pw: 22, th_pw: 279, in_pw: 24, ih_pw: 85            \n",
      "time: 2018-03-02 01:22:14, id: 1519971734, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0231520,     max_depth: 4, subsample: 0.620000, gamma: 2, alpha: 1, cv_num_round: 625, cv_nfolds: 3,    to_pw: 8, s_pw: 51, ob_pw: 6, th_pw: 102, in_pw: 13, ih_pw: 87            \n",
      "time: 2018-03-02 01:28:41, id: 1519972121, th: 0.976800, num_models: 10, colsample_bytree: 0.500000, lr: 0.0329555,     max_depth: 6, subsample: 0.910000, gamma: 2, alpha: 0, cv_num_round: 896, cv_nfolds: 3,    to_pw: 9, s_pw: 59, ob_pw: 20, th_pw: 339, in_pw: 13, ih_pw: 78            \n",
      "time: 2018-03-02 01:34:19, id: 1519972459, th: 0.980300, num_models: 5, colsample_bytree: 0.500000, lr: 0.0071469,     max_depth: 4, subsample: 0.750000, gamma: 2, alpha: 0, cv_num_round: 614, cv_nfolds: 4,    to_pw: 8, s_pw: 23, ob_pw: 3, th_pw: 185, in_pw: 12, ih_pw: 138            \n",
      "time: 2018-03-02 01:41:34, id: 1519972894, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0266129,     max_depth: 2, subsample: 0.560000, gamma: 0, alpha: 0, cv_num_round: 405, cv_nfolds: 4,    to_pw: 5, s_pw: 36, ob_pw: 24, th_pw: 188, in_pw: 7, ih_pw: 65            \n",
      "time: 2018-03-02 01:50:22, id: 1519973422, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0035740,     max_depth: 5, subsample: 0.700000, gamma: 1, alpha: 0, cv_num_round: 425, cv_nfolds: 3,    to_pw: 13, s_pw: 110, ob_pw: 3, th_pw: 64, in_pw: 19, ih_pw: 121            \n",
      "time: 2018-03-02 01:55:49, id: 1519973749, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0818469,     max_depth: 2, subsample: 0.730000, gamma: 1, alpha: 1, cv_num_round: 570, cv_nfolds: 3,    to_pw: 12, s_pw: 71, ob_pw: 8, th_pw: 124, in_pw: 17, ih_pw: 68            \n",
      "time: 2018-03-02 01:59:46, id: 1519973986, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0615287,     max_depth: 2, subsample: 0.580000, gamma: 1, alpha: 1, cv_num_round: 519, cv_nfolds: 4,    to_pw: 14, s_pw: 122, ob_pw: 15, th_pw: 110, in_pw: 19, ih_pw: 72            \n",
      "time: 2018-03-02 02:07:07, id: 1519974427, th: 0.978600, num_models: 8, colsample_bytree: 0.900000, lr: 0.0034895,     max_depth: 6, subsample: 0.970000, gamma: 1, alpha: 0, cv_num_round: 693, cv_nfolds: 4,    to_pw: 12, s_pw: 76, ob_pw: 16, th_pw: 99, in_pw: 23, ih_pw: 41            \n",
      "time: 2018-03-02 02:15:58, id: 1519974958, th: 0.980300, num_models: 5, colsample_bytree: 0.800000, lr: 0.0031782,     max_depth: 3, subsample: 0.630000, gamma: 2, alpha: 0, cv_num_round: 641, cv_nfolds: 4,    to_pw: 12, s_pw: 22, ob_pw: 21, th_pw: 198, in_pw: 21, ih_pw: 46            \n",
      "time: 2018-03-02 02:22:08, id: 1519975328, th: 0.980300, num_models: 5, colsample_bytree: 0.700000, lr: 0.0228572,     max_depth: 4, subsample: 0.580000, gamma: 2, alpha: 0, cv_num_round: 482, cv_nfolds: 4,    to_pw: 2, s_pw: 46, ob_pw: 9, th_pw: 76, in_pw: 23, ih_pw: 96            \n",
      "time: 2018-03-02 02:31:13, id: 1519975873, th: 0.981500, num_models: 4, colsample_bytree: 0.700000, lr: 0.0137914,     max_depth: 6, subsample: 0.920000, gamma: 2, alpha: 0, cv_num_round: 787, cv_nfolds: 4,    to_pw: 12, s_pw: 37, ob_pw: 11, th_pw: 171, in_pw: 11, ih_pw: 57            \n",
      "time: 2018-03-02 02:39:26, id: 1519976366, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0858544,     max_depth: 5, subsample: 0.580000, gamma: 0, alpha: 1, cv_num_round: 876, cv_nfolds: 3,    to_pw: 4, s_pw: 46, ob_pw: 16, th_pw: 100, in_pw: 20, ih_pw: 116            \n",
      "time: 2018-03-02 02:42:07, id: 1519976527, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0616493,     max_depth: 4, subsample: 0.710000, gamma: 0, alpha: 0, cv_num_round: 689, cv_nfolds: 4,    to_pw: 14, s_pw: 88, ob_pw: 19, th_pw: 343, in_pw: 23, ih_pw: 120            \n",
      "time: 2018-03-02 02:47:13, id: 1519976833, th: 0.977400, num_models: 9, colsample_bytree: 0.600000, lr: 0.0070077,     max_depth: 5, subsample: 0.600000, gamma: 1, alpha: 1, cv_num_round: 849, cv_nfolds: 3,    to_pw: 3, s_pw: 119, ob_pw: 3, th_pw: 107, in_pw: 13, ih_pw: 111            \n",
      "time: 2018-03-02 02:52:27, id: 1519977147, th: 0.981500, num_models: 4, colsample_bytree: 0.600000, lr: 0.0983639,     max_depth: 2, subsample: 0.500000, gamma: 1, alpha: 1, cv_num_round: 734, cv_nfolds: 3,    to_pw: 11, s_pw: 27, ob_pw: 22, th_pw: 227, in_pw: 9, ih_pw: 73            \n",
      "time: 2018-03-02 02:55:45, id: 1519977345, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0018059,     max_depth: 7, subsample: 0.560000, gamma: 2, alpha: 0, cv_num_round: 895, cv_nfolds: 3,    to_pw: 6, s_pw: 53, ob_pw: 14, th_pw: 168, in_pw: 4, ih_pw: 131            \n",
      "time: 2018-03-02 02:59:22, id: 1519977562, th: 0.981500, num_models: 4, colsample_bytree: 0.900000, lr: 0.0227284,     max_depth: 6, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 949, cv_nfolds: 4,    to_pw: 10, s_pw: 73, ob_pw: 19, th_pw: 168, in_pw: 6, ih_pw: 135            \n",
      "time: 2018-03-02 03:04:34, id: 1519977874, th: 0.976800, num_models: 10, colsample_bytree: 0.800000, lr: 0.0017340,     max_depth: 7, subsample: 0.850000, gamma: 2, alpha: 1, cv_num_round: 451, cv_nfolds: 4,    to_pw: 5, s_pw: 20, ob_pw: 22, th_pw: 312, in_pw: 20, ih_pw: 53            \n",
      "time: 2018-03-02 03:12:52, id: 1519978372, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0015783,     max_depth: 3, subsample: 0.880000, gamma: 1, alpha: 0, cv_num_round: 742, cv_nfolds: 4,    to_pw: 9, s_pw: 122, ob_pw: 22, th_pw: 318, in_pw: 15, ih_pw: 39            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 03:22:59, id: 1519978979, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0685323,     max_depth: 2, subsample: 0.880000, gamma: 2, alpha: 1, cv_num_round: 984, cv_nfolds: 4,    to_pw: 8, s_pw: 87, ob_pw: 11, th_pw: 93, in_pw: 10, ih_pw: 109            \n",
      "time: 2018-03-02 03:31:54, id: 1519979514, th: 0.976800, num_models: 10, colsample_bytree: 0.900000, lr: 0.0392708,     max_depth: 5, subsample: 0.630000, gamma: 0, alpha: 0, cv_num_round: 498, cv_nfolds: 4,    to_pw: 11, s_pw: 121, ob_pw: 23, th_pw: 351, in_pw: 23, ih_pw: 57            \n",
      "time: 2018-03-02 03:37:12, id: 1519979832, th: 0.977400, num_models: 9, colsample_bytree: 0.700000, lr: 0.0030536,     max_depth: 6, subsample: 0.700000, gamma: 0, alpha: 1, cv_num_round: 873, cv_nfolds: 4,    to_pw: 4, s_pw: 114, ob_pw: 7, th_pw: 341, in_pw: 4, ih_pw: 118            \n",
      "time: 2018-03-02 03:45:08, id: 1519980308, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0649149,     max_depth: 7, subsample: 0.660000, gamma: 2, alpha: 1, cv_num_round: 895, cv_nfolds: 4,    to_pw: 7, s_pw: 97, ob_pw: 16, th_pw: 68, in_pw: 11, ih_pw: 101            \n",
      "time: 2018-03-02 03:50:00, id: 1519980600, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0491951,     max_depth: 7, subsample: 0.630000, gamma: 0, alpha: 1, cv_num_round: 843, cv_nfolds: 4,    to_pw: 4, s_pw: 25, ob_pw: 14, th_pw: 184, in_pw: 24, ih_pw: 41            \n",
      "time: 2018-03-02 03:54:44, id: 1519980884, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0140443,     max_depth: 5, subsample: 0.800000, gamma: 2, alpha: 0, cv_num_round: 704, cv_nfolds: 4,    to_pw: 6, s_pw: 69, ob_pw: 5, th_pw: 109, in_pw: 10, ih_pw: 54            \n",
      "time: 2018-03-02 04:06:50, id: 1519981610, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0191208,     max_depth: 7, subsample: 0.570000, gamma: 0, alpha: 1, cv_num_round: 927, cv_nfolds: 4,    to_pw: 11, s_pw: 111, ob_pw: 22, th_pw: 135, in_pw: 13, ih_pw: 115            \n",
      "time: 2018-03-02 04:15:21, id: 1519982121, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0030881,     max_depth: 7, subsample: 0.780000, gamma: 1, alpha: 0, cv_num_round: 996, cv_nfolds: 3,    to_pw: 13, s_pw: 125, ob_pw: 8, th_pw: 166, in_pw: 23, ih_pw: 88            \n",
      "time: 2018-03-02 04:20:01, id: 1519982401, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0424453,     max_depth: 2, subsample: 0.830000, gamma: 0, alpha: 0, cv_num_round: 548, cv_nfolds: 4,    to_pw: 12, s_pw: 84, ob_pw: 3, th_pw: 326, in_pw: 8, ih_pw: 99            \n",
      "time: 2018-03-02 04:28:01, id: 1519982881, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0268777,     max_depth: 2, subsample: 0.850000, gamma: 2, alpha: 1, cv_num_round: 669, cv_nfolds: 4,    to_pw: 2, s_pw: 85, ob_pw: 19, th_pw: 264, in_pw: 24, ih_pw: 69            \n",
      "time: 2018-03-02 04:39:06, id: 1519983546, th: 0.977400, num_models: 9, colsample_bytree: 0.500000, lr: 0.0082537,     max_depth: 4, subsample: 0.540000, gamma: 1, alpha: 1, cv_num_round: 997, cv_nfolds: 3,    to_pw: 14, s_pw: 92, ob_pw: 3, th_pw: 303, in_pw: 10, ih_pw: 31            \n",
      "time: 2018-03-02 04:50:55, id: 1519984255, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0024649,     max_depth: 2, subsample: 0.950000, gamma: 2, alpha: 1, cv_num_round: 811, cv_nfolds: 4,    to_pw: 2, s_pw: 20, ob_pw: 16, th_pw: 267, in_pw: 11, ih_pw: 76            \n",
      "time: 2018-03-02 05:04:24, id: 1519985064, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0058243,     max_depth: 2, subsample: 0.720000, gamma: 1, alpha: 1, cv_num_round: 484, cv_nfolds: 3,    to_pw: 12, s_pw: 74, ob_pw: 11, th_pw: 323, in_pw: 16, ih_pw: 104            \n",
      "time: 2018-03-02 05:11:20, id: 1519985480, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0681723,     max_depth: 7, subsample: 0.890000, gamma: 2, alpha: 0, cv_num_round: 441, cv_nfolds: 3,    to_pw: 7, s_pw: 29, ob_pw: 15, th_pw: 261, in_pw: 7, ih_pw: 71            \n",
      "time: 2018-03-02 05:14:48, id: 1519985688, th: 0.976800, num_models: 10, colsample_bytree: 0.500000, lr: 0.0133457,     max_depth: 2, subsample: 0.970000, gamma: 0, alpha: 1, cv_num_round: 688, cv_nfolds: 4,    to_pw: 10, s_pw: 33, ob_pw: 19, th_pw: 149, in_pw: 17, ih_pw: 92            \n",
      "time: 2018-03-02 05:26:12, id: 1519986372, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0256713,     max_depth: 5, subsample: 0.760000, gamma: 1, alpha: 0, cv_num_round: 999, cv_nfolds: 3,    to_pw: 13, s_pw: 31, ob_pw: 21, th_pw: 344, in_pw: 9, ih_pw: 69            \n",
      "time: 2018-03-02 05:32:05, id: 1519986725, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0030848,     max_depth: 7, subsample: 0.620000, gamma: 0, alpha: 0, cv_num_round: 748, cv_nfolds: 4,    to_pw: 14, s_pw: 95, ob_pw: 13, th_pw: 66, in_pw: 24, ih_pw: 132            \n",
      "time: 2018-03-02 05:40:19, id: 1519987219, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0178147,     max_depth: 4, subsample: 0.910000, gamma: 1, alpha: 1, cv_num_round: 668, cv_nfolds: 4,    to_pw: 10, s_pw: 81, ob_pw: 23, th_pw: 133, in_pw: 9, ih_pw: 78            \n",
      "time: 2018-03-02 05:54:17, id: 1519988057, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0141059,     max_depth: 4, subsample: 0.740000, gamma: 0, alpha: 0, cv_num_round: 526, cv_nfolds: 3,    to_pw: 14, s_pw: 38, ob_pw: 9, th_pw: 85, in_pw: 23, ih_pw: 101            \n",
      "time: 2018-03-02 06:00:17, id: 1519988417, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0227382,     max_depth: 5, subsample: 0.800000, gamma: 1, alpha: 1, cv_num_round: 935, cv_nfolds: 3,    to_pw: 14, s_pw: 27, ob_pw: 4, th_pw: 305, in_pw: 8, ih_pw: 48            \n",
      "time: 2018-03-02 06:07:11, id: 1519988831, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0010075,     max_depth: 6, subsample: 0.580000, gamma: 2, alpha: 1, cv_num_round: 838, cv_nfolds: 4,    to_pw: 3, s_pw: 59, ob_pw: 24, th_pw: 271, in_pw: 11, ih_pw: 67            \n",
      "time: 2018-03-02 06:13:43, id: 1519989223, th: 0.981500, num_models: 4, colsample_bytree: 0.900000, lr: 0.0068811,     max_depth: 6, subsample: 0.760000, gamma: 1, alpha: 1, cv_num_round: 787, cv_nfolds: 3,    to_pw: 11, s_pw: 113, ob_pw: 5, th_pw: 197, in_pw: 10, ih_pw: 59            \n",
      "time: 2018-03-02 06:20:00, id: 1519989600, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0017916,     max_depth: 2, subsample: 0.510000, gamma: 2, alpha: 1, cv_num_round: 576, cv_nfolds: 4,    to_pw: 10, s_pw: 48, ob_pw: 7, th_pw: 227, in_pw: 7, ih_pw: 115            \n",
      "time: 2018-03-02 06:27:06, id: 1519990026, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0240391,     max_depth: 5, subsample: 0.600000, gamma: 1, alpha: 0, cv_num_round: 736, cv_nfolds: 4,    to_pw: 10, s_pw: 99, ob_pw: 12, th_pw: 348, in_pw: 12, ih_pw: 107            \n",
      "time: 2018-03-02 06:35:47, id: 1519990547, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0121182,     max_depth: 6, subsample: 0.930000, gamma: 1, alpha: 1, cv_num_round: 566, cv_nfolds: 3,    to_pw: 8, s_pw: 116, ob_pw: 24, th_pw: 181, in_pw: 20, ih_pw: 57            \n",
      "time: 2018-03-02 06:44:48, id: 1519991088, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0372951,     max_depth: 4, subsample: 0.590000, gamma: 0, alpha: 0, cv_num_round: 893, cv_nfolds: 4,    to_pw: 10, s_pw: 29, ob_pw: 12, th_pw: 185, in_pw: 8, ih_pw: 81            \n",
      "time: 2018-03-02 06:52:03, id: 1519991523, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0431916,     max_depth: 3, subsample: 0.990000, gamma: 0, alpha: 0, cv_num_round: 629, cv_nfolds: 4,    to_pw: 2, s_pw: 33, ob_pw: 8, th_pw: 102, in_pw: 20, ih_pw: 88            \n",
      "time: 2018-03-02 06:59:56, id: 1519991996, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0202178,     max_depth: 5, subsample: 0.870000, gamma: 0, alpha: 0, cv_num_round: 914, cv_nfolds: 4,    to_pw: 8, s_pw: 49, ob_pw: 4, th_pw: 240, in_pw: 15, ih_pw: 91            \n",
      "time: 2018-03-02 07:09:02, id: 1519992542, th: 0.978600, num_models: 8, colsample_bytree: 0.700000, lr: 0.0229555,     max_depth: 2, subsample: 0.990000, gamma: 0, alpha: 1, cv_num_round: 444, cv_nfolds: 3,    to_pw: 5, s_pw: 113, ob_pw: 19, th_pw: 324, in_pw: 9, ih_pw: 78            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 07:15:18, id: 1519992918, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0010395,     max_depth: 7, subsample: 0.840000, gamma: 1, alpha: 1, cv_num_round: 695, cv_nfolds: 4,    to_pw: 4, s_pw: 85, ob_pw: 23, th_pw: 341, in_pw: 9, ih_pw: 105            \n",
      "time: 2018-03-02 07:21:05, id: 1519993265, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0032795,     max_depth: 2, subsample: 0.860000, gamma: 1, alpha: 1, cv_num_round: 544, cv_nfolds: 3,    to_pw: 9, s_pw: 43, ob_pw: 9, th_pw: 69, in_pw: 19, ih_pw: 61            \n",
      "time: 2018-03-02 07:28:58, id: 1519993738, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0188473,     max_depth: 3, subsample: 0.940000, gamma: 2, alpha: 0, cv_num_round: 405, cv_nfolds: 3,    to_pw: 14, s_pw: 114, ob_pw: 17, th_pw: 344, in_pw: 24, ih_pw: 30            \n",
      "time: 2018-03-02 07:35:41, id: 1519994141, th: 0.977400, num_models: 9, colsample_bytree: 0.800000, lr: 0.0688231,     max_depth: 2, subsample: 0.650000, gamma: 2, alpha: 1, cv_num_round: 456, cv_nfolds: 3,    to_pw: 13, s_pw: 71, ob_pw: 5, th_pw: 294, in_pw: 13, ih_pw: 84            \n",
      "time: 2018-03-02 07:40:04, id: 1519994404, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0164190,     max_depth: 2, subsample: 0.770000, gamma: 0, alpha: 1, cv_num_round: 578, cv_nfolds: 4,    to_pw: 9, s_pw: 124, ob_pw: 15, th_pw: 116, in_pw: 21, ih_pw: 30            \n",
      "time: 2018-03-02 07:49:39, id: 1519994979, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0102957,     max_depth: 3, subsample: 0.740000, gamma: 0, alpha: 1, cv_num_round: 797, cv_nfolds: 4,    to_pw: 10, s_pw: 83, ob_pw: 9, th_pw: 150, in_pw: 7, ih_pw: 111            \n",
      "time: 2018-03-02 08:00:15, id: 1519995615, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0053757,     max_depth: 6, subsample: 0.670000, gamma: 0, alpha: 1, cv_num_round: 678, cv_nfolds: 4,    to_pw: 5, s_pw: 66, ob_pw: 5, th_pw: 214, in_pw: 9, ih_pw: 53            \n",
      "time: 2018-03-02 08:09:14, id: 1519996154, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0354215,     max_depth: 6, subsample: 0.520000, gamma: 1, alpha: 1, cv_num_round: 839, cv_nfolds: 3,    to_pw: 14, s_pw: 39, ob_pw: 18, th_pw: 364, in_pw: 13, ih_pw: 89            \n",
      "time: 2018-03-02 08:14:30, id: 1519996470, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0027659,     max_depth: 5, subsample: 0.690000, gamma: 0, alpha: 0, cv_num_round: 543, cv_nfolds: 4,    to_pw: 13, s_pw: 102, ob_pw: 10, th_pw: 350, in_pw: 13, ih_pw: 57            \n",
      "time: 2018-03-02 08:21:41, id: 1519996901, th: 0.977400, num_models: 9, colsample_bytree: 0.700000, lr: 0.0465120,     max_depth: 3, subsample: 0.690000, gamma: 0, alpha: 1, cv_num_round: 892, cv_nfolds: 4,    to_pw: 11, s_pw: 61, ob_pw: 5, th_pw: 364, in_pw: 21, ih_pw: 61            \n",
      "time: 2018-03-02 08:28:42, id: 1519997322, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0124011,     max_depth: 7, subsample: 0.710000, gamma: 0, alpha: 1, cv_num_round: 440, cv_nfolds: 4,    to_pw: 2, s_pw: 108, ob_pw: 7, th_pw: 212, in_pw: 24, ih_pw: 30            \n",
      "time: 2018-03-02 08:36:45, id: 1519997805, th: 0.976800, num_models: 10, colsample_bytree: 0.700000, lr: 0.0310058,     max_depth: 4, subsample: 0.900000, gamma: 1, alpha: 0, cv_num_round: 831, cv_nfolds: 3,    to_pw: 8, s_pw: 72, ob_pw: 12, th_pw: 200, in_pw: 5, ih_pw: 67            \n",
      "time: 2018-03-02 08:43:18, id: 1519998198, th: 0.978600, num_models: 8, colsample_bytree: 0.800000, lr: 0.0168915,     max_depth: 7, subsample: 0.670000, gamma: 2, alpha: 1, cv_num_round: 570, cv_nfolds: 4,    to_pw: 2, s_pw: 69, ob_pw: 21, th_pw: 307, in_pw: 6, ih_pw: 133            \n",
      "time: 2018-03-02 08:53:25, id: 1519998805, th: 0.977400, num_models: 9, colsample_bytree: 0.500000, lr: 0.0405281,     max_depth: 7, subsample: 0.860000, gamma: 1, alpha: 0, cv_num_round: 934, cv_nfolds: 3,    to_pw: 7, s_pw: 51, ob_pw: 22, th_pw: 107, in_pw: 16, ih_pw: 136            \n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(130):\n",
    "    now = get_time()\n",
    "    search_id = int(time.time())\n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    model_threshold = np.random.choice(ths)#[0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "        \n",
    "    xgb_colsample_bytree = np.random.randint(5, 10)/10\n",
    "    xgb_learning_rate = 1e-2 * (0.1 ** (np.random.rand() * 2 - 1.0)) # 0.001 to 0.0997\n",
    "    xgb_max_depth = np.random.randint(2, 8)\n",
    "    xgb_subsample = np.random.randint(50, 100)/100\n",
    "    xgb_gamma = np.random.randint(0, 3)\n",
    "    xgb_alpha = np.random.randint(0, 2)\n",
    "\n",
    "    xgb_cv_seed = 0\n",
    "    xgb_cv_num_round = np.random.randint(400, 1000)\n",
    "    xgb_cv_nfolds = np.random.randint(3,5)\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = np.random.randint(2, 15) #10\n",
    "    scale_pos_weights['severe_toxic'] = np.random.randint(20, 130) # 100\n",
    "    scale_pos_weights['obscene'] = np.random.randint(3, 25) # 17\n",
    "    scale_pos_weights['threat'] = np.random.randint(60, 380) # 333\n",
    "    scale_pos_weights['insult'] = np.random.randint(4, 25) # 20\n",
    "    scale_pos_weights['identity_hate'] = np.random.randint(30, 140) #112\n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    num_models = len(layer1_oof_train_loaded['toxic'])\n",
    "    #print('Stacking {} models'.format(num_models)) # number of models that will be stacked\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate']))\n",
    "    \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {} # all labels\n",
    "    best_nrounds = {}  # all labels\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "    #     clf = XGBClassifier()\n",
    "    #     #scores = cross_val_score(clf, x_train, train[label], cv=3, scoring='roc_auc')\n",
    "    #     #print(scores)\n",
    "    #     #print(\"Stacking-CV: ROC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    #     clf.fit(x_train, train[label])\n",
    "    #     result[:, i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds_for_current_label = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "\n",
    "        #print('Ensemble-CV: {}: {}+{}'.format(label, cv_mean, cv_std))\n",
    "        metric_dict[label] = cv_mean\n",
    "        best_nrounds[label] = best_nrounds_for_current_label\n",
    "        #metric_dict[label]['cv_mean'] = cv_mean\n",
    "        #metric_dict[label]['cv_std'] = cv_std\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds_for_current_label)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "          \n",
    "    res = '%s,%d,%f,%d,%f,%.7f,%d,%f,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f\\n\\\n",
    "            '%(now,search_id,model_threshold,num_models,xgb_colsample_bytree,\\\n",
    "               xgb_learning_rate,xgb_max_depth,xgb_subsample,xgb_gamma,xgb_alpha,\\\n",
    "               xgb_cv_num_round,xgb_cv_nfolds,scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "               scale_pos_weights['obscene'],scale_pos_weights['threat'],scale_pos_weights['insult'],\\\n",
    "               scale_pos_weights['identity_hate'],-99,-99,-99,-99,-99,best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "               best_nrounds['obscene'],best_nrounds['threat'],best_nrounds['insult'],\\\n",
    "               best_nrounds['identity_hate'],metric_dict['toxic'],metric_dict['severe_toxic'],\\\n",
    "               metric_dict['obscene'],metric_dict['threat'],metric_dict['insult'],\\\n",
    "               metric_dict['identity_hate'],avg_auc)\n",
    "\n",
    "    f = open('./xgb_search.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()\n",
    "\n",
    "#     sub_tile = 'stacking_test_'\n",
    "#     submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "#     submission[label_cols] = result\n",
    "#     submission.to_csv('./StackPreds/' + sub_tile + str(search_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb random search top N training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from base_layer_results_repo import BaseLayerResultsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.09819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>threat_best_round</th>\n",
       "      <th>insult_best_round</th>\n",
       "      <th>identity_hate_best_round</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-02 00:54:48</td>\n",
       "      <td>1519970088</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.096663</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>74</td>\n",
       "      <td>98</td>\n",
       "      <td>0.987579</td>\n",
       "      <td>0.991802</td>\n",
       "      <td>0.995448</td>\n",
       "      <td>0.994074</td>\n",
       "      <td>0.990130</td>\n",
       "      <td>0.991359</td>\n",
       "      <td>0.991732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2018-03-02 08:36:45</td>\n",
       "      <td>1519997805</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.031006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>273</td>\n",
       "      <td>163</td>\n",
       "      <td>195</td>\n",
       "      <td>0.987640</td>\n",
       "      <td>0.991029</td>\n",
       "      <td>0.995461</td>\n",
       "      <td>0.993971</td>\n",
       "      <td>0.990103</td>\n",
       "      <td>0.991239</td>\n",
       "      <td>0.991574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2018-03-02 07:35:41</td>\n",
       "      <td>1519994141</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.068823</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>104</td>\n",
       "      <td>0.987659</td>\n",
       "      <td>0.991810</td>\n",
       "      <td>0.995460</td>\n",
       "      <td>0.992785</td>\n",
       "      <td>0.990203</td>\n",
       "      <td>0.991231</td>\n",
       "      <td>0.991525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               time          id  threshold  num_models  \\\n",
       "9               2018-03-02 00:54:48  1519970088     0.9794           6   \n",
       "73              2018-03-02 08:36:45  1519997805     0.9768          10   \n",
       "65              2018-03-02 07:35:41  1519994141     0.9774           9   \n",
       "\n",
       "    colsample_bytree        lr  max_depth  subsample  gamma  alpha    ...     \\\n",
       "9                0.9  0.096663          3       0.90      2      0    ...      \n",
       "73               0.7  0.031006          4       0.90      1      0    ...      \n",
       "65               0.8  0.068823          2       0.65      2      1    ...      \n",
       "\n",
       "    threat_best_round  insult_best_round  identity_hate_best_round  toxic_auc  \\\n",
       "9                 119                 74                        98   0.987579   \n",
       "73                273                163                       195   0.987640   \n",
       "65                100                 96                       104   0.987659   \n",
       "\n",
       "    severe_toxic_auc  obscene_auc  threat_auc  insult_auc  identity_hate_auc  \\\n",
       "9           0.991802     0.995448    0.994074    0.990130           0.991359   \n",
       "73          0.991029     0.995461    0.993971    0.990103           0.991239   \n",
       "65          0.991810     0.995460    0.992785    0.990203           0.991231   \n",
       "\n",
       "     avg_auc  \n",
       "9   0.991732  \n",
       "73  0.991574  \n",
       "65  0.991525  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search = pd.read_csv('xgb_search.csv').sort_values(by='avg_auc', ascending=False)\n",
    "\n",
    "xgb_search.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>cv_num_round</th>\n",
       "      <th>cv_nfolds</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 10:23:13</td>\n",
       "      <td>1519899793</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.092125</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987677</td>\n",
       "      <td>0.991848</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.994002</td>\n",
       "      <td>0.990178</td>\n",
       "      <td>0.991174</td>\n",
       "      <td>0.991723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 06:45:20</td>\n",
       "      <td>1519886720</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.097664</td>\n",
       "      <td>4</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987663</td>\n",
       "      <td>0.991693</td>\n",
       "      <td>0.995412</td>\n",
       "      <td>0.993793</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.991636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 08:14:47</td>\n",
       "      <td>1519892087</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.098587</td>\n",
       "      <td>3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>809</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987629</td>\n",
       "      <td>0.991796</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>0.991218</td>\n",
       "      <td>0.991635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time          id  threshold  num_models  colsample_bytree  \\\n",
       "0  2018-03-01 10:23:13  1519899793     0.9768          10               0.7   \n",
       "1  2018-03-01 06:45:20  1519886720     0.9794           6               0.8   \n",
       "2  2018-03-01 08:14:47  1519892087     0.9794           6               0.5   \n",
       "\n",
       "         lr  max_depth  subsample  gamma  alpha  cv_num_round  cv_nfolds  \\\n",
       "0  0.092125          2       0.85      2      0           962          4   \n",
       "1  0.097664          4       0.68      1      0           995          3   \n",
       "2  0.098587          3       0.77      1      0           809          4   \n",
       "\n",
       "   toxic_auc  severe_toxic_auc  obscene_auc  threat_auc  insult_auc  \\\n",
       "0   0.987677          0.991848     0.995462    0.994002    0.990178   \n",
       "1   0.987663          0.991693     0.995412    0.993793    0.990062   \n",
       "2   0.987629          0.991796     0.995422    0.993610    0.990133   \n",
       "\n",
       "   identity_hate_auc   avg_auc  \n",
       "0           0.991174  0.991723  \n",
       "1           0.991191  0.991636  \n",
       "2           0.991218  0.991635  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search_round1 = pd.read_csv('xgb_search_ori100.csv').sort_values(by='avg_auc', ascending=False)\n",
    "\n",
    "xgb_search_round1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 11:06:04, id: 1519970088, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0966631,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 607, cv_nfolds: 4,    to_pw: 10, s_pw: 103, ob_pw: 18, th_pw: 120, in_pw: 8, ih_pw: 136, to_br: 145, s_br: 74,    ob_br: 55, th_br: 119, in_br: 74, ih_br: 98            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9769336019ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#         xgb_params['scale_pos_weight'] = scale_pos_weights[label]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mgbdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_nrounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#_proba(x_test)[:,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    \n",
    "    now = get_time()\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "    # per label based params \n",
    "    best_nrounds = {}\n",
    "    best_nrounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "    best_nrounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "    best_nrounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "    best_nrounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "    best_nrounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "    best_nrounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = xgb_search['toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['severe_toxic'] = xgb_search['severe_toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['obscene'] = xgb_search['obscene_pos_scale'].values[i]\n",
    "    scale_pos_weights['threat'] = xgb_search['threat_pos_scale'].values[i]\n",
    "    scale_pos_weights['insult'] = xgb_search['insult_pos_scale'].values[i]\n",
    "    scale_pos_weights['identity_hate'] = xgb_search['identity_hate_pos_scale'].values[i]\n",
    "    \n",
    "    metric_dict_fromcsv = {}\n",
    "#     metric_dict_fromcsv['toxic'] = xgb_search['toxic_auc'].values[i]\n",
    "#     metric_dict_fromcsv['severe_toxic'] = xgb_search['severe_toxic_auc'].values[i]\n",
    "#     metric_dict_fromcsv['obscene'] = xgb_search['obscene_auc'].values[i]\n",
    "#     metric_dict_fromcsv['threat'] = xgb_search['threat_auc'].values[i]\n",
    "#     metric_dict_fromcsv['insult'] = xgb_search['insult_auc'].values[i]\n",
    "#     metric_dict_fromcsv['identity_hate'] = xgb_search['identity_hate_auc'].values[i]\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "\n",
    "    \n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d, to_br: %d, s_br: %d,\\\n",
    "    ob_br: %d, th_br: %d, in_br: %d, ih_br: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate'],\\\n",
    "              best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "              best_nrounds['obscene'],best_nrounds['threat'],\\\n",
    "              best_nrounds['insult'],best_nrounds['identity_hate']))\n",
    "\n",
    "#     print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f,\\\n",
    "#     max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d\\\n",
    "#         '%(now,search_id,model_threshold,num_models,\\\n",
    "#           xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "#           xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "#           xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds))\n",
    "        \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {}\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "#         def xg_eval_auc(yhat, dtrain):\n",
    "#             y = dtrain.get_label()\n",
    "#             return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "#         xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "#         res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "#                  early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "#         # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "#         #pdb.set_trace()\n",
    "#         best_nrounds = res.shape[0] - 1\n",
    "#         #print(res[-3:])\n",
    "#         cv_mean = res.iloc[-1, 0]\n",
    "#         cv_std = res.iloc[-1, 1]\n",
    " \n",
    "#         metric_dict[label] = cv_mean\n",
    "#         print('XGB top N training (id: {}). {}: \\t cv_mean:{} \\t cv_mean_fromcsv {} \\t best nrounds: {}\\\n",
    "#         '.format(search_id, label, cv_mean, metric_dict_fromcsv[label], best_nrounds))\n",
    "    \n",
    "        \n",
    "#         xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds[label])\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "#     avg_auc = 0\n",
    "#     for label in label_cols:\n",
    "#         avg_auc += metric_dict[label]\n",
    "#     avg_auc/=6\n",
    "    \n",
    "#     print('XGB top N training. avg_auc:{} \\t avg_auc_fromcsv {}'.format(avg_auc, metric_dict_fromcsv['avg_auc']))\n",
    "\n",
    "\n",
    "    sub_title = 'xgb_topn_'#posweighted_'\n",
    "    submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "    submission[label_cols] = result\n",
    "    submission.to_csv('./StackPreds/TopN_XGB/{}_{}_{}.csv'.format(sub_title,metric_dict_fromcsv['avg_auc'],search_id), index=False)\n",
    "        \n",
    "        \n",
    "#     val_auc = para['val_auc'].values[i]\n",
    "#     print('Model training done. Validation AUC: %.5f'%val_auc)\n",
    "\n",
    "    \n",
    "#     test_flow = dataGenerator.flow(test_embeddings + test_genre, [test_context], \\\n",
    "#             batch_size=16384, shuffle=False)\n",
    "#     test_pred = model.predict_generator(test_flow, test_flow.__len__(), workers=1)\n",
    "    \n",
    "#     test_sub = pd.DataFrame({'id': test_id, 'target': test_pred.ravel()})\n",
    "#     test_sub.to_csv('./temp_nn/nn_%.5f_%.5f_%d.csv'%(val_auc, train_loss, flag), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model_data_id_list = ['ModelName.RNN_rnn_data_001',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9774)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9760)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9723)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9752)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9726)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9761)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9803)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9793)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9794)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
