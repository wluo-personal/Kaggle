{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=True)\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from fast_text_data import fasttext_data_process\n",
    "from onevsone_data import onevsone_data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "#train = pd.read_csv('/home/kai/data/wei/Toxic/data/Shiyi_training.csv').fillna('na')\n",
    "\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "#train = train.head(1000)\n",
    "#test = test.head(1000)\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "text = pd.concat([train_sentence, test_sentence])\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ModelName(Enum):\n",
    "    XGB = 1\n",
    "    LGB = 2\n",
    "    LOGREG = 3\n",
    "    NBSVM = 4\n",
    "    RF = 5 # random forest\n",
    "    RNN = 6\n",
    "    NBLSVC = 7\n",
    "    ONESVC = 8\n",
    "    ONELOGREG = 9\n",
    "\n",
    "class BaseLayerDataRepo():\n",
    "    def __init__(self):\n",
    "        self._data_repo = {}\n",
    "    \n",
    "    def add_data(self, data_id, x_train, x_test, y_train, label_cols, compatible_model=[ModelName.LOGREG], rnn_data=False):\n",
    "        \"\"\"\n",
    "        x_train, x_test: ndarray\n",
    "        y_train: pd df\n",
    "        \"\"\"\n",
    "        temp = {}\n",
    "        \n",
    "        temp['data_id'] = data_id\n",
    "        temp['x_train'] = x_train\n",
    "        temp['x_test'] = x_test\n",
    "        temp['labes_cols'] = label_cols\n",
    "        temp['compatible_model'] = set(compatible_model)\n",
    "        \n",
    "        if rnn_data: \n",
    "            temp['y_train'] = y_train # here y_train is a df\n",
    "        else:\n",
    "            label_dict = {}\n",
    "            for col in label_cols:\n",
    "                label_dict[col] = y_train[col]\n",
    "            temp['y_train'] = label_dict # hence y_train is a dict with labels as keys\n",
    "        \n",
    "        self._data_repo[data_id] = temp\n",
    "    \n",
    "    def get_data(self, data_id):\n",
    "        return self._data_repo[data_id]\n",
    "    \n",
    "    def remove_data(self, data_id):\n",
    "        self._data_repo.pop(data_id, None)\n",
    "        \n",
    "    def get_compatible_model(self, data_id):\n",
    "        return self._data_repo[data_id]['compatible_model']\n",
    "    \n",
    "    def remove_compatible_model(self, data_id, model_name):\n",
    "        return self._data_repo[data_id]['compatible_model'].discard(model_name)\n",
    "    \n",
    "    def add_compatible_model(self, data_id, model_name):\n",
    "        return self._data_repo[data_id]['compatible_model'].add(model_name)\n",
    "                  \n",
    "    def get_data_by_compatible_model(self, model_name):\n",
    "        data_to_return = []\n",
    "        for data_id in self._data_repo.keys():\n",
    "            data = self._data_repo[data_id]\n",
    "            if model_name in data['compatible_model']:\n",
    "                data_to_return.append(data)\n",
    "        return data_to_return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data_repo)\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = ''\n",
    "        for data_id in self._data_repo.keys():\n",
    "            output+='data_id: {:20} \\n\\tx_train: {}\\tx_test: {}\\n\\ty_train type: {}\\n\\tcompatible_model: {}\\n '\\\n",
    "            .format(data_id, self._data_repo[data_id]['x_train'].shape, \\\n",
    "                    self._data_repo[data_id]['x_test'].shape, \\\n",
    "                    type(self._data_repo[data_id]['y_train']), \\\n",
    "                    self._data_repo[data_id]['compatible_model'])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bldr = BaseLayerDataRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    }
   ],
   "source": [
    "x_train_1v1, y_train_1v1, x_test_1v1, data_id_1v1 = onevsone_data_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compatible_models= [ModelName.ONESVC, ModelName.ONELOGREG]\n",
    "bldr.add_data(data_id_1v1, x_train_1v1, x_test_1v1, y_train_1v1, label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real \n",
      "\tx_train: (159571, 300000)\tx_test: (153164, 300000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.ONESVC: 8>, <ModelName.ONELOGREG: 9>}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n",
      "train shape: (159571, 27). test shape: (153164, 21)\n",
      "\n",
      "Loading FT model\n",
      "300\n",
      "window: 200. dimension(n_features): 300\n"
     ]
    }
   ],
   "source": [
    "x_train_rnn, y_train_rnn, x_test_rnn, _, _ = fasttext_data_process()#first_n_entries=100)\n",
    "\n",
    "rnn_data_id = 'rnn_data_001'\n",
    "compatible_models= [ModelName.RNN]\n",
    "bldr.add_data(rnn_data_id, x_train_rnn, x_test_rnn, y_train_rnn, label_cols, compatible_models, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: rnn_data_001         \n",
      "\tx_train: (159571, 200, 300)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 6>} \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bldr = BaseLayerDataRepo()\n",
    "\n",
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,1),(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        #min_df = i\n",
    "        #word_ngram_range = (1,1)\n",
    "        #char_ngram_range = (1,5)\n",
    "        word_max_features = 200000\n",
    "        #char_max_features = 100000\n",
    "        token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "        data_id = 'tfidf_word_df%d_ng%s_wmf%s'%(min_df,str(word_ngram_range),str(word_max_features))\n",
    "\n",
    "        word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                  min_df=1,\n",
    "                                  ngram_range=word_ngram_range,\n",
    "                                  max_features=word_max_features,\n",
    "                                  token_pattern=token_pattern,\n",
    "                                  stop_words='english',\n",
    "                                  strip_accents='unicode',\n",
    "                                  sublinear_tf=True)\n",
    "        \n",
    "        train_term_doc = word_vec.fit_transform(train.comment_text)\n",
    "        test_term_doc = word_vec.transform(test.comment_text)\n",
    "        #pdb.set_trace()\n",
    "        #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_doc)\n",
    "        #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_doc)\n",
    "    \n",
    "        compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "        bldr.add_data(data_id, train_term_doc, test_term_doc, train[label_cols], label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        for char_max_df in [0.3]:\n",
    "            #min_df = i\n",
    "            #word_ngram_range = (1,1)\n",
    "            #char_ngram_range = (1,5)\n",
    "            word_max_features = 100000\n",
    "            char_max_features = 100000\n",
    "            token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "            data_id = 'tfidf_wordchar_charmaxdf%f_ng%s_wmf%s_cmf%s'%(char_max_df,str(word_ngram_range),str(word_max_features),str(char_max_features))\n",
    "\n",
    "            word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                      min_df=1,\n",
    "                                      ngram_range=word_ngram_range,\n",
    "                                      max_features=word_max_features,\n",
    "                                      token_pattern=token_pattern,\n",
    "                                      stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "\n",
    "            char_vec = TfidfVectorizer(analyzer='char',\n",
    "                                      min_df = 1,\n",
    "                                      max_df = char_max_df,\n",
    "                                      ngram_range=(2,7), \n",
    "                                      max_features=char_max_features, \n",
    "                                      #stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "            train_word_doc = word_vec.fit_transform(train.comment_text)\n",
    "            test_word_doc = word_vec.transform(test.comment_text)\n",
    "\n",
    "            train_char_doc = char_vec.fit_transform(train.comment_text)\n",
    "            test_char_doc = char_vec.transform(test.comment_text)\n",
    "\n",
    "            train_term_tfidf = hstack((train_word_doc, train_char_doc), format='csr')\n",
    "            test_term_tfidf = hstack((test_word_doc, test_char_doc), format='csr')\n",
    "\n",
    "            #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_tfidf)\n",
    "            #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_tfidf)\n",
    "\n",
    "            compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "            bldr.add_data(data_id, train_term_tfidf, test_term_tfidf, train[label_cols], label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open(data_id_list_file, 'wb') as f:\n",
    "    pickle.dump(data_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(data_id_list_file, 'rb') as f:\n",
    "    data_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for data_id in data_ids:\n",
    "    bldr.add_data(data_id, np.load(DATA_PATH+data_id+'_x_train.npy'), np.load(DATA_PATH+data_id+'_x_test.npy'), train[label_cols], label_cols, ['logreg','gbm','rf','nbsvm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: rnn_data_001         \n",
      "\tx_train: (159571, 200, 300)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 6>} data_id: tfidf_word_df2_ng(1, 2)_wmf200000 \n",
      "\tx_train: (159571, 200000)\tx_test: (153164, 200000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} data_id: tfidf_word_df2_ng(1, 1)_wmf200000 \n",
      "\tx_train: (159571, 184719)\tx_test: (153164, 184719)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} data_id: tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 \n",
      "\tx_train: (159571, 200000)\tx_test: (153164, 200000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>} \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>}\n",
      "(159571, 200000)\n",
      "{<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>}\n",
      "(159571, 184719)\n",
      "{<ModelName.NBLSVC: 7>, <ModelName.XGB: 1>, <ModelName.LOGREG: 3>, <ModelName.LGB: 2>, <ModelName.RF: 5>, <ModelName.NBSVM: 4>}\n",
      "(159571, 200000)\n"
     ]
    }
   ],
   "source": [
    "for data in bldr.get_data_by_compatible_model(ModelName.LGB):\n",
    "    print(data['compatible_model'])\n",
    "    print(data['x_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test, nfolds):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_1v1(clf, x_train, y_train, x_test, nfolds, label):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr, label)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te, label)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test, label)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_rnn(clf, x_train, y_train, x_test, nfolds, number_labels):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,number_labels))\n",
    "    oof_test = np.zeros((ntest,number_labels))\n",
    "    oof_test_skf = np.empty((nfolds, ntest, number_labels))\n",
    "    kf = KFold(ntrain, n_folds=nfolds)#, random_state=SEED)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        ################################################################ maybe shuffle train_index\n",
    "        x_tr = x_train[train_index]\n",
    "        #pdb.set_trace()\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_base_layer_est_preds(base_layer_est_preds):\n",
    "    for key in base_layer_est_preds:\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "        submission[label_cols] = base_layer_est_preds[key]\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./BaseEstPreds/' + key + '_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model_pool = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_rnn.shape[1], x_train_rnn.shape[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_ble = RnnBLE(x_train_rnn.shape[1], x_train_rnn.shape[2], label_cols, epochs=2)\n",
    "rnn_model_pool[ModelName.RNN] = rnn_ble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.RNN rnn_data_001\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1556s 13ms/step - loss: 0.0507 - acc: 0.9814\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1552s 13ms/step - loss: 0.0440 - acc: 0.9832\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1618s 14ms/step - loss: 0.0412 - acc: 0.9838\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1600s 13ms/step - loss: 0.0387 - acc: 0.9846\n",
      "Epoch 1/2\n",
      "119678/119678 [==============================] - 1570s 13ms/step - loss: 0.0383 - acc: 0.9848\n",
      "Epoch 2/2\n",
      "119678/119678 [==============================] - 1569s 13ms/step - loss: 0.0362 - acc: 0.9853\n",
      "Epoch 1/2\n",
      "119679/119679 [==============================] - 1569s 13ms/step - loss: 0.0355 - acc: 0.9857\n",
      "Epoch 2/2\n",
      "119679/119679 [==============================] - 1565s 13ms/step - loss: 0.0337 - acc: 0.9862\n",
      "Epoch 1/2\n",
      "159571/159571 [==============================] - 2131s 13ms/step - loss: 0.0333 - acc: 0.9864\n",
      "Epoch 2/2\n",
      "159571/159571 [==============================] - 2092s 13ms/step - loss: 0.0321 - acc: 0.9869\n"
     ]
    }
   ],
   "source": [
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for model_name in rnn_model_pool.keys():\n",
    "    for data in bldr.get_data_by_compatible_model(model_name):\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "\n",
    "        SEED = 0 # for reproducibility\n",
    "        NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "        #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "\n",
    "        current_run = '{} {}'.format(model_name,data['data_id'])\n",
    "        print('Generating: '+current_run)\n",
    "\n",
    "        oof_train, oof_test = get_oof_rnn(rnn_model_pool[model_name], \\\n",
    "                                          x_train, y_train, x_test, NFOLDS, len(label_cols))\n",
    "        for i, label in enumerate(label_cols):\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train[:, i].reshape(-1,1)) # before reshape: (159571,) after: (159571, 1) => good for np.concatenate\n",
    "            layer1_oof_test[label].append(oof_test[:, i].reshape(-1,1))\n",
    "\n",
    "        model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "        model_data_id_list.append(model_data_id)\n",
    "        model = rnn_model_pool[model_name]\n",
    "        model.train(x_train, y_train) ################################ maybe shuffle x_train along with y_train?\n",
    "        est_preds = model.predict(x_test)\n",
    "\n",
    "        base_layer_est_preds[model_data_id] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519810384\n"
     ]
    }
   ],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_est_preds['ModelName.RNN_rnn_data_001'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from base_layer_estimator import OneVSOneReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsOne is using svc kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using svc kernel\n",
      "OneVsOne is using logistic kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using logistic kernel\n"
     ]
    }
   ],
   "source": [
    "onevsone_svc = OneVSOneReg(x_train_1v1, y_train_1v1, model='svc')\n",
    "onevsone_logreg = OneVSOneReg(x_train_1v1, y_train_1v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "model_pool[ModelName.ONESVC] = onevsone_svc\n",
    "model_pool[ModelName.ONELOGREG] = onevsone_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "logreg_params = {\n",
    "    'n_jobs': 3\n",
    "}\n",
    "logreg_ble = SklearnBLE(LogisticRegression, seed=SEED, params=logreg_params)\n",
    "model_pool[ModelName.LOGREG] = logreg_ble\n",
    "\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={'n_jobs': 3})\n",
    "#model_pool[ModelName.RF] = rf_ble\n",
    "\n",
    "\n",
    "nblsvc_params = {\n",
    "    'C':0.02\n",
    "}\n",
    "nblsvc_ble = NbSvmBLE(mode=ModelName.NBLSVC, seed=SEED, params=nblsvc_params)\n",
    "model_pool[ModelName.NBLSVC] = nblsvc_ble\n",
    "\n",
    "\n",
    "nbsvm_params = {\n",
    "    'C':1.0,\n",
    "    'dual':True,\n",
    "    'n_jobs':3\n",
    "}\n",
    "nbsvm_ble = NbSvmBLE(mode=ModelName.NBSVM, seed=SEED, params=nbsvm_params)\n",
    "model_pool[ModelName.NBSVM] = nbsvm_ble\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={})\n",
    "#xgb_ble = XgbBLE(params=xgb_params)\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': -1,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 8,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1\n",
    "}\n",
    "lgb_ble = LightgbmBLE(params=lgb_params)\n",
    "model_pool[ModelName.LGB] = lgb_ble\n",
    "\n",
    "\n",
    "# lg = SklearnBLE(clf=LogisticRegression, seed=SEED, params={'n_jobs': 1})\n",
    "\n",
    "# et = SklearnBLE(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "# ada = SklearnBLE(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "# gb = SklearnBLE(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "# svc = SklearnBLE(clf=SVC, seed=SEED, params=svc_params)\n",
    "\n",
    "#model_pool['rf'] = rf_ble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "########################## ONE VS ONE ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "            if model_name == ModelName.LGB:\n",
    "                model = LogisticRegression(solver='sag')\n",
    "                sfm = SelectFromModel(model, threshold='5*mean')\n",
    "                print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "                x_train = sfm.fit_transform(x_train, y_train)\n",
    "                x_test = sfm.transform(x_test)\n",
    "                print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof_1v1(model_pool[model_name],  x_train, y_train, x_test, NFOLDS, label) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train, label)\n",
    "            est_preds = model.predict(x_test, label)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 2)_wmf200000 toxic\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 1)_wmf200000 toxic\n",
      "Generating: ModelName.NBSVM tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 toxic\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 2)_wmf200000 toxic\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 5688) test:(153164, 5688)\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 1)_wmf200000 toxic\n",
      "dimension before selecting: train:(159571, 184719) test:(153164, 184719)\n",
      "dimension after selecting: train:(159571, 8238) test:(153164, 8238)\n",
      "Generating: ModelName.LGB tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 toxic\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 3477) test:(153164, 3477)\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 2)_wmf200000 toxic\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 1)_wmf200000 toxic\n",
      "Generating: ModelName.NBLSVC tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 toxic\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 2)_wmf200000 toxic\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 1)_wmf200000 toxic\n",
      "Generating: ModelName.LOGREG tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 toxic\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 2)_wmf200000 severe_toxic\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 1)_wmf200000 severe_toxic\n",
      "Generating: ModelName.NBSVM tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 severe_toxic\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 2)_wmf200000 severe_toxic\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 8362) test:(153164, 8362)\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 1)_wmf200000 severe_toxic\n",
      "dimension before selecting: train:(159571, 184719) test:(153164, 184719)\n",
      "dimension after selecting: train:(159571, 7169) test:(153164, 7169)\n",
      "Generating: ModelName.LGB tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 severe_toxic\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 6954) test:(153164, 6954)\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 2)_wmf200000 severe_toxic\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 1)_wmf200000 severe_toxic\n",
      "Generating: ModelName.NBLSVC tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 severe_toxic\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 2)_wmf200000 severe_toxic\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 1)_wmf200000 severe_toxic\n",
      "Generating: ModelName.LOGREG tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 severe_toxic\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 2)_wmf200000 obscene\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 1)_wmf200000 obscene\n",
      "Generating: ModelName.NBSVM tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 obscene\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 2)_wmf200000 obscene\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 7347) test:(153164, 7347)\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 1)_wmf200000 obscene\n",
      "dimension before selecting: train:(159571, 184719) test:(153164, 184719)\n",
      "dimension after selecting: train:(159571, 9352) test:(153164, 9352)\n",
      "Generating: ModelName.LGB tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 obscene\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 3888) test:(153164, 3888)\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 2)_wmf200000 obscene\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 1)_wmf200000 obscene\n",
      "Generating: ModelName.NBLSVC tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 obscene\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 2)_wmf200000 obscene\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 1)_wmf200000 obscene\n",
      "Generating: ModelName.LOGREG tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 obscene\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 2)_wmf200000 threat\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 1)_wmf200000 threat\n",
      "Generating: ModelName.NBSVM tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 threat\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 2)_wmf200000 threat\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 6207) test:(153164, 6207)\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 1)_wmf200000 threat\n",
      "dimension before selecting: train:(159571, 184719) test:(153164, 184719)\n",
      "dimension after selecting: train:(159571, 5470) test:(153164, 5470)\n",
      "Generating: ModelName.LGB tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 threat\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 6813) test:(153164, 6813)\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 2)_wmf200000 threat\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 1)_wmf200000 threat\n",
      "Generating: ModelName.NBLSVC tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 threat\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 2)_wmf200000 threat\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 1)_wmf200000 threat\n",
      "Generating: ModelName.LOGREG tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 threat\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 2)_wmf200000 insult\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 1)_wmf200000 insult\n",
      "Generating: ModelName.NBSVM tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 insult\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 2)_wmf200000 insult\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 7611) test:(153164, 7611)\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 1)_wmf200000 insult\n",
      "dimension before selecting: train:(159571, 184719) test:(153164, 184719)\n",
      "dimension after selecting: train:(159571, 9624) test:(153164, 9624)\n",
      "Generating: ModelName.LGB tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 insult\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 4271) test:(153164, 4271)\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 2)_wmf200000 insult\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 1)_wmf200000 insult\n",
      "Generating: ModelName.NBLSVC tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 insult\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 2)_wmf200000 insult\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 1)_wmf200000 insult\n",
      "Generating: ModelName.LOGREG tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 insult\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 2)_wmf200000 identity_hate\n",
      "Generating: ModelName.NBSVM tfidf_word_df2_ng(1, 1)_wmf200000 identity_hate\n",
      "Generating: ModelName.NBSVM tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 identity_hate\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 2)_wmf200000 identity_hate\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 8679) test:(153164, 8679)\n",
      "Generating: ModelName.LGB tfidf_word_df2_ng(1, 1)_wmf200000 identity_hate\n",
      "dimension before selecting: train:(159571, 184719) test:(153164, 184719)\n",
      "dimension after selecting: train:(159571, 7688) test:(153164, 7688)\n",
      "Generating: ModelName.LGB tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 identity_hate\n",
      "dimension before selecting: train:(159571, 200000) test:(153164, 200000)\n",
      "dimension after selecting: train:(159571, 5998) test:(153164, 5998)\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 2)_wmf200000 identity_hate\n",
      "Generating: ModelName.NBLSVC tfidf_word_df2_ng(1, 1)_wmf200000 identity_hate\n",
      "Generating: ModelName.NBLSVC tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 identity_hate\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 2)_wmf200000 identity_hate\n",
      "Generating: ModelName.LOGREG tfidf_word_df2_ng(1, 1)_wmf200000 identity_hate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.LOGREG tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000 identity_hate\n"
     ]
    }
   ],
   "source": [
    "# base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "# layer1_oof_train = {}\n",
    "# layer1_oof_test = {}\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "            if model_name == ModelName.LGB:\n",
    "                model = LogisticRegression(solver='sag')\n",
    "                sfm = SelectFromModel(model, threshold='5*mean')\n",
    "                print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "                x_train = sfm.fit_transform(x_train, y_train)\n",
    "                x_test = sfm.transform(x_test)\n",
    "                print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof(model_pool[model_name],  x_train, y_train, x_test, NFOLDS) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train)\n",
    "            est_preds = model.predict(x_test)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519871930\n",
      "1519871931\n"
     ]
    }
   ],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(layer1_oof_train) # list keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(layer1_oof_train['toxic']) # number of models to stack (each model will predict one set of toxic, servere_toxic, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(layer1_oof_train['toxic'][0]) # examples in oof_train (meta features, x_train) (meta labels are in train[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(layer1_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(layer1_oof_test['toxic'][0]) # examples in oof_test (will be used by meta model (after validation) to predict the final prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before we choose which models to assemble, we can do:\n",
    "#### 1. scatter plot analysis to check the diversity\n",
    "#### 2. submit to check if the models have similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_layer_oof_per_label(layer1_oof_dict, label):\n",
    "    x = None\n",
    "    data_list = layer1_oof_dict[label]\n",
    "    for i in range(len(data_list)):\n",
    "        if i == 0:\n",
    "            x = data_list[0]\n",
    "        else:\n",
    "            x = np.concatenate((x, data_list[i]), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simple blend of two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "result = np.empty((test.shape[0],len(label_cols)))\n",
    "\n",
    "# mix the first two models\n",
    "for i, label in enumerate(label_cols):\n",
    "    x_train = combine_layer_oof_per_label(layer1_oof_train, label)\n",
    "    x_test = combine_layer_oof_per_label(layer1_oof_test, label)\n",
    "    for j in range(x_train.shape[1]):\n",
    "        roc = roc_auc_score(train[label], x_train[:,j])\n",
    "        print(label, j, roc) # print out roc for meta feature on meta label (which is just the original train label)\n",
    "    \n",
    "    roc_scores_of_a_label = []\n",
    "    alphas = np.linspace(0,1,1001)\n",
    "    best_roc = 0\n",
    "    best_alpha = 0\n",
    "    for alpha in alphas:\n",
    "        roc = roc_auc_score(train[label], alpha*x_train[:,0] + (1-alpha)*x_train[:,1])\n",
    "        if roc > best_roc:\n",
    "            best_roc = roc\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(label, best_roc, best_alpha)\n",
    "    result[:,i] = best_alpha*x_test[:,0] + (1-best_alpha)*x_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "submission[label_cols] = result\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./StackPreds/mixtwo_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insult', 'toxic', 'obscene', 'threat', 'severe_toxic', 'identity_hate']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()#load_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
       " 'ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo.add(layer1_oof_train, layer1_oof_test, base_layer_est_preds, model_data_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_threshold = np.random.choice([0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0 update to 0.9818\n"
     ]
    }
   ],
   "source": [
    "#base_layer_results_repo.add_score('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.9818)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_layer_results_repo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=0.9774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_train_loaded['toxic']) # number of models that will be stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(layer1_oof_train_temp[label_cols[5]]) == len(layer1_oof_test_temp[label_cols[5]]) == len(list(base_layer_est_preds_temp)) == 1\n",
    "                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 27)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40881912"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(time.time()* 1000000 % 45234634)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('./xgb_search.csv', 'a')\n",
    "header = 'time,id,threshold,num_models,colsample_bytree,lr,max_depth,subsample,\\\n",
    "gamma,alpha,cv_num_round,cv_nfolds,toxic_best_round,severe_toxic_best_round,\\\n",
    "obscene_best_round,threat_best_round,insult_best_round,identity_hate_best_round,\\\n",
    "toxic_auc,severe_toxic_auc,obscene_auc,threat_auc,insult_auc,identity_hate_auc,avg_auc\\n'\n",
    "f.write(header)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-212-a449875801c1>(9)<module>()\n",
      "-> model_threshold = np.random.choice([0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
      "(Pdb) n\n",
      "> <ipython-input-212-a449875801c1>(10)<module>()\n",
      "-> layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
      "(Pdb) unt 19\n",
      "> <ipython-input-212-a449875801c1>(20)<module>()\n",
      "-> xgb_cv_seed = 0\n",
      "(Pdb) model_threshold\n",
      "0.9768\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-a449875801c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mxgb_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mxgb_cv_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mxgb_cv_num_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mxgb_cv_nfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-a449875801c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mxgb_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mxgb_cv_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mxgb_cv_num_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mxgb_cv_nfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import datetime, time, gc\n",
    "\n",
    "for i in range(100):\n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    model_threshold = np.random.choice([0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "        \n",
    "    xgb_colsample_bytree = np.random.randint(5, 10)/10\n",
    "    xgb_learning_rate = 1e-2 * (0.1 ** (np.random.rand() * 2 - 1.0)) # 0.001 to 0.0997\n",
    "    xgb_max_depth = np.random.randint(2, 8)\n",
    "    xgb_subsample = np.random.randint(50, 100)/100\n",
    "    xgb_gamma = np.random.randint(0, 3)\n",
    "    xgb_alpha = np.random.randint(0, 2)\n",
    "\n",
    "    xgb_cv_seed = 0\n",
    "    xgb_cv_num_round = np.random.randint(400, 1000)\n",
    "    xgb_cv_nfolds = np.random.randint(3,5)\n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    num_models = len(layer1_oof_train_loaded['toxic'])\n",
    "    #print('Stacking {} models'.format(num_models)) # number of models that will be stacked\n",
    "    \n",
    "    search_id = int(time.time())\n",
    "\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f,\\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds))\n",
    "    \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {} # all labels\n",
    "    best_rounds = {}  # all labels\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "    #     clf = XGBClassifier()\n",
    "\n",
    "    #     #scores = cross_val_score(clf, x_train, train[label], cv=3, scoring='roc_auc')\n",
    "\n",
    "    #     #print(scores)\n",
    "    #     #print(\"Stacking-CV: ROC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    #     clf.fit(x_train, train[label])\n",
    "\n",
    "    #     result[:, i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds_for_current_label = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "\n",
    "        #print('Ensemble-CV: {}: {}+{}'.format(label, cv_mean, cv_std))\n",
    "        metric_dict[label] = cv_mean\n",
    "        best_rounds[label] = best_nrounds_for_current_label\n",
    "        #metric_dict[label]['cv_mean'] = cv_mean\n",
    "        #metric_dict[label]['cv_std'] = cv_std\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds_for_current_label)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "          \n",
    "    res = '%s,%d,%f,%d,%f,%.7f,%d,%f,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f\\n\\\n",
    "            '%(now,search_id,model_threshold,num_models,xgb_colsample_bytree,\\\n",
    "               xgb_learning_rate,xgb_max_depth,xgb_subsample,xgb_gamma,xgb_alpha,\\\n",
    "               xgb_cv_num_round,xgb_cv_nfolds,best_rounds['toxic'],best_rounds['severe_toxic'],\\\n",
    "               best_rounds['obscene'],best_rounds['threat'],best_rounds['insult'],\\\n",
    "               best_rounds['identity_hate'],metric_dict['toxic'],metric_dict['severe_toxic'],\\\n",
    "               metric_dict['obscene'],metric_dict['threat'],metric_dict['insult'],\\\n",
    "               metric_dict['identity_hate'],avg_auc)\n",
    "\n",
    "    f = open('./xgb_search.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()\n",
    "\n",
    "#     sub_tile = 'stacking_test_'\n",
    "#     submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "#     submission[label_cols] = result\n",
    "#     submission.to_csv('./StackPreds/' + sub_tile + str(search_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb random search top N training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_search = pd.read_csv('xgb_search.csv').sort_values(by='avg_auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>cv_num_round</th>\n",
       "      <th>cv_nfolds</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 10:23:13</td>\n",
       "      <td>1519899793</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.092125</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987677</td>\n",
       "      <td>0.991848</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.994002</td>\n",
       "      <td>0.990178</td>\n",
       "      <td>0.991174</td>\n",
       "      <td>0.991723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 06:45:20</td>\n",
       "      <td>1519886720</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.097664</td>\n",
       "      <td>4</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987663</td>\n",
       "      <td>0.991693</td>\n",
       "      <td>0.995412</td>\n",
       "      <td>0.993793</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.991636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 08:14:47</td>\n",
       "      <td>1519892087</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.098587</td>\n",
       "      <td>3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>809</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987629</td>\n",
       "      <td>0.991796</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>0.991218</td>\n",
       "      <td>0.991635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time          id  threshold  num_models  colsample_bytree  \\\n",
       "0  2018-03-01 10:23:13  1519899793     0.9768          10               0.7   \n",
       "1  2018-03-01 06:45:20  1519886720     0.9794           6               0.8   \n",
       "2  2018-03-01 08:14:47  1519892087     0.9794           6               0.5   \n",
       "\n",
       "         lr  max_depth  subsample  gamma  alpha  cv_num_round  cv_nfolds  \\\n",
       "0  0.092125          2       0.85      2      0           962          4   \n",
       "1  0.097664          4       0.68      1      0           995          3   \n",
       "2  0.098587          3       0.77      1      0           809          4   \n",
       "\n",
       "   toxic_auc  severe_toxic_auc  obscene_auc  threat_auc  insult_auc  \\\n",
       "0   0.987677          0.991848     0.995462    0.994002    0.990178   \n",
       "1   0.987663          0.991693     0.995412    0.993793    0.990062   \n",
       "2   0.987629          0.991796     0.995422    0.993610    0.990133   \n",
       "\n",
       "   identity_hate_auc   avg_auc  \n",
       "0           0.991174  0.991723  \n",
       "1           0.991191  0.991636  \n",
       "2           0.991218  0.991635  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-01 16:22:48, id: 1519899793, th: 0.976800, num_models: 10, colsample_bytree: 0.700000, lr: 0.0921252,    max_depth: 2, subsample: 0.850000, gamma: 2, alpha: 0, cv_num_round: 962, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519899793). toxic: \t cv_mean:0.9876772500000001 \t cv_mean_fromcsv 0.98767725 \t best nrounds: 165        \n",
      "XGB top N training (id: 1519899793). severe_toxic: \t cv_mean:0.9918480000000001 \t cv_mean_fromcsv 0.991848 \t best nrounds: 73        \n",
      "XGB top N training (id: 1519899793). obscene: \t cv_mean:0.995462 \t cv_mean_fromcsv 0.9954620000000001 \t best nrounds: 117        \n",
      "XGB top N training (id: 1519899793). threat: \t cv_mean:0.994002 \t cv_mean_fromcsv 0.994002 \t best nrounds: 136        \n",
      "XGB top N training (id: 1519899793). insult: \t cv_mean:0.99017775 \t cv_mean_fromcsv 0.99017775 \t best nrounds: 100        \n",
      "XGB top N training (id: 1519899793). identity_hate: \t cv_mean:0.9911735 \t cv_mean_fromcsv 0.9911735 \t best nrounds: 107        \n",
      "XGB top N training. avg_auc:0.9917234166666667 \t avg_auc_fromcsv 0.99172342\n",
      "time: 2018-03-01 16:25:03, id: 1519886720, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0976641,    max_depth: 4, subsample: 0.680000, gamma: 1, alpha: 0, cv_num_round: 995, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519886720). toxic: \t cv_mean:0.9876626666666667 \t cv_mean_fromcsv 0.98766267 \t best nrounds: 73        \n",
      "XGB top N training (id: 1519886720). severe_toxic: \t cv_mean:0.9916933333333334 \t cv_mean_fromcsv 0.99169333 \t best nrounds: 79        \n",
      "XGB top N training (id: 1519886720). obscene: \t cv_mean:0.995412 \t cv_mean_fromcsv 0.9954120000000001 \t best nrounds: 78        \n",
      "XGB top N training (id: 1519886720). threat: \t cv_mean:0.993793 \t cv_mean_fromcsv 0.993793 \t best nrounds: 122        \n",
      "XGB top N training (id: 1519886720). insult: \t cv_mean:0.990062 \t cv_mean_fromcsv 0.9900620000000001 \t best nrounds: 63        \n",
      "XGB top N training (id: 1519886720). identity_hate: \t cv_mean:0.9911909999999998 \t cv_mean_fromcsv 0.9911909999999999 \t best nrounds: 86        \n",
      "XGB top N training. avg_auc:0.9916356666666667 \t avg_auc_fromcsv 0.99163567\n",
      "time: 2018-03-01 16:26:13, id: 1519892087, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0985875,    max_depth: 3, subsample: 0.770000, gamma: 1, alpha: 0, cv_num_round: 809, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519892087). toxic: \t cv_mean:0.98762925 \t cv_mean_fromcsv 0.98762925 \t best nrounds: 108        \n",
      "XGB top N training (id: 1519892087). severe_toxic: \t cv_mean:0.9917959999999999 \t cv_mean_fromcsv 0.9917959999999999 \t best nrounds: 77        \n",
      "XGB top N training (id: 1519892087). obscene: \t cv_mean:0.995422 \t cv_mean_fromcsv 0.9954219999999999 \t best nrounds: 103        \n",
      "XGB top N training (id: 1519892087). threat: \t cv_mean:0.9936105 \t cv_mean_fromcsv 0.9936105 \t best nrounds: 99        \n",
      "XGB top N training (id: 1519892087). insult: \t cv_mean:0.99013325 \t cv_mean_fromcsv 0.99013325 \t best nrounds: 120        \n",
      "XGB top N training (id: 1519892087). identity_hate: \t cv_mean:0.99121775 \t cv_mean_fromcsv 0.99121775 \t best nrounds: 84        \n",
      "XGB top N training. avg_auc:0.9916347916666667 \t avg_auc_fromcsv 0.99163479\n",
      "time: 2018-03-01 16:27:55, id: 1519896542, th: 0.978600, num_models: 8, colsample_bytree: 0.900000, lr: 0.0793773,    max_depth: 4, subsample: 0.660000, gamma: 0, alpha: 0, cv_num_round: 526, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519896542). toxic: \t cv_mean:0.9876143333333333 \t cv_mean_fromcsv 0.98761433 \t best nrounds: 95        \n",
      "XGB top N training (id: 1519896542). severe_toxic: \t cv_mean:0.9917366666666667 \t cv_mean_fromcsv 0.99173667 \t best nrounds: 87        \n",
      "XGB top N training (id: 1519896542). obscene: \t cv_mean:0.9954316666666667 \t cv_mean_fromcsv 0.99543167 \t best nrounds: 87        \n",
      "XGB top N training (id: 1519896542). threat: \t cv_mean:0.9938320000000002 \t cv_mean_fromcsv 0.9938319999999999 \t best nrounds: 129        \n",
      "XGB top N training (id: 1519896542). insult: \t cv_mean:0.9901446666666667 \t cv_mean_fromcsv 0.99014467 \t best nrounds: 109        \n",
      "XGB top N training (id: 1519896542). identity_hate: \t cv_mean:0.9909545 \t cv_mean_fromcsv 0.9909545 \t best nrounds: 93        \n",
      "XGB top N training. avg_auc:0.9916189722222222 \t avg_auc_fromcsv 0.99161897\n",
      "time: 2018-03-01 16:29:15, id: 1519888479, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0398248,    max_depth: 4, subsample: 0.900000, gamma: 1, alpha: 0, cv_num_round: 455, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519888479). toxic: \t cv_mean:0.987727 \t cv_mean_fromcsv 0.987727 \t best nrounds: 264        \n",
      "XGB top N training (id: 1519888479). severe_toxic: \t cv_mean:0.99084825 \t cv_mean_fromcsv 0.99084825 \t best nrounds: 60        \n",
      "XGB top N training (id: 1519888479). obscene: \t cv_mean:0.9955075 \t cv_mean_fromcsv 0.9955075000000001 \t best nrounds: 179        \n",
      "XGB top N training (id: 1519888479). threat: \t cv_mean:0.9939672500000001 \t cv_mean_fromcsv 0.9939645 \t best nrounds: 250        \n",
      "XGB top N training (id: 1519888479). insult: \t cv_mean:0.99018625 \t cv_mean_fromcsv 0.99018625 \t best nrounds: 180        \n",
      "XGB top N training (id: 1519888479). identity_hate: \t cv_mean:0.99143675 \t cv_mean_fromcsv 0.9914364999999999 \t best nrounds: 155        \n",
      "XGB top N training. avg_auc:0.9916121666666666 \t avg_auc_fromcsv 0.99161167\n",
      "time: 2018-03-01 16:32:19, id: 1519897796, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0852665,    max_depth: 3, subsample: 0.520000, gamma: 2, alpha: 0, cv_num_round: 672, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519897796). toxic: \t cv_mean:0.9876849999999999 \t cv_mean_fromcsv 0.987685 \t best nrounds: 126        \n",
      "XGB top N training (id: 1519897796). severe_toxic: \t cv_mean:0.991855 \t cv_mean_fromcsv 0.9918549999999999 \t best nrounds: 69        \n",
      "XGB top N training (id: 1519897796). obscene: \t cv_mean:0.9954688333333334 \t cv_mean_fromcsv 0.99546883 \t best nrounds: 95        \n",
      "XGB top N training (id: 1519897796). threat: \t cv_mean:0.9933763333333333 \t cv_mean_fromcsv 0.99337633 \t best nrounds: 138        \n",
      "XGB top N training (id: 1519897796). insult: \t cv_mean:0.9901673333333333 \t cv_mean_fromcsv 0.99016733 \t best nrounds: 97        \n",
      "XGB top N training (id: 1519897796). identity_hate: \t cv_mean:0.9910423333333335 \t cv_mean_fromcsv 0.99104233 \t best nrounds: 81        \n",
      "XGB top N training. avg_auc:0.9915991388888888 \t avg_auc_fromcsv 0.99159914\n",
      "time: 2018-03-01 16:33:36, id: 1519900333, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0490772,    max_depth: 3, subsample: 0.880000, gamma: 1, alpha: 1, cv_num_round: 587, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519900333). toxic: \t cv_mean:0.9876326666666667 \t cv_mean_fromcsv 0.98763267 \t best nrounds: 317        \n",
      "XGB top N training (id: 1519900333). severe_toxic: \t cv_mean:0.9914663333333333 \t cv_mean_fromcsv 0.99146633 \t best nrounds: 101        \n",
      "XGB top N training (id: 1519900333). obscene: \t cv_mean:0.9953890000000002 \t cv_mean_fromcsv 0.995389 \t best nrounds: 184        \n",
      "XGB top N training (id: 1519900333). threat: \t cv_mean:0.9935439999999999 \t cv_mean_fromcsv 0.9935440000000001 \t best nrounds: 211        \n",
      "XGB top N training (id: 1519900333). insult: \t cv_mean:0.990114 \t cv_mean_fromcsv 0.9901139999999999 \t best nrounds: 298        \n",
      "XGB top N training (id: 1519900333). identity_hate: \t cv_mean:0.9911979999999999 \t cv_mean_fromcsv 0.9911979999999999 \t best nrounds: 186        \n",
      "XGB top N training. avg_auc:0.9915573333333333 \t avg_auc_fromcsv 0.99155733\n",
      "time: 2018-03-01 16:36:04, id: 1519889074, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0841977,    max_depth: 2, subsample: 0.620000, gamma: 2, alpha: 0, cv_num_round: 636, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519889074). toxic: \t cv_mean:0.98765525 \t cv_mean_fromcsv 0.98765525 \t best nrounds: 228        \n",
      "XGB top N training (id: 1519889074). severe_toxic: \t cv_mean:0.9917935 \t cv_mean_fromcsv 0.9917935 \t best nrounds: 161        \n",
      "XGB top N training (id: 1519889074). obscene: \t cv_mean:0.995408 \t cv_mean_fromcsv 0.995408 \t best nrounds: 162        \n",
      "XGB top N training (id: 1519889074). threat: \t cv_mean:0.99305575 \t cv_mean_fromcsv 0.99305575 \t best nrounds: 115        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB top N training (id: 1519889074). insult: \t cv_mean:0.9901372500000001 \t cv_mean_fromcsv 0.99013725 \t best nrounds: 156        \n",
      "XGB top N training (id: 1519889074). identity_hate: \t cv_mean:0.9912777500000001 \t cv_mean_fromcsv 0.99127775 \t best nrounds: 106        \n",
      "XGB top N training. avg_auc:0.9915545833333334 \t avg_auc_fromcsv 0.99155458\n",
      "time: 2018-03-01 16:38:22, id: 1519890027, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0465127,    max_depth: 2, subsample: 0.620000, gamma: 0, alpha: 0, cv_num_round: 798, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519890027). toxic: \t cv_mean:0.9874896666666667 \t cv_mean_fromcsv 0.98748967 \t best nrounds: 158        \n",
      "XGB top N training (id: 1519890027). severe_toxic: \t cv_mean:0.9917513333333333 \t cv_mean_fromcsv 0.99175133 \t best nrounds: 177        \n",
      "XGB top N training (id: 1519890027). obscene: \t cv_mean:0.9953436666666667 \t cv_mean_fromcsv 0.99534367 \t best nrounds: 118        \n",
      "XGB top N training (id: 1519890027). threat: \t cv_mean:0.9933346666666667 \t cv_mean_fromcsv 0.99333467 \t best nrounds: 188        \n",
      "XGB top N training (id: 1519890027). insult: \t cv_mean:0.9900236666666666 \t cv_mean_fromcsv 0.99002367 \t best nrounds: 161        \n",
      "XGB top N training (id: 1519890027). identity_hate: \t cv_mean:0.991291 \t cv_mean_fromcsv 0.991291 \t best nrounds: 151        \n",
      "XGB top N training. avg_auc:0.9915390000000001 \t avg_auc_fromcsv 0.991539\n",
      "time: 2018-03-01 16:40:02, id: 1519893827, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0336228,    max_depth: 4, subsample: 0.740000, gamma: 2, alpha: 1, cv_num_round: 859, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519893827). toxic: \t cv_mean:0.9876834999999999 \t cv_mean_fromcsv 0.9876834999999999 \t best nrounds: 261        \n",
      "XGB top N training (id: 1519893827). severe_toxic: \t cv_mean:0.9908537500000001 \t cv_mean_fromcsv 0.99085375 \t best nrounds: 128        \n",
      "XGB top N training (id: 1519893827). obscene: \t cv_mean:0.99546 \t cv_mean_fromcsv 0.99546 \t best nrounds: 262        \n",
      "XGB top N training (id: 1519893827). threat: \t cv_mean:0.99322875 \t cv_mean_fromcsv 0.99322875 \t best nrounds: 223        \n",
      "XGB top N training (id: 1519893827). insult: \t cv_mean:0.9901570000000001 \t cv_mean_fromcsv 0.990157 \t best nrounds: 312        \n",
      "XGB top N training (id: 1519893827). identity_hate: \t cv_mean:0.990831 \t cv_mean_fromcsv 0.9908309999999999 \t best nrounds: 178        \n",
      "XGB top N training. avg_auc:0.9913690000000001 \t avg_auc_fromcsv 0.9913690000000001\n",
      "time: 2018-03-01 16:45:25, id: 1519901223, th: 0.978600, num_models: 8, colsample_bytree: 0.900000, lr: 0.0856698,    max_depth: 5, subsample: 0.740000, gamma: 1, alpha: 1, cv_num_round: 612, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519901223). toxic: \t cv_mean:0.9876 \t cv_mean_fromcsv 0.9876 \t best nrounds: 99        \n",
      "XGB top N training (id: 1519901223). severe_toxic: \t cv_mean:0.9916010000000001 \t cv_mean_fromcsv 0.9916010000000001 \t best nrounds: 93        \n",
      "XGB top N training (id: 1519901223). obscene: \t cv_mean:0.9954126666666666 \t cv_mean_fromcsv 0.99541267 \t best nrounds: 82        \n",
      "XGB top N training (id: 1519901223). threat: \t cv_mean:0.992581 \t cv_mean_fromcsv 0.9925809999999999 \t best nrounds: 75        \n",
      "XGB top N training (id: 1519901223). insult: \t cv_mean:0.990119 \t cv_mean_fromcsv 0.9901190000000001 \t best nrounds: 77        \n",
      "XGB top N training (id: 1519901223). identity_hate: \t cv_mean:0.9908906666666666 \t cv_mean_fromcsv 0.99089067 \t best nrounds: 86        \n",
      "XGB top N training. avg_auc:0.9913673888888889 \t avg_auc_fromcsv 0.99136739\n",
      "time: 2018-03-01 16:47:25, id: 1519890993, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0938080,    max_depth: 7, subsample: 0.690000, gamma: 0, alpha: 1, cv_num_round: 518, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519890993). toxic: \t cv_mean:0.9873996666666667 \t cv_mean_fromcsv 0.98739967 \t best nrounds: 63        \n",
      "XGB top N training (id: 1519890993). severe_toxic: \t cv_mean:0.9913693333333334 \t cv_mean_fromcsv 0.99136933 \t best nrounds: 82        \n",
      "XGB top N training (id: 1519890993). obscene: \t cv_mean:0.9952873333333333 \t cv_mean_fromcsv 0.99528733 \t best nrounds: 59        \n",
      "XGB top N training (id: 1519890993). threat: \t cv_mean:0.9933690000000001 \t cv_mean_fromcsv 0.9933690000000001 \t best nrounds: 104        \n",
      "XGB top N training (id: 1519890993). insult: \t cv_mean:0.9899919999999999 \t cv_mean_fromcsv 0.989992 \t best nrounds: 70        \n",
      "XGB top N training (id: 1519890993). identity_hate: \t cv_mean:0.9906903333333332 \t cv_mean_fromcsv 0.99069033 \t best nrounds: 76        \n",
      "XGB top N training. avg_auc:0.9913512777777777 \t avg_auc_fromcsv 0.99135128\n",
      "time: 2018-03-01 16:49:27, id: 1519900859, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0294727,    max_depth: 4, subsample: 0.680000, gamma: 1, alpha: 0, cv_num_round: 867, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519900859). toxic: \t cv_mean:0.987685 \t cv_mean_fromcsv 0.987685 \t best nrounds: 259        \n",
      "XGB top N training (id: 1519900859). severe_toxic: \t cv_mean:0.99091325 \t cv_mean_fromcsv 0.99091325 \t best nrounds: 86        \n",
      "XGB top N training (id: 1519900859). obscene: \t cv_mean:0.99547575 \t cv_mean_fromcsv 0.99547575 \t best nrounds: 221        \n",
      "XGB top N training (id: 1519900859). threat: \t cv_mean:0.9927555 \t cv_mean_fromcsv 0.9927555 \t best nrounds: 169        \n",
      "XGB top N training (id: 1519900859). insult: \t cv_mean:0.990121 \t cv_mean_fromcsv 0.9901209999999999 \t best nrounds: 174        \n",
      "XGB top N training (id: 1519900859). identity_hate: \t cv_mean:0.99113625 \t cv_mean_fromcsv 0.99113625 \t best nrounds: 178        \n",
      "XGB top N training. avg_auc:0.9913477916666666 \t avg_auc_fromcsv 0.99134779\n",
      "time: 2018-03-01 16:52:56, id: 1519892523, th: 0.976800, num_models: 10, colsample_bytree: 0.800000, lr: 0.0786296,    max_depth: 7, subsample: 0.710000, gamma: 2, alpha: 1, cv_num_round: 591, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519892523). toxic: \t cv_mean:0.9875563333333334 \t cv_mean_fromcsv 0.98755633 \t best nrounds: 86        \n",
      "XGB top N training (id: 1519892523). severe_toxic: \t cv_mean:0.9915983333333332 \t cv_mean_fromcsv 0.99159833 \t best nrounds: 85        \n",
      "XGB top N training (id: 1519892523). obscene: \t cv_mean:0.9953936666666666 \t cv_mean_fromcsv 0.99539367 \t best nrounds: 73        \n",
      "XGB top N training (id: 1519892523). threat: \t cv_mean:0.9924590000000001 \t cv_mean_fromcsv 0.992459 \t best nrounds: 82        \n",
      "XGB top N training (id: 1519892523). insult: \t cv_mean:0.9900509999999999 \t cv_mean_fromcsv 0.990051 \t best nrounds: 85        \n",
      "XGB top N training (id: 1519892523). identity_hate: \t cv_mean:0.9909910000000001 \t cv_mean_fromcsv 0.990991 \t best nrounds: 106        \n",
      "XGB top N training. avg_auc:0.9913415555555556 \t avg_auc_fromcsv 0.99134156\n",
      "time: 2018-03-01 16:55:04, id: 1519891076, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0468944,    max_depth: 2, subsample: 0.820000, gamma: 2, alpha: 1, cv_num_round: 552, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519891076). toxic: \t cv_mean:0.9876897499999999 \t cv_mean_fromcsv 0.98768975 \t best nrounds: 488        \n",
      "XGB top N training (id: 1519891076). severe_toxic: \t cv_mean:0.9907375 \t cv_mean_fromcsv 0.9907375 \t best nrounds: 68        \n",
      "XGB top N training (id: 1519891076). obscene: \t cv_mean:0.9954350000000001 \t cv_mean_fromcsv 0.9954350000000001 \t best nrounds: 315        \n",
      "XGB top N training (id: 1519891076). threat: \t cv_mean:0.9927785 \t cv_mean_fromcsv 0.9927785 \t best nrounds: 142        \n",
      "XGB top N training (id: 1519891076). insult: \t cv_mean:0.99014475 \t cv_mean_fromcsv 0.99014475 \t best nrounds: 315        \n",
      "XGB top N training (id: 1519891076). identity_hate: \t cv_mean:0.9912635000000001 \t cv_mean_fromcsv 0.9912635000000001 \t best nrounds: 310        \n",
      "XGB top N training. avg_auc:0.9913414999999999 \t avg_auc_fromcsv 0.9913415000000001\n",
      "time: 2018-03-01 16:59:34, id: 1519891515, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0287715,    max_depth: 2, subsample: 0.510000, gamma: 1, alpha: 0, cv_num_round: 689, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519891515). toxic: \t cv_mean:0.987655 \t cv_mean_fromcsv 0.987655 \t best nrounds: 464        \n",
      "XGB top N training (id: 1519891515). severe_toxic: \t cv_mean:0.99078025 \t cv_mean_fromcsv 0.99078025 \t best nrounds: 85        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB top N training (id: 1519891515). obscene: \t cv_mean:0.9954445 \t cv_mean_fromcsv 0.9954445 \t best nrounds: 367        \n",
      "XGB top N training (id: 1519891515). threat: \t cv_mean:0.9925642499999999 \t cv_mean_fromcsv 0.99256425 \t best nrounds: 202        \n",
      "XGB top N training (id: 1519891515). insult: \t cv_mean:0.9901135 \t cv_mean_fromcsv 0.9901135 \t best nrounds: 229        \n",
      "XGB top N training (id: 1519891515). identity_hate: \t cv_mean:0.99126925 \t cv_mean_fromcsv 0.99126925 \t best nrounds: 214        \n",
      "XGB top N training. avg_auc:0.9913044583333334 \t avg_auc_fromcsv 0.99130446\n",
      "time: 2018-03-01 17:04:04, id: 1519884412, th: 0.980300, num_models: 5, colsample_bytree: 0.800000, lr: 0.0515072,    max_depth: 4, subsample: 0.520000, gamma: 2, alpha: 1, cv_num_round: 634, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519884412). toxic: \t cv_mean:0.9876025 \t cv_mean_fromcsv 0.9876024999999999 \t best nrounds: 177        \n",
      "XGB top N training (id: 1519884412). severe_toxic: \t cv_mean:0.99180575 \t cv_mean_fromcsv 0.99180575 \t best nrounds: 142        \n",
      "XGB top N training (id: 1519884412). obscene: \t cv_mean:0.99536275 \t cv_mean_fromcsv 0.99536275 \t best nrounds: 118        \n",
      "XGB top N training (id: 1519884412). threat: \t cv_mean:0.9918125 \t cv_mean_fromcsv 0.9918125 \t best nrounds: 124        \n",
      "XGB top N training (id: 1519884412). insult: \t cv_mean:0.990031 \t cv_mean_fromcsv 0.990031 \t best nrounds: 131        \n",
      "XGB top N training (id: 1519884412). identity_hate: \t cv_mean:0.991178875 \t cv_mean_fromcsv 0.991179 \t best nrounds: 137        \n",
      "XGB top N training. avg_auc:0.9912988958333333 \t avg_auc_fromcsv 0.99129892\n",
      "time: 2018-03-01 17:07:17, id: 1519894544, th: 0.980300, num_models: 5, colsample_bytree: 0.500000, lr: 0.0723110,    max_depth: 2, subsample: 0.560000, gamma: 2, alpha: 1, cv_num_round: 514, cv_nfolds: 3        \n",
      "XGB top N training (id: 1519894544). toxic: \t cv_mean:0.987534 \t cv_mean_fromcsv 0.987534 \t best nrounds: 303        \n",
      "XGB top N training (id: 1519894544). severe_toxic: \t cv_mean:0.9916936666666668 \t cv_mean_fromcsv 0.99169367 \t best nrounds: 193        \n",
      "XGB top N training (id: 1519894544). obscene: \t cv_mean:0.9952976666666666 \t cv_mean_fromcsv 0.99529767 \t best nrounds: 223        \n",
      "XGB top N training (id: 1519894544). threat: \t cv_mean:0.9919698333333334 \t cv_mean_fromcsv 0.99196983 \t best nrounds: 114        \n",
      "XGB top N training (id: 1519894544). insult: \t cv_mean:0.9899703333333334 \t cv_mean_fromcsv 0.98997033 \t best nrounds: 282        \n",
      "XGB top N training (id: 1519894544). identity_hate: \t cv_mean:0.9911919999999999 \t cv_mean_fromcsv 0.991192 \t best nrounds: 245        \n",
      "XGB top N training. avg_auc:0.99127625 \t avg_auc_fromcsv 0.99127625\n",
      "time: 2018-03-01 17:10:14, id: 1519895544, th: 0.980300, num_models: 5, colsample_bytree: 0.500000, lr: 0.0639420,    max_depth: 4, subsample: 0.620000, gamma: 1, alpha: 1, cv_num_round: 982, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519895544). toxic: \t cv_mean:0.9875195 \t cv_mean_fromcsv 0.9875195 \t best nrounds: 175        \n",
      "XGB top N training (id: 1519895544). severe_toxic: \t cv_mean:0.9917050000000001 \t cv_mean_fromcsv 0.991705 \t best nrounds: 149        \n",
      "XGB top N training (id: 1519895544). obscene: \t cv_mean:0.99531925 \t cv_mean_fromcsv 0.99531925 \t best nrounds: 151        \n",
      "XGB top N training (id: 1519895544). threat: \t cv_mean:0.992076 \t cv_mean_fromcsv 0.9920760000000001 \t best nrounds: 127        \n",
      "XGB top N training (id: 1519895544). insult: \t cv_mean:0.9899245 \t cv_mean_fromcsv 0.9899245000000001 \t best nrounds: 200        \n",
      "XGB top N training (id: 1519895544). identity_hate: \t cv_mean:0.9910209999999999 \t cv_mean_fromcsv 0.9910209999999999 \t best nrounds: 142        \n",
      "XGB top N training. avg_auc:0.991260875 \t avg_auc_fromcsv 0.99126088\n",
      "time: 2018-03-01 17:13:11, id: 1519889225, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0391968,    max_depth: 2, subsample: 0.610000, gamma: 1, alpha: 0, cv_num_round: 803, cv_nfolds: 4        \n",
      "XGB top N training (id: 1519889225). toxic: \t cv_mean:0.9877445 \t cv_mean_fromcsv 0.9877445 \t best nrounds: 520        \n",
      "XGB top N training (id: 1519889225). severe_toxic: \t cv_mean:0.99083775 \t cv_mean_fromcsv 0.99083775 \t best nrounds: 65        \n",
      "XGB top N training (id: 1519889225). obscene: \t cv_mean:0.9954475 \t cv_mean_fromcsv 0.99544775 \t best nrounds: 154        \n",
      "XGB top N training (id: 1519889225). threat: \t cv_mean:0.9927012500000001 \t cv_mean_fromcsv 0.99270125 \t best nrounds: 170        \n",
      "XGB top N training (id: 1519889225). insult: \t cv_mean:0.9901685 \t cv_mean_fromcsv 0.9901684999999999 \t best nrounds: 190        \n",
      "XGB top N training (id: 1519889225). identity_hate: \t cv_mean:0.99062725 \t cv_mean_fromcsv 0.99062725 \t best nrounds: 145        \n",
      "XGB top N training. avg_auc:0.9912544583333335 \t avg_auc_fromcsv 0.9912545\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    \n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "            \n",
    "#     best_rounds = {}\n",
    "#     best_rounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "#     best_rounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "#     best_rounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "#     best_rounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "#     best_rounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "#     best_rounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    \n",
    "    \n",
    "    metric_dict_fromcsv = {}\n",
    "    metric_dict_fromcsv['toxic'] = xgb_search['toxic_auc'].values[i]\n",
    "    metric_dict_fromcsv['severe_toxic'] = xgb_search['severe_toxic_auc'].values[i]\n",
    "    metric_dict_fromcsv['obscene'] = xgb_search['obscene_auc'].values[i]\n",
    "    metric_dict_fromcsv['threat'] = xgb_search['threat_auc'].values[i]\n",
    "    metric_dict_fromcsv['insult'] = xgb_search['insult_auc'].values[i]\n",
    "    metric_dict_fromcsv['identity_hate'] = xgb_search['identity_hate_auc'].values[i]\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "\n",
    "    \n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f,\\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d\\\n",
    "        '%(now,search_id,model_threshold,num_models,\\\n",
    "          xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "          xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "          xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds))\n",
    "        \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {}\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "        \n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #########################     best_nrounds    #########################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "        #######################################################################\n",
    "\n",
    "        \n",
    "        metric_dict[label] = cv_mean\n",
    "        print('XGB top N training (id: {}). {}: \\t cv_mean:{} \\t cv_mean_fromcsv {} \\t best nrounds: {}\\\n",
    "        '.format(search_id, label, cv_mean, metric_dict_fromcsv[label], best_nrounds))\n",
    "        \n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "    \n",
    "    print('XGB top N training. avg_auc:{} \\t avg_auc_fromcsv {}'.format(avg_auc, metric_dict_fromcsv['avg_auc']))\n",
    "\n",
    "\n",
    "    sub_tile = 'xgb_topn_'\n",
    "    submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "    submission[label_cols] = result\n",
    "    submission.to_csv('./StackPreds/TopN_XGB/{}_{}_{}.csv'.format(sub_title,search_id,metric_dict_fromcsv['avg_auc']), index=False)\n",
    "        \n",
    "        \n",
    "#     val_auc = para['val_auc'].values[i]\n",
    "#     print('Model training done. Validation AUC: %.5f'%val_auc)\n",
    "\n",
    "    \n",
    "#     test_flow = dataGenerator.flow(test_embeddings + test_genre, [test_context], \\\n",
    "#             batch_size=16384, shuffle=False)\n",
    "#     test_pred = model.predict_generator(test_flow, test_flow.__len__(), workers=1)\n",
    "    \n",
    "#     test_sub = pd.DataFrame({'id': test_id, 'target': test_pred.ravel()})\n",
    "#     test_sub.to_csv('./temp_nn/nn_%.5f_%.5f_%d.csv'%(val_auc, train_loss, flag), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "preds_mats = []\n",
    "for label_idx, label in enumerate(label_cols):\n",
    "    preds_list = []\n",
    "    print(label)\n",
    "    for i, (key, value) in enumerate(base_layer_est_preds_loaded.items()):\n",
    "        preds_list.append(value[:,label_idx].reshape(-1,1))\n",
    "    preds_mats.append(np.hstack(preds_list))\n",
    "    \n",
    "    \n",
    "assert len(preds_mats) == len(label_cols)\n",
    "assert preds_mats[0].shape[1] == (len(base_layer_est_preds_loaded))\n",
    "\n",
    "# analyze the first label in scatter matrix\n",
    "temp = pd.DataFrame(preds_mats[0], columns=[key.split('_')[0] + re.sub('\\D','',key) for key in base_layer_est_preds_loaded.keys()])\n",
    "print(temp.head(3))\n",
    "\n",
    "scatter_matrix(temp, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparamstune(clf, x, y, label_cols, cv=5, scoring='roc_auc'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model_data_id_list = ['ModelName.RNN_rnn_data_001',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9774)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9760)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9723)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9752)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9726)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9761)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9803)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9793)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9794)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
