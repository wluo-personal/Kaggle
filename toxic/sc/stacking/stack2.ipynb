{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading FastText model...\n",
      "fasttext model loaded. embedding dimemsion: 300\n"
     ]
    }
   ],
   "source": [
    "from base_layer_utils import BaseLayerDataRepo, BaseLayerResultsRepo, ModelName\n",
    "from base_layer_utils import LightgbmBLE, OneVSOneRegBLE, SklearnBLE, NbSvmBLE, XGBoostBLE, RnnBLE\n",
    "\n",
    "from fast_text_data import FastTextDataGenerator# fasttext_data_process\n",
    "from tfidf_data import tfidf_data_process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.cross_validation import KFold # replace with model_selection?\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=True)\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "#train = pd.read_csv('/home/kai/data/wei/Toxic/data/Shiyi_training.csv').fillna('na')\n",
    "\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "#train = train.head(1000)\n",
    "#test = test.head(1000)\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "text = pd.concat([train_sentence, test_sentence])\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train = train[:5000]\n",
    "test = test[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfidf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bldr = BaseLayerDataRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting word\n",
      "transforming train word\n",
      "transforming test word\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, data_id = tfidf_data_process(word_ngram=(1,1), word_max=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compatible_models= [ModelName.LSVC, ModelName.LOGREG, ]\n",
    "bldr.add_data(data_id, x_train, y_train, x_test, label_cols, compatible_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real \n",
      "\tx_train: (159571, 300000)\tx_test: (153164, 300000)\n",
      "\ty_train type: <class 'dict'>\n",
      "\tcompatible_model: {<ModelName.NBSVM: 4>, <ModelName.NBLSVC: 7>}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(bldr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test, nfolds, seed):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds, random_state=seed)\n",
    "    \n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_1v1(clf, x_train, y_train, x_test, nfolds, label, seed):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    kf = KFold(ntrain, n_folds=nfolds, random_state=seed) # seed is only useful if shuffle = True\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #pdb.set_trace()\n",
    "        clf.train(x_tr, y_tr, label)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te, label)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test, label)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof_rnn(clf, x_train, y_train, x_test, nfolds, number_labels, stratified=False, shuffle=True, seed=1001):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        shuffle: whether to shuffle in KFold.\n",
    "        seed: random seed for shuffling. (set it for reproducing purpose)\n",
    "    \"\"\"\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,number_labels))\n",
    "    oof_test = np.zeros((ntest,number_labels))\n",
    "    oof_test_skf = np.empty((nfolds, ntest, number_labels))\n",
    "    #kf = KFold(ntrain, n_folds=nfolds, random_state=seed)\n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits=nfolds, shuffle=shuffle, random_state=seed)\n",
    "    else:\n",
    "        kf = KFold(n_splits=nfolds, shuffle=shuffle, random_state=seed)\n",
    "\n",
    "    # tr_index, te_index: say there are four examples in training data(x_train, y_train)\n",
    "    # index: 0,1,2,3. nfolds = 4. \n",
    "    # Then (when no shuffle): (the following is shown as an example. the actual may be different)\n",
    "    #   in fold 1: tr_index = 1, 2, 3; tx_index = 0\n",
    "    #   in fold 2: tr_index = 0, 2, 3; tx_index = 1\n",
    "    #   in fold 3: tr_index = 0, 1, 3; tx_index = 2\n",
    "    #   in fold 4: tr_index = 0, 1, 2, tx_index = 3\n",
    "    # naming it as tr_index and te_index is because you training a model using tr_index \n",
    "    # and predict the tx_index to get out-of-fold predicions.\n",
    "    for i, (tr_index, te_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        x_tr, x_te = x_train[tr_index], x_train[te_index]\n",
    "        y_tr, y_te = y_train.iloc[tr_index], y_train.iloc[te_index]\n",
    "        \n",
    "        \n",
    "        clf.train(x_tr, y_tr, ft_data_gen, )\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :, :] = clf.predict(x_test)\n",
    "\n",
    "#     for i, (train_index, test_index) in enumerate(kf):\n",
    "#         ################################################################ maybe shuffle train_index\n",
    "#         x_tr = x_train[train_index]\n",
    "#         #pdb.set_trace()\n",
    "#         y_tr = y_train.iloc[train_index]\n",
    "#         x_te = x_train[test_index]\n",
    "#         clf.train(x_tr, y_tr)\n",
    "\n",
    "        \n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7480"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start generating oof_train, oof_test, baselayer estimater prediction and model data id list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_base_layer_est_preds(base_layer_est_preds):\n",
    "    for key in base_layer_est_preds:\n",
    "        submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "        submission[label_cols] = base_layer_est_preds[key]\n",
    "        sub_id = int(time.time())\n",
    "        print(sub_id)\n",
    "        submission.to_csv('./BaseEstPreds/' + key + '_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = FastTextDataGenerator.df_to_data(test, 'comment_text_cleaned', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: _ep9_fasttext_datagen_200_300 \n",
      "\tx_train: (159571, 1)\tx_test: (153164, 200, 300)\n",
      "\ty_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "\tcompatible_model: {<ModelName.RNN: 9>}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "bldr = BaseLayerDataRepo()\n",
    "\n",
    "# x_train_rnn, y_train_rnn, x_test_rnn, _, _ = fasttext_data_process()#first_n_entries=100)\n",
    "#bldr.add_data(rnn_data_id, x_train_rnn, x_test_rnn, y_train_rnn, label_cols, compatible_models, True)\n",
    "\n",
    "eps = 9\n",
    "rnn_data_id = '_ep{}_fasttext_datagen_200_300'.format(eps)\n",
    "compatible_models= [ModelName.RNN]\n",
    "bldr.add_data(rnn_data_id, train['comment_text_cleaned'].values.reshape(-1,1), x_test, train[label_cols], label_cols, compatible_models, True)\n",
    "\n",
    "print(bldr)\n",
    "\n",
    "#bldr.add_compatible_model('wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', ModelName.LGB)\n",
    "\n",
    "for data in bldr.get_data_by_compatible_model(ModelName.LGB):\n",
    "    print(data['compatible_model'])\n",
    "    print(data['x_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model_pool = {}\n",
    "\n",
    "#x_train_rnn.shape[1], x_train_rnn.shape[2] \n",
    "\n",
    "rnn_ble = rnn_m = RnnBLE(200, 300, label_cols)#RnnBLE(x_train_rnn.shape[1], x_train_rnn.shape[2], label_cols, epochs=6)\n",
    "rnn_model_pool[ModelName.RNN] = rnn_ble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10479"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.RNN _ep9_fasttext_datagen_200_300\n",
      "temp_id: 1520456363\n",
      "load model: obj/rnn_models/model_ep8_1520451611\n",
      "training with datagen\n",
      "Epoch 1/1\n",
      "1795/4987 [=========>....................] - ETA: 22:56 - loss: 0.0303 - acc: 0.9873"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9ba6a48c4ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         model.train(1, data_gen=ft_data_gen.data_gen(), \n\u001b[1;32m     46\u001b[0m                     \u001b[0mtraining_steps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mft_data_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     callbacks=callbacks, load_model=True, load_model_file='obj/rnn_models/model_ep8_1520451611')\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-be832cdcabf4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, x_train, y_train, batch_size, callbacks, validation_split, validation_data, data_gen, training_steps_per_epoch, load_model, load_model_file)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# (x_val, y_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             )\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for model_name in rnn_model_pool.keys():\n",
    "    for data in bldr.get_data_by_compatible_model(model_name):\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "\n",
    "        SEED = 1001 # for reproducibility\n",
    "        NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "        #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "\n",
    "        current_run = '{} {}'.format(model_name,data['data_id'])\n",
    "        print('Generating: '+current_run)\n",
    "\n",
    "#         oof_train, oof_test = get_oof_rnn(rnn_model_pool[model_name], \\\n",
    "#                                           x_train, y_train, x_test, NFOLDS, len(label_cols), False, SEED)\n",
    "#         for i, label in enumerate(label_cols):\n",
    "#             if label not in layer1_oof_train:\n",
    "#                 layer1_oof_train[label] = []\n",
    "#                 layer1_oof_test[label] = []\n",
    "#             layer1_oof_train[label].append(oof_train[:, i].reshape(-1,1)) # before reshape: (159571,) after: (159571, 1) => good for np.concatenate\n",
    "#             layer1_oof_test[label].append(oof_test[:, i].reshape(-1,1))\n",
    "\n",
    "        \n",
    "        model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "        model_data_id_list.append(model_data_id)\n",
    "        model = rnn_model_pool[model_name]\n",
    "        \n",
    "        df_for_data_gen = pd.DataFrame(np.hstack((x_train, y_train)), columns=[['comment']+label_cols])\n",
    "        ft_data_gen = FastTextDataGenerator(df_for_data_gen, label_cols, 'comment', 200, 32, True)\n",
    "        \n",
    "        temp_id = int(time.time())\n",
    "        print('temp_id: ' + str(temp_id))\n",
    "        model_file = 'obj/rnn_models/model_ep{}_{}'.format(eps, temp_id)\n",
    "        checkpoint = ModelCheckpoint(model_file, monitor='acc', verbose=1)#, save_best_only=True, mode='min')\n",
    "        #earlystopping = EarlyStopping(monitor=\"acc\", mode=\"max\", patience=2)\n",
    "        callbacks = [checkpoint]#, earlystopping]\n",
    "        \n",
    "        model.train(1, data_gen=ft_data_gen.data_gen(), \n",
    "                    training_steps_per_epoch=ft_data_gen.training_steps_per_epoch, \n",
    "                    callbacks=callbacks, load_model=True, load_model_file='obj/rnn_models/model_ep8_1520451611')\n",
    "        est_preds = model.predict(x_test)\n",
    "\n",
    "        base_layer_est_preds[model_data_id] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520454192\n"
     ]
    }
   ],
   "source": [
    "generate_base_layer_est_preds(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([106217.,   5191.,   3578.,   3145.,   2953.,   3032.,   3212.,\n",
       "          3841.,   5201.,  16794.]),\n",
       " array([6.69230167e-06, 9.99994904e-02, 1.99992289e-01, 2.99985087e-01,\n",
       "        3.99977885e-01, 4.99970683e-01, 5.99963481e-01, 6.99956279e-01,\n",
       "        7.99949077e-01, 8.99941875e-01, 9.99934673e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEd1JREFUeJzt3H+s3fVdx/Hna1T2w43BRrfMFi3GqqskZqxh1SVzrgsUZlb+GKbESbc0NkHmj7monf5Rs7mE+Qsl2dA66sqiY4hGGtfZNIxlagZyGZMNkHBlCFdwXC3DKdlm59s/zod5Uk7v/XDPvff09j4fycn5ft/fz/f7/Xy4t7zu9/P9npOqQpKkHs+bdAckSSuHoSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqduaSXdgsZ199tm1YcOGSXdDklaUu+6669+rau187U650NiwYQNTU1OT7oYkrShJ/qWnndNTkqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6n3CfCx7Fhzycnct6Hr37LRM4rSc+VVxqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqNm9oJNmf5IkkXxqqvSzJkSQPtvezWj1Jrk0yneSeJOcP7bOztX8wyc6h+muTfLHtc22SzHUOSdLk9FxpfBTYdlxtD3BrVW0Ebm3rABcDG9trN3AdDAIA2Au8DrgA2DsUAte1ts/st22ec0iSJmTe0KiqzwJHjytvBw605QPApUP1G2rgduDMJK8CLgKOVNXRqnoSOAJsa9vOqKrPVVUBNxx3rFHnkCRNyELvabyyqh4HaO+vaPV1wKND7WZaba76zIj6XOeQJE3IYt8Iz4haLaD+3E6a7E4ylWRqdnb2ue4uSeq00ND4Sptaor0/0eozwDlD7dYDj81TXz+iPtc5nqWq9lXV5qravHbt2gUOSZI0n4WGxkHgmSegdgK3DNWvaE9RbQGealNLh4ELk5zVboBfCBxu276WZEt7auqK44416hySpAmZ96vRk3wceCNwdpIZBk9BXQ3clGQX8AhwWWt+CLgEmAaeBt4JUFVHk7wfuLO1e19VPXNz/UoGT2i9EPhUezHHOSRJEzJvaFTV5SfYtHVE2wKuOsFx9gP7R9SngPNG1P9j1DkkSZPjJ8IlSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUrexQiPJu5Pcm+RLST6e5AVJzk1yR5IHk3wiyemt7fPb+nTbvmHoOO9t9QeSXDRU39Zq00n2jNNXSdL4FhwaSdYBPw9srqrzgNOAHcAHgWuqaiPwJLCr7bILeLKqvg+4prUjyaa23w8B24APJzktyWnAh4CLgU3A5a2tJGlCxp2eWgO8MMka4EXA48CbgJvb9gPApW15e1unbd+aJK1+Y1V9o6q+DEwDF7TXdFU9VFXfBG5sbSVJE7Lg0KiqfwV+B3iEQVg8BdwFfLWqjrVmM8C6trwOeLTte6y1f/lw/bh9TlSXJE3IONNTZzH4y/9c4LuA72QwlXS8emaXE2x7rvVRfdmdZCrJ1Ozs7HxdlyQt0DjTU28GvlxVs1X1P8BfAj8KnNmmqwDWA4+15RngHIC2/aXA0eH6cfucqP4sVbWvqjZX1ea1a9eOMSRJ0lzGCY1HgC1JXtTuTWwF7gNuA97W2uwEbmnLB9s6bfunq6pafUd7uupcYCPwD8CdwMb2NNbpDG6WHxyjv5KkMa2Zv8loVXVHkpuBzwPHgLuBfcAngRuT/GarXd92uR74WJJpBlcYO9px7k1yE4PAOQZcVVXfAkjyLuAwgyez9lfVvQvtryRpfAsODYCq2gvsPa78EIMnn45v+3XgshMc5wPAB0bUDwGHxumjJGnx+IlwSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktRtrNBIcmaSm5P8U5L7k/xIkpclOZLkwfZ+VmubJNcmmU5yT5Lzh46zs7V/MMnOofprk3yx7XNtkozTX0nSeMa90vgD4G+q6geBHwbuB/YAt1bVRuDWtg5wMbCxvXYD1wEkeRmwF3gdcAGw95mgaW12D+23bcz+SpLGsODQSHIG8AbgeoCq+mZVfRXYDhxozQ4Al7bl7cANNXA7cGaSVwEXAUeq6mhVPQkcAba1bWdU1eeqqoAbho4lSZqAca40vheYBf4kyd1JPpLkO4FXVtXjAO39Fa39OuDRof1nWm2u+syIuiRpQsYJjTXA+cB1VfUa4L/5/6moUUbdj6gF1J994GR3kqkkU7Ozs3P3WpK0YOOExgwwU1V3tPWbGYTIV9rUEu39iaH25wztvx54bJ76+hH1Z6mqfVW1uao2r127dowhSZLmsuDQqKp/Ax5N8gOttBW4DzgIPPME1E7glrZ8ELiiPUW1BXiqTV8dBi5Mcla7AX4hcLht+1qSLe2pqSuGjiVJmoA1Y+7/c8CfJjkdeAh4J4MguinJLuAR4LLW9hBwCTANPN3aUlVHk7wfuLO1e19VHW3LVwIfBV4IfKq9JEkTMlZoVNUXgM0jNm0d0baAq05wnP3A/hH1KeC8cfooSVo8fiJcktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUbOzSSnJbk7iR/3dbPTXJHkgeTfCLJ6a3+/LY+3bZvGDrGe1v9gSQXDdW3tdp0kj3j9lWSNJ7FuNL4BeD+ofUPAtdU1UbgSWBXq+8Cnqyq7wOuae1IsgnYAfwQsA34cAui04APARcDm4DLW1tJ0oSMFRpJ1gNvAT7S1gO8Cbi5NTkAXNqWt7d12vatrf124Maq+kZVfRmYBi5or+mqeqiqvgnc2NpKkiZk3CuN3wd+Bfjftv5y4KtVdaytzwDr2vI64FGAtv2p1v7b9eP2OVFdkjQhCw6NJD8BPFFVdw2XRzStebY91/qovuxOMpVkanZ2do5eS5LGMc6VxuuBtyZ5mMHU0ZsYXHmcmWRNa7MeeKwtzwDnALTtLwWODteP2+dE9Wepqn1VtbmqNq9du3aMIUmS5rLg0Kiq91bV+qrawOBG9qer6qeA24C3tWY7gVva8sG2Ttv+6aqqVt/Rnq46F9gI/ANwJ7CxPY11ejvHwYX2V5I0vjXzN3nOfhW4MclvAncD17f69cDHkkwzuMLYAVBV9ya5CbgPOAZcVVXfAkjyLuAwcBqwv6ruXYL+SpI6LUpoVNVngM+05YcYPPl0fJuvA5edYP8PAB8YUT8EHFqMPkqSxucnwiVJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlStwWHRpJzktyW5P4k9yb5hVZ/WZIjSR5s72e1epJcm2Q6yT1Jzh861s7W/sEkO4fqr03yxbbPtUkyzmAlSeMZ50rjGPCeqno1sAW4KskmYA9wa1VtBG5t6wAXAxvbazdwHQxCBtgLvA64ANj7TNC0NruH9ts2Rn8lSWNacGhU1eNV9fm2/DXgfmAdsB040JodAC5ty9uBG2rgduDMJK8CLgKOVNXRqnoSOAJsa9vOqKrPVVUBNwwdS5I0AYtyTyPJBuA1wB3AK6vqcRgEC/CK1mwd8OjQbjOtNld9ZkRdkjQhY4dGkhcDfwH8YlX951xNR9RqAfVRfdidZCrJ1Ozs7HxdliQt0FihkeQ7GATGn1bVX7byV9rUEu39iVafAc4Z2n098Ng89fUj6s9SVfuqanNVbV67du04Q5IkzWGcp6cCXA/cX1W/N7TpIPDME1A7gVuG6le0p6i2AE+16avDwIVJzmo3wC8EDrdtX0uypZ3riqFjSZImYM0Y+74e+Gngi0m+0Gq/BlwN3JRkF/AIcFnbdgi4BJgGngbeCVBVR5O8H7iztXtfVR1ty1cCHwVeCHyqvSRJE7Lg0Kiqv2P0fQeArSPaF3DVCY61H9g/oj4FnLfQPkqSFpefCJckdRtnekqSdJwNez45kfM+fPVbluU8XmlIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuaybdAcGGPZ+c2LkfvvotEzu3tJQm+e/qVOaVhiSpm1cakpaMf+2fegyNVW5S/6idFpNWJkNDE+FfoNLK5D0NSVK3kz40kmxL8kCS6SR7Jt0fSVrNTurQSHIa8CHgYmATcHmSTZPtlSStXid1aAAXANNV9VBVfRO4Edg+4T5J0qp1sofGOuDRofWZVpMkTcDJ/vRURtTqWY2S3cDutvpfSR5Y4PnOBv59gfuuVI55dXDMp7h8cOzxfk9Po5M9NGaAc4bW1wOPHd+oqvYB+8Y9WZKpqto87nFWEse8OjjmU99yjfdkn566E9iY5NwkpwM7gIMT7pMkrVon9ZVGVR1L8i7gMHAasL+q7p1wtyRp1TqpQwOgqg4Bh5bpdGNPca1Ajnl1cMynvmUZb6qedV9ZkqSRTvZ7GpKkk8iqDI35vpokyfOTfKJtvyPJhuXv5eLqGPMvJbkvyT1Jbk3S9fjdyaz3K2iSvC1JJVnRT9r0jDfJT7af871J/my5+7jYOn6vvzvJbUnubr/bl0yin4spyf4kTyT50gm2J8m17b/JPUnOX9QOVNWqejG4of7PwPcCpwP/CGw6rs3PAn/YlncAn5h0v5dhzD8OvKgtX7kaxtzavQT4LHA7sHnS/V7in/FG4G7grLb+ikn3exnGvA+4si1vAh6edL8XYdxvAM4HvnSC7ZcAn2LwObctwB2Lef7VeKXR89Uk24EDbflmYGuSUR80XCnmHXNV3VZVT7fV2xl8JmYl6/0KmvcDvwV8fTk7twR6xvszwIeq6kmAqnpimfu42HrGXMAZbfmljPic10pTVZ8Fjs7RZDtwQw3cDpyZ5FWLdf7VGBo9X03y7TZVdQx4Cnj5svRuaTzXr2PZxeAvlZVs3jEneQ1wTlX99XJ2bIn0/Iy/H/j+JH+f5PYk25atd0ujZ8y/Abw9yQyDpzB/bnm6NlFL+vVLJ/0jt0ug56tJur6+ZAXpHk+StwObgR9b0h4tvTnHnOR5wDXAO5arQ0us52e8hsEU1RsZXEn+bZLzquqrS9y3pdIz5suBj1bV7yb5EeBjbcz/u/Tdm5gl/f/XarzS6Plqkm+3SbKGwWXtXJeDJ7uur2NJ8mbg14G3VtU3lqlvS2W+Mb8EOA/4TJKHGcz9HlzBN8N7f69vqar/qaovAw8wCJGVqmfMu4CbAKrqc8ALGHwn1ams69/7Qq3G0Oj5apKDwM62/Dbg09XuMK1Q8465TdX8EYPAWOlz3TDPmKvqqao6u6o2VNUGBvdx3lpVU5Pp7th6fq//isEDDyQ5m8F01UPL2svF1TPmR4CtAElezSA0Zpe1l8vvIHBFe4pqC/BUVT2+WAdfddNTdYKvJknyPmCqqg4C1zO4jJ1mcIWxY3I9Hl/nmH8beDHw5+2e/yNV9daJdXpMnWM+ZXSO9zBwYZL7gG8Bv1xV/zG5Xo+nc8zvAf44ybsZTNG8Y4X/AUiSjzOYYjy73avZC3wHQFX9IYN7N5cA08DTwDsX9fwr/L+fJGkZrcbpKUnSAhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6vZ/JcVndt17OH8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f91f8019e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(base_layer_est_preds['ModelName.RNN__ep2_fasttext_datagen_200_300'][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer_est_preds['ModelName.RNN__ep2_fasttext_datagen_200_300'][:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_est_preds['ModelName.RNN_rnn_data_001'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1VS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsOne is using svc kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using svc kernel\n",
      "OneVsOne is using logistic kernel\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "initializing done\n",
      "OneVsOne is using logistic kernel\n"
     ]
    }
   ],
   "source": [
    "onevsone_svc = OneVSOneReg(x_train_1v1, y_train_1v1, model='svc')\n",
    "onevsone_logreg = OneVSOneReg(x_train_1v1, y_train_1v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "model_pool[ModelName.ONESVC] = onevsone_svc\n",
    "model_pool[ModelName.ONELOGREG] = onevsone_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real severe_toxic\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real obscene\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real threat\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real insult\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONESVC wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training linear svc regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Generating: ModelName.ONELOGREG wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real identity_hate\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n",
      "Starting One vs One dataset splitting\n",
      "splitting done!\n",
      "start training logistic regression\n",
      "training done\n",
      "applying naive bayes to dataset\n",
      "predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "########################## ONE VS ONE ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "            if model_name == ModelName.LGB:\n",
    "                model = LogisticRegression(solver='sag')\n",
    "                sfm = SelectFromModel(model, threshold='5*mean')\n",
    "                print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "                x_train = sfm.fit_transform(x_train, y_train)\n",
    "                x_test = sfm.transform(x_test)\n",
    "                print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof_1v1(model_pool[model_name],  x_train, y_train, x_test, NFOLDS, label, SEED) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train, label)\n",
    "            est_preds = model.predict(x_test, label)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "model_pool = {}\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "# logreg_params = {\n",
    "#     'n_jobs': 3\n",
    "# }\n",
    "# logreg_ble = SklearnBLE(LogisticRegression, seed=SEED, params=logreg_params)\n",
    "# model_pool[ModelName.LOGREG] = logreg_ble\n",
    "\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={'n_jobs': 3})\n",
    "#model_pool[ModelName.RF] = rf_ble\n",
    "\n",
    "\n",
    "# nblsvc_params = {\n",
    "#     'C':0.02\n",
    "# }\n",
    "# nblsvc_ble = NbSvmBLE(mode=ModelName.NBLSVC, seed=SEED, params=nblsvc_params)\n",
    "# model_pool[ModelName.NBLSVC] = nblsvc_ble\n",
    "\n",
    "\n",
    "# nbsvm_params = {\n",
    "#     'C':0.25,\n",
    "#     #'dual':True,\n",
    "#     'n_jobs':5\n",
    "# }\n",
    "# nbsvm_ble = NbSvmBLE(mode=ModelName.NBSVM, seed=SEED, params=nbsvm_params)\n",
    "# model_pool[ModelName.NBSVM] = nbsvm_ble\n",
    "\n",
    "#rf_ble = SklearnBLE(RandomForestClassifier, seed=SEED, params={})\n",
    "#xgb_ble = XgbBLE(params=xgb_params)\n",
    "\n",
    "lgb_params = {\n",
    "    #'learning_rate': 0.05,\n",
    "    'is_unbalance': True,\n",
    "    'early_stopping_round': 25,\n",
    "    'max_depth': -1,\n",
    "    'num_boost_round': 3000,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': 1,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 14\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 1\n",
    "}\n",
    "lgb_ble = LightgbmBLE(x_train_1v1, y_train_1v1, params=lgb_params, nb=False)\n",
    "model_pool[ModelName.LGB] = lgb_ble\n",
    "\n",
    "\n",
    "# lg = SklearnBLE(clf=LogisticRegression, seed=SEED, params={'n_jobs': 1})\n",
    "\n",
    "# et = SklearnBLE(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "# ada = SklearnBLE(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "# gb = SklearnBLE(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "# svc = SklearnBLE(clf=SVC, seed=SEED, params=svc_params)\n",
    "\n",
    "#model_pool['rf'] = rf_ble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating: ModelName.LGB wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds.\n",
      "[20]\tvalid_0's auc: 0.94673\n",
      "[40]\tvalid_0's auc: 0.9676\n",
      "[60]\tvalid_0's auc: 0.978568\n",
      "[80]\tvalid_0's auc: 0.984588\n",
      "[100]\tvalid_0's auc: 0.988221\n",
      "[120]\tvalid_0's auc: 0.990593\n",
      "[140]\tvalid_0's auc: 0.992301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-9d2b9f2d6b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#                 print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNFOLDS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Logreg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer1_oof_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mlayer1_oof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6de8033a6a47>\u001b[0m in \u001b[0;36mget_oof\u001b[0;34m(clf, x_train, y_train, x_test, nfolds)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/kaggle/toxic/sc/stacking/base_layer_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, label)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mlgb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mlgb_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgb_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1519\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################## NORMAL MODELS ######################\n",
    "base_layer_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer1_oof_train = {}\n",
    "layer1_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "for i, label in enumerate(label_cols):\n",
    "#     layer1_oof_train[label] = []\n",
    "#     layer1_oof_test[label] = []\n",
    "    for model_name in model_pool.keys():\n",
    "        for data in bldr.get_data_by_compatible_model(model_name):\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train'][label]\n",
    "            x_test = data['x_test']\n",
    "            \n",
    "            SEED = 0 # for reproducibility\n",
    "            NFOLDS = 4 # set folds for out-of-fold prediction\n",
    "            #print(x_train.shape,y_train.shape,x_test.shape,label)\n",
    "            \n",
    "            current_run = '{} {} {}'.format(model_name,data['data_id'],label)\n",
    "            print('Generating: '+current_run)\n",
    "            \n",
    "#             if model_name == ModelName.LGB:\n",
    "#                 model = LogisticRegression(solver='sag')\n",
    "#                 sfm = SelectFromModel(model, threshold='5*mean')\n",
    "#                 print('dimension before selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "#                 x_train = sfm.fit_transform(x_train, y_train)\n",
    "#                 x_test = sfm.transform(x_test)\n",
    "#                 print('dimension after selecting: train:{} test:{}'.format(x_train.shape, x_test.shape))\n",
    "            \n",
    "            oof_train, oof_test = get_oof(model_pool[model_name],  x_train, y_train, x_test, NFOLDS, SEED) # Logreg\n",
    "            if label not in layer1_oof_train:\n",
    "                layer1_oof_train[label] = []\n",
    "                layer1_oof_test[label] = []\n",
    "            layer1_oof_train[label].append(oof_train)\n",
    "            layer1_oof_test[label].append(oof_test)\n",
    "            \n",
    "            model_data_id = '{}_{}'.format(model_name, data['data_id'])\n",
    "            model = model_pool[model_name]\n",
    "            model.train(x_train, y_train)\n",
    "            est_preds = model.predict(x_test)\n",
    "            \n",
    "            if model_data_id not in base_layer_est_preds:\n",
    "                base_layer_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "                model_data_id_list.append(model_data_id)\n",
    "            base_layer_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submit the base layer estimator predictions. If they look fine, save them to BaseLayerResultsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.RNN_rnn_data_fasttext_200_300']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.RNN_rnn_data_fasttext_200_300']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'threat', 'obscene', 'identity_hate', 'severe_toxic', 'insult']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_train) # list keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_train['toxic']) # number of models to stack (each model will predict one set of toxic, servere_toxic, etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_train['toxic'][0]) # examples in oof_train (meta features, x_train) (meta labels are in train[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'threat', 'obscene', 'identity_hate', 'severe_toxic', 'insult']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer1_oof_test['toxic'][0]) # examples in oof_test (will be used by meta model (after validation) to predict the final prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()#load_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.09819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0\tModelName.RNN_rnn_data_fasttext_200_300\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add(layer1_oof_train, layer1_oof_test, base_layer_est_preds, model_data_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', 0.9826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_layer_results_repo.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before we choose which models to assemble, we can do:\n",
    "#### 1. scatter plot analysis to check the diversity\n",
    "#### 2. submit to check if the models have similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_layer_oof_per_label(layer1_oof_dict, label):\n",
    "    x = None\n",
    "    data_list = layer1_oof_dict[label]\n",
    "    for i in range(len(data_list)):\n",
    "        if i == 0:\n",
    "            x = data_list[0]\n",
    "        else:\n",
    "            x = np.concatenate((x, data_list[i]), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simple blend of two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "result = np.empty((test.shape[0],len(label_cols)))\n",
    "\n",
    "# mix the first two models\n",
    "for i, label in enumerate(label_cols):\n",
    "    x_train = combine_layer_oof_per_label(layer1_oof_train, label)\n",
    "    x_test = combine_layer_oof_per_label(layer1_oof_test, label)\n",
    "    for j in range(x_train.shape[1]):\n",
    "        roc = roc_auc_score(train[label], x_train[:,j])\n",
    "        print(label, j, roc) # print out roc for meta feature on meta label (which is just the original train label)\n",
    "    \n",
    "    roc_scores_of_a_label = []\n",
    "    alphas = np.linspace(0,1,1001)\n",
    "    best_roc = 0\n",
    "    best_alpha = 0\n",
    "    for alpha in alphas:\n",
    "        roc = roc_auc_score(train[label], alpha*x_train[:,0] + (1-alpha)*x_train[:,1])\n",
    "        if roc > best_roc:\n",
    "            best_roc = roc\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(label, best_roc, best_alpha)\n",
    "    result[:,i] = best_alpha*x_test[:,0] + (1-best_alpha)*x_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "submission[label_cols] = result\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./StackPreds/mixtwo_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insult', 'toxic', 'obscene', 'threat', 'severe_toxic', 'identity_hate']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer1_oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0.9826 update to 0.09826\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.09826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.09819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ths = [score for _, score in scores][3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9815, 0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0 update to 0.9818\n"
     ]
    }
   ],
   "source": [
    "#base_layer_results_repo.add_score('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.9818)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5e6a15ce28a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('./xgb_search.csv', 'a')\n",
    "# header = 'time,id,th,amt,csbt,lr,md,ss,ga,a,rounds,folds,tops,sps,ops,\\\n",
    "# thps,inps,ihps,tobr,sbr,obr,thbr,inbr,ihbr,sth1,sth2,sth3,sth4,sth5,\\\n",
    "# to_auc,s_auc,o_auc,th_auc,in_auc,ih_auc,avg_auc\\n'\n",
    "header = 'time,id,threshold,num_models,colsample_bytree,lr,max_depth,subsample,\\\n",
    "gamma,alpha,cv_num_round,cv_nfolds,toxic_pos_scale,severe_toxic_pos_scale,obscene_pos_scale,\\\n",
    "threat_pos_scale,insult_pos_scale,identity_hate_pos_scale,sth1,sth2,sth3,sth4,sth5,\\\n",
    "toxic_best_round,severe_toxic_best_round,obscene_best_round,threat_best_round,\\\n",
    "insult_best_round,identity_hate_best_round,toxic_auc,severe_toxic_auc,obscene_auc,\\\n",
    "threat_auc,insult_auc,identity_hate_auc,avg_auc\\n'\n",
    "f.write(header)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "    \n",
    "    return est.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 00:25:50, id: 1519968350, th: 0.980300, num_models: 5, colsample_bytree: 0.700000, lr: 0.0152982,     max_depth: 4, subsample: 0.690000, gamma: 2, alpha: 0, cv_num_round: 633, cv_nfolds: 4,    to_pw: 6, s_pw: 74, ob_pw: 14, th_pw: 139, in_pw: 10, ih_pw: 128            \n",
      "time: 2018-03-02 00:34:34, id: 1519968874, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0105918,     max_depth: 4, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 729, cv_nfolds: 4,    to_pw: 7, s_pw: 121, ob_pw: 23, th_pw: 292, in_pw: 21, ih_pw: 42            \n",
      "time: 2018-03-02 00:40:43, id: 1519969243, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0027763,     max_depth: 4, subsample: 0.810000, gamma: 0, alpha: 1, cv_num_round: 471, cv_nfolds: 3,    to_pw: 7, s_pw: 52, ob_pw: 24, th_pw: 251, in_pw: 13, ih_pw: 118            \n",
      "time: 2018-03-02 00:44:33, id: 1519969473, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0010538,     max_depth: 7, subsample: 0.560000, gamma: 1, alpha: 1, cv_num_round: 481, cv_nfolds: 3,    to_pw: 13, s_pw: 26, ob_pw: 3, th_pw: 105, in_pw: 13, ih_pw: 66            \n",
      "time: 2018-03-02 00:47:37, id: 1519969657, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0018798,     max_depth: 3, subsample: 0.810000, gamma: 1, alpha: 0, cv_num_round: 834, cv_nfolds: 4,    to_pw: 12, s_pw: 43, ob_pw: 13, th_pw: 207, in_pw: 5, ih_pw: 137            \n",
      "time: 2018-03-02 00:54:48, id: 1519970088, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0966631,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 607, cv_nfolds: 4,    to_pw: 10, s_pw: 103, ob_pw: 18, th_pw: 120, in_pw: 8, ih_pw: 136            \n",
      "time: 2018-03-02 00:58:52, id: 1519970332, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0038114,     max_depth: 6, subsample: 0.890000, gamma: 0, alpha: 1, cv_num_round: 618, cv_nfolds: 3,    to_pw: 3, s_pw: 34, ob_pw: 11, th_pw: 110, in_pw: 13, ih_pw: 55            \n",
      "time: 2018-03-02 01:04:15, id: 1519970655, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0100228,     max_depth: 5, subsample: 0.560000, gamma: 1, alpha: 1, cv_num_round: 637, cv_nfolds: 3,    to_pw: 6, s_pw: 31, ob_pw: 9, th_pw: 218, in_pw: 24, ih_pw: 125            \n",
      "time: 2018-03-02 01:11:03, id: 1519971063, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0017185,     max_depth: 2, subsample: 0.990000, gamma: 1, alpha: 0, cv_num_round: 989, cv_nfolds: 3,    to_pw: 12, s_pw: 67, ob_pw: 23, th_pw: 144, in_pw: 10, ih_pw: 91            \n",
      "time: 2018-03-02 01:18:06, id: 1519971486, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0027707,     max_depth: 5, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 708, cv_nfolds: 3,    to_pw: 7, s_pw: 96, ob_pw: 22, th_pw: 279, in_pw: 24, ih_pw: 85            \n",
      "time: 2018-03-02 01:22:14, id: 1519971734, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0231520,     max_depth: 4, subsample: 0.620000, gamma: 2, alpha: 1, cv_num_round: 625, cv_nfolds: 3,    to_pw: 8, s_pw: 51, ob_pw: 6, th_pw: 102, in_pw: 13, ih_pw: 87            \n",
      "time: 2018-03-02 01:28:41, id: 1519972121, th: 0.976800, num_models: 10, colsample_bytree: 0.500000, lr: 0.0329555,     max_depth: 6, subsample: 0.910000, gamma: 2, alpha: 0, cv_num_round: 896, cv_nfolds: 3,    to_pw: 9, s_pw: 59, ob_pw: 20, th_pw: 339, in_pw: 13, ih_pw: 78            \n",
      "time: 2018-03-02 01:34:19, id: 1519972459, th: 0.980300, num_models: 5, colsample_bytree: 0.500000, lr: 0.0071469,     max_depth: 4, subsample: 0.750000, gamma: 2, alpha: 0, cv_num_round: 614, cv_nfolds: 4,    to_pw: 8, s_pw: 23, ob_pw: 3, th_pw: 185, in_pw: 12, ih_pw: 138            \n",
      "time: 2018-03-02 01:41:34, id: 1519972894, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0266129,     max_depth: 2, subsample: 0.560000, gamma: 0, alpha: 0, cv_num_round: 405, cv_nfolds: 4,    to_pw: 5, s_pw: 36, ob_pw: 24, th_pw: 188, in_pw: 7, ih_pw: 65            \n",
      "time: 2018-03-02 01:50:22, id: 1519973422, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0035740,     max_depth: 5, subsample: 0.700000, gamma: 1, alpha: 0, cv_num_round: 425, cv_nfolds: 3,    to_pw: 13, s_pw: 110, ob_pw: 3, th_pw: 64, in_pw: 19, ih_pw: 121            \n",
      "time: 2018-03-02 01:55:49, id: 1519973749, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0818469,     max_depth: 2, subsample: 0.730000, gamma: 1, alpha: 1, cv_num_round: 570, cv_nfolds: 3,    to_pw: 12, s_pw: 71, ob_pw: 8, th_pw: 124, in_pw: 17, ih_pw: 68            \n",
      "time: 2018-03-02 01:59:46, id: 1519973986, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0615287,     max_depth: 2, subsample: 0.580000, gamma: 1, alpha: 1, cv_num_round: 519, cv_nfolds: 4,    to_pw: 14, s_pw: 122, ob_pw: 15, th_pw: 110, in_pw: 19, ih_pw: 72            \n",
      "time: 2018-03-02 02:07:07, id: 1519974427, th: 0.978600, num_models: 8, colsample_bytree: 0.900000, lr: 0.0034895,     max_depth: 6, subsample: 0.970000, gamma: 1, alpha: 0, cv_num_round: 693, cv_nfolds: 4,    to_pw: 12, s_pw: 76, ob_pw: 16, th_pw: 99, in_pw: 23, ih_pw: 41            \n",
      "time: 2018-03-02 02:15:58, id: 1519974958, th: 0.980300, num_models: 5, colsample_bytree: 0.800000, lr: 0.0031782,     max_depth: 3, subsample: 0.630000, gamma: 2, alpha: 0, cv_num_round: 641, cv_nfolds: 4,    to_pw: 12, s_pw: 22, ob_pw: 21, th_pw: 198, in_pw: 21, ih_pw: 46            \n",
      "time: 2018-03-02 02:22:08, id: 1519975328, th: 0.980300, num_models: 5, colsample_bytree: 0.700000, lr: 0.0228572,     max_depth: 4, subsample: 0.580000, gamma: 2, alpha: 0, cv_num_round: 482, cv_nfolds: 4,    to_pw: 2, s_pw: 46, ob_pw: 9, th_pw: 76, in_pw: 23, ih_pw: 96            \n",
      "time: 2018-03-02 02:31:13, id: 1519975873, th: 0.981500, num_models: 4, colsample_bytree: 0.700000, lr: 0.0137914,     max_depth: 6, subsample: 0.920000, gamma: 2, alpha: 0, cv_num_round: 787, cv_nfolds: 4,    to_pw: 12, s_pw: 37, ob_pw: 11, th_pw: 171, in_pw: 11, ih_pw: 57            \n",
      "time: 2018-03-02 02:39:26, id: 1519976366, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0858544,     max_depth: 5, subsample: 0.580000, gamma: 0, alpha: 1, cv_num_round: 876, cv_nfolds: 3,    to_pw: 4, s_pw: 46, ob_pw: 16, th_pw: 100, in_pw: 20, ih_pw: 116            \n",
      "time: 2018-03-02 02:42:07, id: 1519976527, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0616493,     max_depth: 4, subsample: 0.710000, gamma: 0, alpha: 0, cv_num_round: 689, cv_nfolds: 4,    to_pw: 14, s_pw: 88, ob_pw: 19, th_pw: 343, in_pw: 23, ih_pw: 120            \n",
      "time: 2018-03-02 02:47:13, id: 1519976833, th: 0.977400, num_models: 9, colsample_bytree: 0.600000, lr: 0.0070077,     max_depth: 5, subsample: 0.600000, gamma: 1, alpha: 1, cv_num_round: 849, cv_nfolds: 3,    to_pw: 3, s_pw: 119, ob_pw: 3, th_pw: 107, in_pw: 13, ih_pw: 111            \n",
      "time: 2018-03-02 02:52:27, id: 1519977147, th: 0.981500, num_models: 4, colsample_bytree: 0.600000, lr: 0.0983639,     max_depth: 2, subsample: 0.500000, gamma: 1, alpha: 1, cv_num_round: 734, cv_nfolds: 3,    to_pw: 11, s_pw: 27, ob_pw: 22, th_pw: 227, in_pw: 9, ih_pw: 73            \n",
      "time: 2018-03-02 02:55:45, id: 1519977345, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0018059,     max_depth: 7, subsample: 0.560000, gamma: 2, alpha: 0, cv_num_round: 895, cv_nfolds: 3,    to_pw: 6, s_pw: 53, ob_pw: 14, th_pw: 168, in_pw: 4, ih_pw: 131            \n",
      "time: 2018-03-02 02:59:22, id: 1519977562, th: 0.981500, num_models: 4, colsample_bytree: 0.900000, lr: 0.0227284,     max_depth: 6, subsample: 0.670000, gamma: 1, alpha: 0, cv_num_round: 949, cv_nfolds: 4,    to_pw: 10, s_pw: 73, ob_pw: 19, th_pw: 168, in_pw: 6, ih_pw: 135            \n",
      "time: 2018-03-02 03:04:34, id: 1519977874, th: 0.976800, num_models: 10, colsample_bytree: 0.800000, lr: 0.0017340,     max_depth: 7, subsample: 0.850000, gamma: 2, alpha: 1, cv_num_round: 451, cv_nfolds: 4,    to_pw: 5, s_pw: 20, ob_pw: 22, th_pw: 312, in_pw: 20, ih_pw: 53            \n",
      "time: 2018-03-02 03:12:52, id: 1519978372, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0015783,     max_depth: 3, subsample: 0.880000, gamma: 1, alpha: 0, cv_num_round: 742, cv_nfolds: 4,    to_pw: 9, s_pw: 122, ob_pw: 22, th_pw: 318, in_pw: 15, ih_pw: 39            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 03:22:59, id: 1519978979, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0685323,     max_depth: 2, subsample: 0.880000, gamma: 2, alpha: 1, cv_num_round: 984, cv_nfolds: 4,    to_pw: 8, s_pw: 87, ob_pw: 11, th_pw: 93, in_pw: 10, ih_pw: 109            \n",
      "time: 2018-03-02 03:31:54, id: 1519979514, th: 0.976800, num_models: 10, colsample_bytree: 0.900000, lr: 0.0392708,     max_depth: 5, subsample: 0.630000, gamma: 0, alpha: 0, cv_num_round: 498, cv_nfolds: 4,    to_pw: 11, s_pw: 121, ob_pw: 23, th_pw: 351, in_pw: 23, ih_pw: 57            \n",
      "time: 2018-03-02 03:37:12, id: 1519979832, th: 0.977400, num_models: 9, colsample_bytree: 0.700000, lr: 0.0030536,     max_depth: 6, subsample: 0.700000, gamma: 0, alpha: 1, cv_num_round: 873, cv_nfolds: 4,    to_pw: 4, s_pw: 114, ob_pw: 7, th_pw: 341, in_pw: 4, ih_pw: 118            \n",
      "time: 2018-03-02 03:45:08, id: 1519980308, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0649149,     max_depth: 7, subsample: 0.660000, gamma: 2, alpha: 1, cv_num_round: 895, cv_nfolds: 4,    to_pw: 7, s_pw: 97, ob_pw: 16, th_pw: 68, in_pw: 11, ih_pw: 101            \n",
      "time: 2018-03-02 03:50:00, id: 1519980600, th: 0.981500, num_models: 4, colsample_bytree: 0.800000, lr: 0.0491951,     max_depth: 7, subsample: 0.630000, gamma: 0, alpha: 1, cv_num_round: 843, cv_nfolds: 4,    to_pw: 4, s_pw: 25, ob_pw: 14, th_pw: 184, in_pw: 24, ih_pw: 41            \n",
      "time: 2018-03-02 03:54:44, id: 1519980884, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0140443,     max_depth: 5, subsample: 0.800000, gamma: 2, alpha: 0, cv_num_round: 704, cv_nfolds: 4,    to_pw: 6, s_pw: 69, ob_pw: 5, th_pw: 109, in_pw: 10, ih_pw: 54            \n",
      "time: 2018-03-02 04:06:50, id: 1519981610, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0191208,     max_depth: 7, subsample: 0.570000, gamma: 0, alpha: 1, cv_num_round: 927, cv_nfolds: 4,    to_pw: 11, s_pw: 111, ob_pw: 22, th_pw: 135, in_pw: 13, ih_pw: 115            \n",
      "time: 2018-03-02 04:15:21, id: 1519982121, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0030881,     max_depth: 7, subsample: 0.780000, gamma: 1, alpha: 0, cv_num_round: 996, cv_nfolds: 3,    to_pw: 13, s_pw: 125, ob_pw: 8, th_pw: 166, in_pw: 23, ih_pw: 88            \n",
      "time: 2018-03-02 04:20:01, id: 1519982401, th: 0.979300, num_models: 7, colsample_bytree: 0.800000, lr: 0.0424453,     max_depth: 2, subsample: 0.830000, gamma: 0, alpha: 0, cv_num_round: 548, cv_nfolds: 4,    to_pw: 12, s_pw: 84, ob_pw: 3, th_pw: 326, in_pw: 8, ih_pw: 99            \n",
      "time: 2018-03-02 04:28:01, id: 1519982881, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0268777,     max_depth: 2, subsample: 0.850000, gamma: 2, alpha: 1, cv_num_round: 669, cv_nfolds: 4,    to_pw: 2, s_pw: 85, ob_pw: 19, th_pw: 264, in_pw: 24, ih_pw: 69            \n",
      "time: 2018-03-02 04:39:06, id: 1519983546, th: 0.977400, num_models: 9, colsample_bytree: 0.500000, lr: 0.0082537,     max_depth: 4, subsample: 0.540000, gamma: 1, alpha: 1, cv_num_round: 997, cv_nfolds: 3,    to_pw: 14, s_pw: 92, ob_pw: 3, th_pw: 303, in_pw: 10, ih_pw: 31            \n",
      "time: 2018-03-02 04:50:55, id: 1519984255, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0024649,     max_depth: 2, subsample: 0.950000, gamma: 2, alpha: 1, cv_num_round: 811, cv_nfolds: 4,    to_pw: 2, s_pw: 20, ob_pw: 16, th_pw: 267, in_pw: 11, ih_pw: 76            \n",
      "time: 2018-03-02 05:04:24, id: 1519985064, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0058243,     max_depth: 2, subsample: 0.720000, gamma: 1, alpha: 1, cv_num_round: 484, cv_nfolds: 3,    to_pw: 12, s_pw: 74, ob_pw: 11, th_pw: 323, in_pw: 16, ih_pw: 104            \n",
      "time: 2018-03-02 05:11:20, id: 1519985480, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0681723,     max_depth: 7, subsample: 0.890000, gamma: 2, alpha: 0, cv_num_round: 441, cv_nfolds: 3,    to_pw: 7, s_pw: 29, ob_pw: 15, th_pw: 261, in_pw: 7, ih_pw: 71            \n",
      "time: 2018-03-02 05:14:48, id: 1519985688, th: 0.976800, num_models: 10, colsample_bytree: 0.500000, lr: 0.0133457,     max_depth: 2, subsample: 0.970000, gamma: 0, alpha: 1, cv_num_round: 688, cv_nfolds: 4,    to_pw: 10, s_pw: 33, ob_pw: 19, th_pw: 149, in_pw: 17, ih_pw: 92            \n",
      "time: 2018-03-02 05:26:12, id: 1519986372, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0256713,     max_depth: 5, subsample: 0.760000, gamma: 1, alpha: 0, cv_num_round: 999, cv_nfolds: 3,    to_pw: 13, s_pw: 31, ob_pw: 21, th_pw: 344, in_pw: 9, ih_pw: 69            \n",
      "time: 2018-03-02 05:32:05, id: 1519986725, th: 0.979300, num_models: 7, colsample_bytree: 0.600000, lr: 0.0030848,     max_depth: 7, subsample: 0.620000, gamma: 0, alpha: 0, cv_num_round: 748, cv_nfolds: 4,    to_pw: 14, s_pw: 95, ob_pw: 13, th_pw: 66, in_pw: 24, ih_pw: 132            \n",
      "time: 2018-03-02 05:40:19, id: 1519987219, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0178147,     max_depth: 4, subsample: 0.910000, gamma: 1, alpha: 1, cv_num_round: 668, cv_nfolds: 4,    to_pw: 10, s_pw: 81, ob_pw: 23, th_pw: 133, in_pw: 9, ih_pw: 78            \n",
      "time: 2018-03-02 05:54:17, id: 1519988057, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0141059,     max_depth: 4, subsample: 0.740000, gamma: 0, alpha: 0, cv_num_round: 526, cv_nfolds: 3,    to_pw: 14, s_pw: 38, ob_pw: 9, th_pw: 85, in_pw: 23, ih_pw: 101            \n",
      "time: 2018-03-02 06:00:17, id: 1519988417, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0227382,     max_depth: 5, subsample: 0.800000, gamma: 1, alpha: 1, cv_num_round: 935, cv_nfolds: 3,    to_pw: 14, s_pw: 27, ob_pw: 4, th_pw: 305, in_pw: 8, ih_pw: 48            \n",
      "time: 2018-03-02 06:07:11, id: 1519988831, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0010075,     max_depth: 6, subsample: 0.580000, gamma: 2, alpha: 1, cv_num_round: 838, cv_nfolds: 4,    to_pw: 3, s_pw: 59, ob_pw: 24, th_pw: 271, in_pw: 11, ih_pw: 67            \n",
      "time: 2018-03-02 06:13:43, id: 1519989223, th: 0.981500, num_models: 4, colsample_bytree: 0.900000, lr: 0.0068811,     max_depth: 6, subsample: 0.760000, gamma: 1, alpha: 1, cv_num_round: 787, cv_nfolds: 3,    to_pw: 11, s_pw: 113, ob_pw: 5, th_pw: 197, in_pw: 10, ih_pw: 59            \n",
      "time: 2018-03-02 06:20:00, id: 1519989600, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0017916,     max_depth: 2, subsample: 0.510000, gamma: 2, alpha: 1, cv_num_round: 576, cv_nfolds: 4,    to_pw: 10, s_pw: 48, ob_pw: 7, th_pw: 227, in_pw: 7, ih_pw: 115            \n",
      "time: 2018-03-02 06:27:06, id: 1519990026, th: 0.979300, num_models: 7, colsample_bytree: 0.900000, lr: 0.0240391,     max_depth: 5, subsample: 0.600000, gamma: 1, alpha: 0, cv_num_round: 736, cv_nfolds: 4,    to_pw: 10, s_pw: 99, ob_pw: 12, th_pw: 348, in_pw: 12, ih_pw: 107            \n",
      "time: 2018-03-02 06:35:47, id: 1519990547, th: 0.976800, num_models: 10, colsample_bytree: 0.600000, lr: 0.0121182,     max_depth: 6, subsample: 0.930000, gamma: 1, alpha: 1, cv_num_round: 566, cv_nfolds: 3,    to_pw: 8, s_pw: 116, ob_pw: 24, th_pw: 181, in_pw: 20, ih_pw: 57            \n",
      "time: 2018-03-02 06:44:48, id: 1519991088, th: 0.978600, num_models: 8, colsample_bytree: 0.600000, lr: 0.0372951,     max_depth: 4, subsample: 0.590000, gamma: 0, alpha: 0, cv_num_round: 893, cv_nfolds: 4,    to_pw: 10, s_pw: 29, ob_pw: 12, th_pw: 185, in_pw: 8, ih_pw: 81            \n",
      "time: 2018-03-02 06:52:03, id: 1519991523, th: 0.979400, num_models: 6, colsample_bytree: 0.600000, lr: 0.0431916,     max_depth: 3, subsample: 0.990000, gamma: 0, alpha: 0, cv_num_round: 629, cv_nfolds: 4,    to_pw: 2, s_pw: 33, ob_pw: 8, th_pw: 102, in_pw: 20, ih_pw: 88            \n",
      "time: 2018-03-02 06:59:56, id: 1519991996, th: 0.979400, num_models: 6, colsample_bytree: 0.500000, lr: 0.0202178,     max_depth: 5, subsample: 0.870000, gamma: 0, alpha: 0, cv_num_round: 914, cv_nfolds: 4,    to_pw: 8, s_pw: 49, ob_pw: 4, th_pw: 240, in_pw: 15, ih_pw: 91            \n",
      "time: 2018-03-02 07:09:02, id: 1519992542, th: 0.978600, num_models: 8, colsample_bytree: 0.700000, lr: 0.0229555,     max_depth: 2, subsample: 0.990000, gamma: 0, alpha: 1, cv_num_round: 444, cv_nfolds: 3,    to_pw: 5, s_pw: 113, ob_pw: 19, th_pw: 324, in_pw: 9, ih_pw: 78            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 07:15:18, id: 1519992918, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0010395,     max_depth: 7, subsample: 0.840000, gamma: 1, alpha: 1, cv_num_round: 695, cv_nfolds: 4,    to_pw: 4, s_pw: 85, ob_pw: 23, th_pw: 341, in_pw: 9, ih_pw: 105            \n",
      "time: 2018-03-02 07:21:05, id: 1519993265, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0032795,     max_depth: 2, subsample: 0.860000, gamma: 1, alpha: 1, cv_num_round: 544, cv_nfolds: 3,    to_pw: 9, s_pw: 43, ob_pw: 9, th_pw: 69, in_pw: 19, ih_pw: 61            \n",
      "time: 2018-03-02 07:28:58, id: 1519993738, th: 0.981500, num_models: 4, colsample_bytree: 0.500000, lr: 0.0188473,     max_depth: 3, subsample: 0.940000, gamma: 2, alpha: 0, cv_num_round: 405, cv_nfolds: 3,    to_pw: 14, s_pw: 114, ob_pw: 17, th_pw: 344, in_pw: 24, ih_pw: 30            \n",
      "time: 2018-03-02 07:35:41, id: 1519994141, th: 0.977400, num_models: 9, colsample_bytree: 0.800000, lr: 0.0688231,     max_depth: 2, subsample: 0.650000, gamma: 2, alpha: 1, cv_num_round: 456, cv_nfolds: 3,    to_pw: 13, s_pw: 71, ob_pw: 5, th_pw: 294, in_pw: 13, ih_pw: 84            \n",
      "time: 2018-03-02 07:40:04, id: 1519994404, th: 0.979300, num_models: 7, colsample_bytree: 0.500000, lr: 0.0164190,     max_depth: 2, subsample: 0.770000, gamma: 0, alpha: 1, cv_num_round: 578, cv_nfolds: 4,    to_pw: 9, s_pw: 124, ob_pw: 15, th_pw: 116, in_pw: 21, ih_pw: 30            \n",
      "time: 2018-03-02 07:49:39, id: 1519994979, th: 0.977400, num_models: 9, colsample_bytree: 0.900000, lr: 0.0102957,     max_depth: 3, subsample: 0.740000, gamma: 0, alpha: 1, cv_num_round: 797, cv_nfolds: 4,    to_pw: 10, s_pw: 83, ob_pw: 9, th_pw: 150, in_pw: 7, ih_pw: 111            \n",
      "time: 2018-03-02 08:00:15, id: 1519995615, th: 0.979400, num_models: 6, colsample_bytree: 0.800000, lr: 0.0053757,     max_depth: 6, subsample: 0.670000, gamma: 0, alpha: 1, cv_num_round: 678, cv_nfolds: 4,    to_pw: 5, s_pw: 66, ob_pw: 5, th_pw: 214, in_pw: 9, ih_pw: 53            \n",
      "time: 2018-03-02 08:09:14, id: 1519996154, th: 0.979400, num_models: 6, colsample_bytree: 0.700000, lr: 0.0354215,     max_depth: 6, subsample: 0.520000, gamma: 1, alpha: 1, cv_num_round: 839, cv_nfolds: 3,    to_pw: 14, s_pw: 39, ob_pw: 18, th_pw: 364, in_pw: 13, ih_pw: 89            \n",
      "time: 2018-03-02 08:14:30, id: 1519996470, th: 0.979300, num_models: 7, colsample_bytree: 0.700000, lr: 0.0027659,     max_depth: 5, subsample: 0.690000, gamma: 0, alpha: 0, cv_num_round: 543, cv_nfolds: 4,    to_pw: 13, s_pw: 102, ob_pw: 10, th_pw: 350, in_pw: 13, ih_pw: 57            \n",
      "time: 2018-03-02 08:21:41, id: 1519996901, th: 0.977400, num_models: 9, colsample_bytree: 0.700000, lr: 0.0465120,     max_depth: 3, subsample: 0.690000, gamma: 0, alpha: 1, cv_num_round: 892, cv_nfolds: 4,    to_pw: 11, s_pw: 61, ob_pw: 5, th_pw: 364, in_pw: 21, ih_pw: 61            \n",
      "time: 2018-03-02 08:28:42, id: 1519997322, th: 0.980300, num_models: 5, colsample_bytree: 0.600000, lr: 0.0124011,     max_depth: 7, subsample: 0.710000, gamma: 0, alpha: 1, cv_num_round: 440, cv_nfolds: 4,    to_pw: 2, s_pw: 108, ob_pw: 7, th_pw: 212, in_pw: 24, ih_pw: 30            \n",
      "time: 2018-03-02 08:36:45, id: 1519997805, th: 0.976800, num_models: 10, colsample_bytree: 0.700000, lr: 0.0310058,     max_depth: 4, subsample: 0.900000, gamma: 1, alpha: 0, cv_num_round: 831, cv_nfolds: 3,    to_pw: 8, s_pw: 72, ob_pw: 12, th_pw: 200, in_pw: 5, ih_pw: 67            \n",
      "time: 2018-03-02 08:43:18, id: 1519998198, th: 0.978600, num_models: 8, colsample_bytree: 0.800000, lr: 0.0168915,     max_depth: 7, subsample: 0.670000, gamma: 2, alpha: 1, cv_num_round: 570, cv_nfolds: 4,    to_pw: 2, s_pw: 69, ob_pw: 21, th_pw: 307, in_pw: 6, ih_pw: 133            \n",
      "time: 2018-03-02 08:53:25, id: 1519998805, th: 0.977400, num_models: 9, colsample_bytree: 0.500000, lr: 0.0405281,     max_depth: 7, subsample: 0.860000, gamma: 1, alpha: 0, cv_num_round: 934, cv_nfolds: 3,    to_pw: 7, s_pw: 51, ob_pw: 22, th_pw: 107, in_pw: 16, ih_pw: 136            \n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for i in range(130):\n",
    "    now = get_time()\n",
    "    search_id = int(time.time())\n",
    "    np.random.seed(int(time.time()* 1000000) % 45234634)\n",
    "    \n",
    "    model_threshold = np.random.choice(ths)#[0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768, 0.9765])\n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "        \n",
    "    xgb_colsample_bytree = np.random.randint(5, 10)/10\n",
    "    xgb_learning_rate = 1e-2 * (0.1 ** (np.random.rand() * 2 - 1.0)) # 0.001 to 0.0997\n",
    "    xgb_max_depth = np.random.randint(2, 8)\n",
    "    xgb_subsample = np.random.randint(50, 100)/100\n",
    "    xgb_gamma = np.random.randint(0, 3)\n",
    "    xgb_alpha = np.random.randint(0, 2)\n",
    "\n",
    "    xgb_cv_seed = 0\n",
    "    xgb_cv_num_round = np.random.randint(400, 1000)\n",
    "    xgb_cv_nfolds = np.random.randint(3,5)\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = np.random.randint(2, 15) #10\n",
    "    scale_pos_weights['severe_toxic'] = np.random.randint(20, 130) # 100\n",
    "    scale_pos_weights['obscene'] = np.random.randint(3, 25) # 17\n",
    "    scale_pos_weights['threat'] = np.random.randint(60, 380) # 333\n",
    "    scale_pos_weights['insult'] = np.random.randint(4, 25) # 20\n",
    "    scale_pos_weights['identity_hate'] = np.random.randint(30, 140) #112\n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    num_models = len(layer1_oof_train_loaded['toxic'])\n",
    "    #print('Stacking {} models'.format(num_models)) # number of models that will be stacked\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate']))\n",
    "    \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {} # all labels\n",
    "    best_nrounds = {}  # all labels\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "    #     clf = XGBClassifier()\n",
    "    #     #scores = cross_val_score(clf, x_train, train[label], cv=3, scoring='roc_auc')\n",
    "    #     #print(scores)\n",
    "    #     #print(\"Stacking-CV: ROC: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    #     clf.fit(x_train, train[label])\n",
    "    #     result[:, i] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "        def xg_eval_auc(yhat, dtrain):\n",
    "            y = dtrain.get_label()\n",
    "            return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "        xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "        res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "                 early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "        # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "        #pdb.set_trace()\n",
    "        best_nrounds_for_current_label = res.shape[0] - 1\n",
    "        #print(res[-3:])\n",
    "        cv_mean = res.iloc[-1, 0]\n",
    "        cv_std = res.iloc[-1, 1]\n",
    "\n",
    "        #print('Ensemble-CV: {}: {}+{}'.format(label, cv_mean, cv_std))\n",
    "        metric_dict[label] = cv_mean\n",
    "        best_nrounds[label] = best_nrounds_for_current_label\n",
    "        #metric_dict[label]['cv_mean'] = cv_mean\n",
    "        #metric_dict[label]['cv_std'] = cv_std\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds_for_current_label)\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "    avg_auc = 0\n",
    "    for label in label_cols:\n",
    "        avg_auc += metric_dict[label]\n",
    "    avg_auc/=6\n",
    "          \n",
    "    res = '%s,%d,%f,%d,%f,%.7f,%d,%f,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%d,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f,%.8f\\n\\\n",
    "            '%(now,search_id,model_threshold,num_models,xgb_colsample_bytree,\\\n",
    "               xgb_learning_rate,xgb_max_depth,xgb_subsample,xgb_gamma,xgb_alpha,\\\n",
    "               xgb_cv_num_round,xgb_cv_nfolds,scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "               scale_pos_weights['obscene'],scale_pos_weights['threat'],scale_pos_weights['insult'],\\\n",
    "               scale_pos_weights['identity_hate'],-99,-99,-99,-99,-99,best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "               best_nrounds['obscene'],best_nrounds['threat'],best_nrounds['insult'],\\\n",
    "               best_nrounds['identity_hate'],metric_dict['toxic'],metric_dict['severe_toxic'],\\\n",
    "               metric_dict['obscene'],metric_dict['threat'],metric_dict['insult'],\\\n",
    "               metric_dict['identity_hate'],avg_auc)\n",
    "\n",
    "    f = open('./xgb_search.csv', 'a')\n",
    "    f.write(res)\n",
    "    f.close()\n",
    "\n",
    "#     sub_tile = 'stacking_test_'\n",
    "#     submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "#     submission[label_cols] = result\n",
    "#     submission.to_csv('./StackPreds/' + sub_tile + str(search_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb random search top N training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from base_layer_results_repo import BaseLayerResultsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9825\tModelName.RNN_rnn_data_001\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.09819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n"
     ]
    }
   ],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>threat_best_round</th>\n",
       "      <th>insult_best_round</th>\n",
       "      <th>identity_hate_best_round</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-02 00:54:48</td>\n",
       "      <td>1519970088</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.096663</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>74</td>\n",
       "      <td>98</td>\n",
       "      <td>0.987579</td>\n",
       "      <td>0.991802</td>\n",
       "      <td>0.995448</td>\n",
       "      <td>0.994074</td>\n",
       "      <td>0.990130</td>\n",
       "      <td>0.991359</td>\n",
       "      <td>0.991732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2018-03-02 08:36:45</td>\n",
       "      <td>1519997805</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.031006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>273</td>\n",
       "      <td>163</td>\n",
       "      <td>195</td>\n",
       "      <td>0.987640</td>\n",
       "      <td>0.991029</td>\n",
       "      <td>0.995461</td>\n",
       "      <td>0.993971</td>\n",
       "      <td>0.990103</td>\n",
       "      <td>0.991239</td>\n",
       "      <td>0.991574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2018-03-02 07:35:41</td>\n",
       "      <td>1519994141</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.068823</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>104</td>\n",
       "      <td>0.987659</td>\n",
       "      <td>0.991810</td>\n",
       "      <td>0.995460</td>\n",
       "      <td>0.992785</td>\n",
       "      <td>0.990203</td>\n",
       "      <td>0.991231</td>\n",
       "      <td>0.991525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               time          id  threshold  num_models  \\\n",
       "9               2018-03-02 00:54:48  1519970088     0.9794           6   \n",
       "73              2018-03-02 08:36:45  1519997805     0.9768          10   \n",
       "65              2018-03-02 07:35:41  1519994141     0.9774           9   \n",
       "\n",
       "    colsample_bytree        lr  max_depth  subsample  gamma  alpha    ...     \\\n",
       "9                0.9  0.096663          3       0.90      2      0    ...      \n",
       "73               0.7  0.031006          4       0.90      1      0    ...      \n",
       "65               0.8  0.068823          2       0.65      2      1    ...      \n",
       "\n",
       "    threat_best_round  insult_best_round  identity_hate_best_round  toxic_auc  \\\n",
       "9                 119                 74                        98   0.987579   \n",
       "73                273                163                       195   0.987640   \n",
       "65                100                 96                       104   0.987659   \n",
       "\n",
       "    severe_toxic_auc  obscene_auc  threat_auc  insult_auc  identity_hate_auc  \\\n",
       "9           0.991802     0.995448    0.994074    0.990130           0.991359   \n",
       "73          0.991029     0.995461    0.993971    0.990103           0.991239   \n",
       "65          0.991810     0.995460    0.992785    0.990203           0.991231   \n",
       "\n",
       "     avg_auc  \n",
       "9   0.991732  \n",
       "73  0.991574  \n",
       "65  0.991525  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search = pd.read_csv('xgb_search.csv').sort_values(by='avg_auc', ascending=False)\n",
    "\n",
    "xgb_search.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>num_models</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>lr</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "      <th>gamma</th>\n",
       "      <th>alpha</th>\n",
       "      <th>cv_num_round</th>\n",
       "      <th>cv_nfolds</th>\n",
       "      <th>toxic_auc</th>\n",
       "      <th>severe_toxic_auc</th>\n",
       "      <th>obscene_auc</th>\n",
       "      <th>threat_auc</th>\n",
       "      <th>insult_auc</th>\n",
       "      <th>identity_hate_auc</th>\n",
       "      <th>avg_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 10:23:13</td>\n",
       "      <td>1519899793</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.092125</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987677</td>\n",
       "      <td>0.991848</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.994002</td>\n",
       "      <td>0.990178</td>\n",
       "      <td>0.991174</td>\n",
       "      <td>0.991723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 06:45:20</td>\n",
       "      <td>1519886720</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.097664</td>\n",
       "      <td>4</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "      <td>3</td>\n",
       "      <td>0.987663</td>\n",
       "      <td>0.991693</td>\n",
       "      <td>0.995412</td>\n",
       "      <td>0.993793</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.991636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 08:14:47</td>\n",
       "      <td>1519892087</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.098587</td>\n",
       "      <td>3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>809</td>\n",
       "      <td>4</td>\n",
       "      <td>0.987629</td>\n",
       "      <td>0.991796</td>\n",
       "      <td>0.995422</td>\n",
       "      <td>0.993610</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>0.991218</td>\n",
       "      <td>0.991635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time          id  threshold  num_models  colsample_bytree  \\\n",
       "0  2018-03-01 10:23:13  1519899793     0.9768          10               0.7   \n",
       "1  2018-03-01 06:45:20  1519886720     0.9794           6               0.8   \n",
       "2  2018-03-01 08:14:47  1519892087     0.9794           6               0.5   \n",
       "\n",
       "         lr  max_depth  subsample  gamma  alpha  cv_num_round  cv_nfolds  \\\n",
       "0  0.092125          2       0.85      2      0           962          4   \n",
       "1  0.097664          4       0.68      1      0           995          3   \n",
       "2  0.098587          3       0.77      1      0           809          4   \n",
       "\n",
       "   toxic_auc  severe_toxic_auc  obscene_auc  threat_auc  insult_auc  \\\n",
       "0   0.987677          0.991848     0.995462    0.994002    0.990178   \n",
       "1   0.987663          0.991693     0.995412    0.993793    0.990062   \n",
       "2   0.987629          0.991796     0.995422    0.993610    0.990133   \n",
       "\n",
       "   identity_hate_auc   avg_auc  \n",
       "0           0.991174  0.991723  \n",
       "1           0.991191  0.991636  \n",
       "2           0.991218  0.991635  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_search_round1 = pd.read_csv('xgb_search_ori100.csv').sort_values(by='avg_auc', ascending=False)\n",
    "\n",
    "xgb_search_round1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2018-03-02 11:06:04, id: 1519970088, th: 0.979400, num_models: 6, colsample_bytree: 0.900000, lr: 0.0966631,     max_depth: 3, subsample: 0.900000, gamma: 2, alpha: 0, cv_num_round: 607, cv_nfolds: 4,    to_pw: 10, s_pw: 103, ob_pw: 18, th_pw: 120, in_pw: 8, ih_pw: 136, to_br: 145, s_br: 74,    ob_br: 55, th_br: 119, in_br: 74, ih_br: 98            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9769336019ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#         xgb_params['scale_pos_weight'] = scale_pos_weights[label]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mgbdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_nrounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#_proba(x_test)[:,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    \n",
    "    now = get_time()\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "    # per label based params \n",
    "    best_nrounds = {}\n",
    "    best_nrounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "    best_nrounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "    best_nrounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "    best_nrounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "    best_nrounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "    best_nrounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = xgb_search['toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['severe_toxic'] = xgb_search['severe_toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['obscene'] = xgb_search['obscene_pos_scale'].values[i]\n",
    "    scale_pos_weights['threat'] = xgb_search['threat_pos_scale'].values[i]\n",
    "    scale_pos_weights['insult'] = xgb_search['insult_pos_scale'].values[i]\n",
    "    scale_pos_weights['identity_hate'] = xgb_search['identity_hate_pos_scale'].values[i]\n",
    "    \n",
    "    metric_dict_fromcsv = {}\n",
    "#     metric_dict_fromcsv['toxic'] = xgb_search['toxic_auc'].values[i]\n",
    "#     metric_dict_fromcsv['severe_toxic'] = xgb_search['severe_toxic_auc'].values[i]\n",
    "#     metric_dict_fromcsv['obscene'] = xgb_search['obscene_auc'].values[i]\n",
    "#     metric_dict_fromcsv['threat'] = xgb_search['threat_auc'].values[i]\n",
    "#     metric_dict_fromcsv['insult'] = xgb_search['insult_auc'].values[i]\n",
    "#     metric_dict_fromcsv['identity_hate'] = xgb_search['identity_hate_auc'].values[i]\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "\n",
    "    \n",
    "    layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=model_threshold)\n",
    "    gc.collect() \n",
    "\n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f, \\\n",
    "    max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d,\\\n",
    "    to_pw: %d, s_pw: %d, ob_pw: %d, th_pw: %d, in_pw: %d, ih_pw: %d, to_br: %d, s_br: %d,\\\n",
    "    ob_br: %d, th_br: %d, in_br: %d, ih_br: %d\\\n",
    "            '%(now,search_id,model_threshold,num_models,\\\n",
    "              xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "              xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "              xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds,\\\n",
    "              scale_pos_weights['toxic'],scale_pos_weights['severe_toxic'],\\\n",
    "              scale_pos_weights['obscene'],scale_pos_weights['threat'],\\\n",
    "              scale_pos_weights['insult'],scale_pos_weights['identity_hate'],\\\n",
    "              best_nrounds['toxic'],best_nrounds['severe_toxic'],\\\n",
    "              best_nrounds['obscene'],best_nrounds['threat'],\\\n",
    "              best_nrounds['insult'],best_nrounds['identity_hate']))\n",
    "\n",
    "#     print('time: %s, id: %d, th: %f, num_models: %d, colsample_bytree: %f, lr: %.7f,\\\n",
    "#     max_depth: %d, subsample: %f, gamma: %d, alpha: %d, cv_num_round: %d, cv_nfolds: %d\\\n",
    "#         '%(now,search_id,model_threshold,num_models,\\\n",
    "#           xgb_colsample_bytree,xgb_learning_rate,\\\n",
    "#           xgb_max_depth,xgb_subsample,xgb_gamma,\\\n",
    "#           xgb_alpha,xgb_cv_num_round,xgb_cv_nfolds))\n",
    "        \n",
    "\n",
    "    result = np.empty((test.shape[0],len(label_cols)))\n",
    "    metric_dict = {}\n",
    "\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "        dtrain = xgb.DMatrix(x_train, train[label]) # check if train is still in right shape\n",
    "        dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "#         def xg_eval_auc(yhat, dtrain):\n",
    "#             y = dtrain.get_label()\n",
    "#             return 'auc', roc_auc_score(y, yhat)\n",
    "\n",
    "#         xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        \n",
    "#         res = xgb.cv(xgb_params, dtrain, num_boost_round=xgb_cv_num_round, nfold=xgb_cv_nfolds, seed=xgb_cv_seed, stratified=False,\n",
    "#                  early_stopping_rounds=25, verbose_eval=None, show_stdv=False, feval=xg_eval_auc, maximize=True)\n",
    "#         # early stopping is based on eavl on test fold. so check out the test-auc\n",
    "#         #pdb.set_trace()\n",
    "#         best_nrounds = res.shape[0] - 1\n",
    "#         #print(res[-3:])\n",
    "#         cv_mean = res.iloc[-1, 0]\n",
    "#         cv_std = res.iloc[-1, 1]\n",
    " \n",
    "#         metric_dict[label] = cv_mean\n",
    "#         print('XGB top N training (id: {}). {}: \\t cv_mean:{} \\t cv_mean_fromcsv {} \\t best nrounds: {}\\\n",
    "#         '.format(search_id, label, cv_mean, metric_dict_fromcsv[label], best_nrounds))\n",
    "    \n",
    "        \n",
    "#         xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "        gbdt = xgb.train(xgb_params, dtrain, best_nrounds[label])\n",
    "\n",
    "        result[:,i] = gbdt.predict(dtest)#_proba(x_test)[:,1]\n",
    "\n",
    "    #print('Stacking done')\n",
    "\n",
    "#     avg_auc = 0\n",
    "#     for label in label_cols:\n",
    "#         avg_auc += metric_dict[label]\n",
    "#     avg_auc/=6\n",
    "    \n",
    "#     print('XGB top N training. avg_auc:{} \\t avg_auc_fromcsv {}'.format(avg_auc, metric_dict_fromcsv['avg_auc']))\n",
    "\n",
    "\n",
    "    sub_title = 'xgb_topn_'#posweighted_'\n",
    "    submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "    submission[label_cols] = result\n",
    "    submission.to_csv('./StackPreds/TopN_XGB/{}_{}_{}.csv'.format(sub_title,metric_dict_fromcsv['avg_auc'],search_id), index=False)\n",
    "        \n",
    "        \n",
    "#     val_auc = para['val_auc'].values[i]\n",
    "#     print('Model training done. Validation AUC: %.5f'%val_auc)\n",
    "\n",
    "    \n",
    "#     test_flow = dataGenerator.flow(test_embeddings + test_genre, [test_context], \\\n",
    "#             batch_size=16384, shuffle=False)\n",
    "#     test_pred = model.predict_generator(test_flow, test_flow.__len__(), workers=1)\n",
    "    \n",
    "#     test_sub = pd.DataFrame({'id': test_id, 'target': test_pred.ravel()})\n",
    "#     test_sub.to_csv('./temp_nn/nn_%.5f_%.5f_%d.csv'%(val_auc, train_loss, flag), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(base_layer_est_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model_data_id_list = ['ModelName.RNN_rnn_data_001',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000',\n",
    "'ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9774)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9760)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9723)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9815)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9752)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9726)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9761)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9803)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9793)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000', 0.9794)\n",
    "\n",
    "base_layer_results_repo.add_score('ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
