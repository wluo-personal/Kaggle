{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from enum import Enum\n",
    "class ModelName(Enum):\n",
    "    XGB = 1\n",
    "    NBXGB = 2\n",
    "    LGB = 3\n",
    "    NBLGB = 4\n",
    "    LOGREG = 5\n",
    "    NBSVM = 6\n",
    "    NBLSVC = 7\n",
    "    RF = 8 # random forest\n",
    "    RNN = 9\n",
    "    ONESVC = 10\n",
    "    ONELOGREG = 11\n",
    "\n",
    "\n",
    "class BaseLayerEstimator(ABC):\n",
    "    \n",
    "    def _pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def _nb(self, x_train, y_train):\n",
    "        assert isinstance(y_train, pd.DataFrame)\n",
    "        r = {}\n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            r[col] = np.log(self._pr(1, y_train[col].values, x_train) / self._pr(0, y_train[col], x_train))\n",
    "        return r\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np array\n",
    "            y_train: pd series\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, x_train):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "class OneVSOneRegBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, model='logistic'):\n",
    "        \"\"\"\n",
    "        x_train: sparse matrix, raw tfidf\n",
    "        y_train: dataframe, with only label columns. should be 6 columns in total\n",
    "        model: only support logistic or svc\n",
    "        \"\"\"\n",
    "        self.r = {}\n",
    "        self.setModelName(model)\n",
    "        assert self.model_name in ['logistic', 'svc']\n",
    "        self.param = {}\n",
    "        self.param['logistic'] = {'identity_hate': 9.0,\n",
    "                                     'insult': 1.5,\n",
    "                                     'obscene': 1.0,\n",
    "                                     'severe_toxic': 4.0,\n",
    "                                     'threat': 9.0,\n",
    "                                     'toxic': 2.7}\n",
    "        self.param['svc'] = {'identity_hate': 0.9,\n",
    "                             'insult': 0.15,\n",
    "                             'obscene': 0.15,\n",
    "                             'severe_toxic': 0.15,\n",
    "                             'threat': 1.0,\n",
    "                             'toxic': 0.29}\n",
    "        \n",
    "        \n",
    "        \n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            self.r[col] = np.log(self.pr(1, y_train[col].values, x_train) / self.pr(0, y_train[col], x_train))\n",
    "        print('initializing done')\n",
    "        print('OneVsOne is using {} kernel'.format(self.model_name))\n",
    "        \n",
    "    def setModelName(self, name):\n",
    "        self.model_name = name\n",
    "        assert self.model_name in ['logistic', 'svc']\n",
    "        print('OneVsOne is using {} kernel'.format(self.model_name))\n",
    "        \n",
    "    def pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def oneVsOneSplit(self, x_train, y_train, label):\n",
    "        print('Starting One vs One dataset splitting')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        model_train = x_train[np.array(y_train == 1)]\n",
    "        y_model_train = y_train[np.array(y_train == 1)]\n",
    "        non_model_train = x_train[np.array(y_train == 0)]\n",
    "        non_model_train = non_model_train[:model_train.shape[0]]\n",
    "        y_non_model_train = y_train[np.array(y_train == 0)]\n",
    "        y_non_model_train = y_non_model_train[:model_train.shape[0]]\n",
    "        x_model_stack = vstack([model_train, non_model_train])\n",
    "        y_model_stack = np.concatenate([y_model_train, y_non_model_train])\n",
    "        x_nb = x_model_stack.multiply(self.r[label]).tocsr()\n",
    "        y_nb = y_model_stack\n",
    "        print('splitting done!')\n",
    "        return (x_nb, y_nb)\n",
    "    \n",
    "    def train(self, x_train, y_train, label):\n",
    "        ### construct one vs one\n",
    "        x_nb, y_nb = self.oneVsOneSplit(x_train, y_train, label)\n",
    "        ### start training\n",
    "        if self.model_name is 'logistic':\n",
    "            print('start training logistic regression')\n",
    "            self.model = LogisticRegression(C=self.param['logistic'][label])\n",
    "            self.model.fit(x_nb, y_nb)\n",
    "            print('training done')\n",
    "            \n",
    "        else:\n",
    "            print('start training linear svc regression')\n",
    "            lsvc = LinearSVC(C=self.param['svc'][label])\n",
    "            self.model = CalibratedClassifierCV(lsvc) \n",
    "            self.model.fit(x_nb, y_nb)\n",
    "            print('training done')\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self, x_test, label):\n",
    "        print('applying naive bayes to dataset')\n",
    "        x_nb_test = x_test.multiply(self.r[label]).tocsr()\n",
    "        print('predicting')\n",
    "        pred = self.model.predict_proba(x_nb_test)[:,1]\n",
    "        print('predicting done')\n",
    "        return pred\n",
    "    \n",
    "##### example        \n",
    "# aa = OneVSOneReg(train_tfidf, train[label_cols], model='logistic')\n",
    "# aa.setModelName('svc')\n",
    "# aa.train(train_tfidf,train['toxic'], 'toxic')\n",
    "# aa.predict(test_tfidf, 'toxic')\n",
    "\n",
    "\n",
    "\n",
    "class SklearnBLE(BaseLayerEstimator):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "class NbSvmBLE(BaseLayerEstimator, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, mode=ModelName.NBSVM, seed=0, params=None):\n",
    "        self._mode = mode\n",
    "        params['random_state'] = seed\n",
    "        self._params = params\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        #return self._clf.predict(x.multiply(self._r))\n",
    "        return self._clf.predict_proba(x.multiply(self._r))[:,1] # chance of being 1 ([:,0] chance of being 0)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        #self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        self._clf = LogisticRegression(**self._params).fit(x_nb, y)\n",
    "        if self._mode == ModelName.NBLSVC:\n",
    "            self._clf = CalibratedClassifierCV(LinearSVC(**self._params)).fit(x_nb, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.fit(x_train, y_train)\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        return self._clf.feature_importance\n",
    "\n",
    "    \n",
    "class XgbBLE(BaseLayerEstimator):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict_proba(xgb.DMatrix(x))[:,1]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "class LightgbmBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, params=None, nb=True, seed=0):\n",
    "        \"\"\"\n",
    "        constructor:\n",
    "\n",
    "            x_train: should be a np/scipy/ 2-d array or matrix. only be used when nb is true\n",
    "            y_train: should be a dataframe\n",
    "            \n",
    "        Example:\n",
    "            ll = LightgbmBLE(train_tfidf, train[label_cols], params=params, nb=True)\n",
    "            result = pd.DataFrame()\n",
    "            for col in label_cols:\n",
    "                    print(col)\n",
    "                    ll.train(train_tfidf, train[col], col)\n",
    "                    result[col] = ll.predict(test_tfidf, col)\n",
    "        \"\"\"\n",
    "        #### check naive bayes\n",
    "        if nb:\n",
    "            print('Naive Bayes is enabled')\n",
    "            self.r = self._nb(x_train, y_train)\n",
    "        else:\n",
    "            print('Naive Bayes is disabled')\n",
    "            self.r = None\n",
    "        ##### set values    \n",
    "        self.nb = nb\n",
    "        self.set_params(params)\n",
    "        print('LightgbmBLE is initialized')\n",
    "    \n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _pre_process(self, x_train, y_train, label=None):\n",
    "        if self.nb:\n",
    "            if label is None:\n",
    "                raise ValueError('Naive Bayes is enabled. label cannot be None.')\n",
    "            print('apply naive bayes to feature set')\n",
    "            x = x_train.multiply(self.r[label])\n",
    "            if isinstance(x_train, csr_matrix):\n",
    "                x = x.tocsr()\n",
    "        else:\n",
    "            x = x_train\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y = y_train.values\n",
    "        else:\n",
    "            y = y_train\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train, label=None, valid_set_percent=0):\n",
    "        x, y = self._pre_process(x_train, y_train, label)\n",
    "        \n",
    "        if valid_set_percent != 0:\n",
    "            if valid_set_percent > 1 or valid_set_percent < 0:\n",
    "                raise ValueError('valid_set_percent must >= 0 and <= 1')\n",
    "            x, x_val, y, y_val = train_test_split(x, y, test_size=valid_set_percent)\n",
    "\n",
    "\n",
    "        lgb_train = lgb.Dataset(x, y)\n",
    "        if valid_set_percent != 0:\n",
    "            lgb_val = lgb.Dataset(x_val, y_val)\n",
    "            self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_val)\n",
    "        else:\n",
    "            self.model = lgb.train(self.params, lgb_train)\n",
    "        \n",
    "        \n",
    "    def predict(self, x_train, label=None):\n",
    "        x, _ = self._pre_process(x_train, y_train=None, label=label)\n",
    "        print('starting predicting')\n",
    "        result = self.model.predict(x)\n",
    "        print('predicting done')\n",
    "        return result\n",
    "        \n",
    "            \n",
    "\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "class RnnBLE(BaseLayerEstimator):\n",
    "    def __init__(self, window_length, n_features, label_cols, rnn_units=50, dense_units=50, dropout=0.1, mode='LSTM', bidirection=True, batch_size=32, epochs=2):\n",
    "        self._model = RnnBLE.get_lstm_model(window_length, n_features, label_cols, rnn_units, dense_units, dropout, mode, bidirection)\n",
    "        self._batch_size = batch_size\n",
    "        self._epochs = epochs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_lstm_model(window_length, n_features, label_cols, rnn_units, dense_units, dropout, mode, bidirection):\n",
    "        input = Input(shape=(window_length, n_features))\n",
    "        rnn_layer = LSTM(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)\n",
    "        if mode == 'GRU':\n",
    "            rnn_layer = GRU(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)\n",
    "        if bidirection:\n",
    "            x = Bidirectional(rnn_layer)(input)\n",
    "        else:\n",
    "            x = rnn_layer(input)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(dense_units, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(len(label_cols), activation='sigmoid')(x)\n",
    "        model = Model(inputs=input, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self._model.fit(x_train, y_train, batch_size=self._batch_size, epochs=self._epochs)\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self._model.predict(x)#, batch_size=1024)\n",
    "    \n",
    "    \n",
    "    \n",
    "class BaseLayerDataRepo():\n",
    "    def __init__(self):\n",
    "        self._data_repo = {}\n",
    "    \n",
    "    def add_data(self, data_id, x_train, x_test, y_train, label_cols, compatible_model=[ModelName.LOGREG], rnn_data=False):\n",
    "        \"\"\"\n",
    "        x_train, x_test: ndarray\n",
    "        y_train: pd df\n",
    "        \"\"\"\n",
    "        temp = {}\n",
    "        \n",
    "        temp['data_id'] = data_id\n",
    "        temp['x_train'] = x_train\n",
    "        temp['x_test'] = x_test\n",
    "        temp['labes_cols'] = label_cols\n",
    "        temp['compatible_model'] = set(compatible_model)\n",
    "        \n",
    "        if rnn_data: \n",
    "            temp['y_train'] = y_train # here y_train is a df\n",
    "        else:\n",
    "            label_dict = {}\n",
    "            for col in label_cols:\n",
    "                label_dict[col] = y_train[col]\n",
    "            temp['y_train'] = label_dict # hence y_train is a dict with labels as keys\n",
    "        \n",
    "        self._data_repo[data_id] = temp\n",
    "    \n",
    "    def get_data(self, data_id):\n",
    "        return self._data_repo[data_id]\n",
    "    \n",
    "    def remove_data(self, data_id):\n",
    "        self._data_repo.pop(data_id, None)\n",
    "        \n",
    "    def get_compatible_model(self, data_id):\n",
    "        return self._data_repo[data_id]['compatible_model']\n",
    "    \n",
    "    def remove_compatible_model(self, data_id, model_name):\n",
    "        return self._data_repo[data_id]['compatible_model'].discard(model_name)\n",
    "    \n",
    "    def add_compatible_model(self, data_id, model_name):\n",
    "        return self._data_repo[data_id]['compatible_model'].add(model_name)\n",
    "                  \n",
    "    def get_data_by_compatible_model(self, model_name):\n",
    "        data_to_return = []\n",
    "        for data_id in self._data_repo.keys():\n",
    "            data = self._data_repo[data_id]\n",
    "            if model_name in data['compatible_model']:\n",
    "                data_to_return.append(data)\n",
    "        return data_to_return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data_repo)\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = ''\n",
    "        for data_id in self._data_repo.keys():\n",
    "            output+='data_id: {:20} \\n\\tx_train: {}\\tx_test: {}\\n\\ty_train type: {}\\n\\tcompatible_model: {}\\n '\\\n",
    "            .format(data_id, self._data_repo[data_id]['x_train'].shape, \\\n",
    "                    self._data_repo[data_id]['x_test'].shape, \\\n",
    "                    type(self._data_repo[data_id]['y_train']), \\\n",
    "                    self._data_repo[data_id]['compatible_model'])\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "import copy\n",
    "class BaseLayerResultsRepo:\n",
    "    def __init__(self, label_cols=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], load_from_file=True):\n",
    "        self._layer1_oof_train = {}\n",
    "        self._layer1_oof_test = {}\n",
    "        for label in label_cols:\n",
    "            self._layer1_oof_train[label] = []\n",
    "            self._layer1_oof_test[label] = []\n",
    "        self._base_layer_est_preds = {}\n",
    "        self._model_data_id_list = []\n",
    "        self._base_layer_est_scores = {}\n",
    "        self._label_cols = label_cols\n",
    "        self._save_lock = False # will be set to True if remove() is invoked successfully\n",
    "        if load_from_file:\n",
    "            print('load from file')\n",
    "            self._layer1_oof_train = load_obj('13models_layer1_oof_train')\n",
    "            self._layer1_oof_test = load_obj('13models_layer1_oof_test')\n",
    "            self._base_layer_est_preds = load_obj('13models_base_layer_est_preds')\n",
    "            self._model_data_id_list = load_obj('13models_model_data_id_list')\n",
    "            self._base_layer_est_scores = load_obj('13models_base_layer_est_scores')\n",
    "\n",
    "    def get_model_data_id_list(self):\n",
    "        return self._model_data_id_list\n",
    "    \n",
    "    def add(self, layer1_oof_train, layer1_oof_test, base_layer_est_preds, model_data_id_list):\n",
    "        assert type(layer1_oof_train) == dict\n",
    "        assert len(list(layer1_oof_train)) == len(self._label_cols)\n",
    "        assert set(list(layer1_oof_train)) - set(self._label_cols) == set()\n",
    "        assert type(layer1_oof_test) == dict\n",
    "        assert len(list(layer1_oof_test)) == len(self._label_cols)\n",
    "        assert set(list(layer1_oof_test)) - set(self._label_cols) == set()\n",
    "        for label in self._label_cols:\n",
    "            len(layer1_oof_train[label]) == len(layer1_oof_test[label]) == len(list(base_layer_est_preds))\n",
    "        assert type(base_layer_est_preds) == dict\n",
    "        assert type(model_data_id_list) == list\n",
    "        assert set(list(base_layer_est_preds)) - set(model_data_id_list) == set()\n",
    "        for model_data_id in model_data_id_list:\n",
    "            if model_data_id in set(self._model_data_id_list):\n",
    "                raise ValueError('{} is already in the repo'.format(model_data_id))\n",
    "        for model_data_id in model_data_id_list:\n",
    "            if model_data_id not in set(self._model_data_id_list):\n",
    "                self._model_data_id_list.append(model_data_id)\n",
    "                self._base_layer_est_scores[model_data_id] = 0\n",
    "        for (key, values) in base_layer_est_preds.items():\n",
    "            self._base_layer_est_preds[key] = values\n",
    "        for label in self._label_cols:\n",
    "            self._layer1_oof_train[label] += layer1_oof_train[label]\n",
    "            self._layer1_oof_test[label] += layer1_oof_test[label]\n",
    "    \n",
    "    def add_score(self, model_data_id, score):\n",
    "        assert score <= 1 and score >= 0\n",
    "        if model_data_id not in set(self._model_data_id_list):\n",
    "            raise ValueError('{} not in the repo. please add it first'.format(model_data_id))\n",
    "        if model_data_id in set(self._model_data_id_list):\n",
    "            print('{} already existed in the repo. score: {} update to {}'\\\n",
    "                  .format(model_data_id, self._base_layer_est_scores[model_data_id], score))\n",
    "        self._base_layer_est_scores[model_data_id] = score\n",
    "    \n",
    "    def show_scores(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of (name, score) tuple in sorted order by score\n",
    "        \"\"\"\n",
    "        sorted_list_from_dict = sorted(self._base_layer_est_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "        for key, value in sorted_list_from_dict:\n",
    "            print('{}\\t{}'.format(value, key))\n",
    "        return sorted_list_from_dict\n",
    "    \n",
    "    def get_results(self, threshold=None, chosen_ones=None):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            Note: threshold and chosen_ones can NOT both be not-None\n",
    "            threshold: if not None, then return only ones that score >= threshold\n",
    "            chosen_ones: list of model_data_id\n",
    "        Returns: \n",
    "            layer1_oof_train, layer1_oof_test, base_layer_est_preds\n",
    "        \"\"\"\n",
    "        if threshold != None and chosen_ones != None:\n",
    "            raise ValueError('threshold and chosen_ones can NOT both be not-None')\n",
    "        if threshold == None and chosen_ones == None:\n",
    "            return self._layer1_oof_train, self._layer1_oof_test, self._base_layer_est_preds\n",
    "        else:\n",
    "            layer1_oof_train_temp = copy.deepcopy(self._layer1_oof_train) # copy only keep the keys, not the value reference\n",
    "            layer1_oof_test_temp = copy.deepcopy(self._layer1_oof_test)   # deepcopy also keep the value reference\n",
    "            base_layer_est_preds_temp = self._base_layer_est_preds.copy()\n",
    "            base_layer_est_scores_temp = self._base_layer_est_scores.copy()\n",
    "            model_data_id_list_temp = self._model_data_id_list.copy()\n",
    "            if threshold != None:\n",
    "                assert threshold <= 1 and threshold >= 0\n",
    "                for (key, value) in base_layer_est_scores_temp.items():\n",
    "                    if value < threshold:\n",
    "                        self.remove(key)\n",
    "            else: # chosen_ones != None\n",
    "                assert type(chosen_ones) == list\n",
    "                for model_data_id in chosen_ones:\n",
    "                    self.remove(model_data_id)\n",
    "            \n",
    "            self._save_lock = False # not actually removed, so set it back to True\n",
    "\n",
    "            r1, r2, r3 = self._layer1_oof_train, self._layer1_oof_test, self._base_layer_est_preds\n",
    "\n",
    "            self._layer1_oof_train = layer1_oof_train_temp\n",
    "            self._layer1_oof_test = layer1_oof_test_temp\n",
    "            self._base_layer_est_preds = base_layer_est_preds_temp\n",
    "            self._base_layer_est_scores = base_layer_est_scores_temp\n",
    "            self._model_data_id_list = model_data_id_list_temp\n",
    "            return r1, r2, r3\n",
    "    \n",
    "    def remove(self, model_data_id):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        mdid_index = self._model_data_id_list.index(model_data_id)\n",
    "        self._model_data_id_list.pop(mdid_index)\n",
    "        self._base_layer_est_preds.pop(model_data_id)\n",
    "        self._base_layer_est_scores.pop(model_data_id)\n",
    "        for label in self._label_cols:\n",
    "            self._layer1_oof_train[label].pop(mdid_index)\n",
    "            self._layer1_oof_test[label].pop(mdid_index)\n",
    "        self._save_lock = True\n",
    "            \n",
    "    def unlock_save(self):\n",
    "        self._save_lock = False\n",
    "            \n",
    "    def save(self):\n",
    "        if self._save_lock:\n",
    "            print('save function is locked due to some results removed from the repo. \\\n",
    "            Call unlock_save() to unlock the save function and save again.')\n",
    "        else:\n",
    "            save_obj(self._model_data_id_list, '13models_model_data_id_list')\n",
    "            save_obj(self._layer1_oof_train, '13models_layer1_oof_train')\n",
    "            save_obj(self._layer1_oof_test, '13models_layer1_oof_test')\n",
    "            save_obj(self._base_layer_est_preds, '13models_base_layer_est_preds')\n",
    "            save_obj(self._base_layer_est_scores, '13models_base_layer_est_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tfidf_data import tfidf_data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, data_id = tfidf_data_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    #'learning_rate': 0.05,\n",
    "    'is_unbalance': True,\n",
    "    'early_stopping_round': 25,\n",
    "    'max_depth': -1,\n",
    "    'num_boost_round': 3000,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': 1,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 4\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 1\n",
    "}\n",
    "lgb_ble = LightgbmBLE(x_train, y_train, params=lgb_params, nb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.870293\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[2]\tvalid_0's auc: 0.879855\n",
      "[3]\tvalid_0's auc: 0.890069\n",
      "[4]\tvalid_0's auc: 0.895269\n",
      "[5]\tvalid_0's auc: 0.900742\n",
      "[6]\tvalid_0's auc: 0.904055\n",
      "[7]\tvalid_0's auc: 0.90557\n",
      "[8]\tvalid_0's auc: 0.908777\n",
      "[9]\tvalid_0's auc: 0.911911\n",
      "[10]\tvalid_0's auc: 0.912996\n",
      "[11]\tvalid_0's auc: 0.916038\n",
      "[12]\tvalid_0's auc: 0.918541\n",
      "[13]\tvalid_0's auc: 0.921163\n",
      "[14]\tvalid_0's auc: 0.922745\n",
      "[15]\tvalid_0's auc: 0.925085\n",
      "[16]\tvalid_0's auc: 0.926786\n",
      "[17]\tvalid_0's auc: 0.927603\n",
      "[18]\tvalid_0's auc: 0.931456\n",
      "[19]\tvalid_0's auc: 0.932031\n",
      "[20]\tvalid_0's auc: 0.934025\n",
      "[21]\tvalid_0's auc: 0.93553\n",
      "[22]\tvalid_0's auc: 0.936878\n",
      "[23]\tvalid_0's auc: 0.938081\n",
      "[24]\tvalid_0's auc: 0.93911\n",
      "[25]\tvalid_0's auc: 0.940473\n",
      "[26]\tvalid_0's auc: 0.941336\n",
      "[27]\tvalid_0's auc: 0.942148\n",
      "[28]\tvalid_0's auc: 0.942864\n",
      "[29]\tvalid_0's auc: 0.943854\n",
      "[30]\tvalid_0's auc: 0.944219\n",
      "[31]\tvalid_0's auc: 0.945139\n",
      "[32]\tvalid_0's auc: 0.947068\n",
      "[33]\tvalid_0's auc: 0.947943\n",
      "[34]\tvalid_0's auc: 0.949299\n",
      "[35]\tvalid_0's auc: 0.949881\n",
      "[36]\tvalid_0's auc: 0.950709\n",
      "[37]\tvalid_0's auc: 0.951376\n",
      "[38]\tvalid_0's auc: 0.952251\n",
      "[39]\tvalid_0's auc: 0.95297\n",
      "[40]\tvalid_0's auc: 0.953703\n",
      "[41]\tvalid_0's auc: 0.954556\n",
      "[42]\tvalid_0's auc: 0.955345\n",
      "[43]\tvalid_0's auc: 0.955883\n",
      "[44]\tvalid_0's auc: 0.956224\n",
      "[45]\tvalid_0's auc: 0.956448\n",
      "[46]\tvalid_0's auc: 0.957143\n",
      "[47]\tvalid_0's auc: 0.958222\n",
      "[48]\tvalid_0's auc: 0.958521\n",
      "[49]\tvalid_0's auc: 0.958812\n",
      "[50]\tvalid_0's auc: 0.959535\n",
      "[51]\tvalid_0's auc: 0.960217\n",
      "[52]\tvalid_0's auc: 0.960455\n",
      "[53]\tvalid_0's auc: 0.960892\n",
      "[54]\tvalid_0's auc: 0.961406\n",
      "[55]\tvalid_0's auc: 0.962039\n",
      "[56]\tvalid_0's auc: 0.962328\n",
      "[57]\tvalid_0's auc: 0.962745\n",
      "[58]\tvalid_0's auc: 0.963123\n",
      "[59]\tvalid_0's auc: 0.963432\n",
      "[60]\tvalid_0's auc: 0.963828\n",
      "[61]\tvalid_0's auc: 0.964368\n",
      "[62]\tvalid_0's auc: 0.96478\n",
      "[63]\tvalid_0's auc: 0.964981\n",
      "[64]\tvalid_0's auc: 0.965402\n",
      "[65]\tvalid_0's auc: 0.965689\n",
      "[66]\tvalid_0's auc: 0.965989\n",
      "[67]\tvalid_0's auc: 0.966119\n",
      "[68]\tvalid_0's auc: 0.966557\n",
      "[69]\tvalid_0's auc: 0.966636\n",
      "[70]\tvalid_0's auc: 0.966908\n",
      "[71]\tvalid_0's auc: 0.967093\n",
      "[72]\tvalid_0's auc: 0.967389\n",
      "[73]\tvalid_0's auc: 0.967575\n",
      "[74]\tvalid_0's auc: 0.967705\n",
      "[75]\tvalid_0's auc: 0.967973\n",
      "[76]\tvalid_0's auc: 0.968204\n",
      "[77]\tvalid_0's auc: 0.968479\n",
      "[78]\tvalid_0's auc: 0.968569\n",
      "[79]\tvalid_0's auc: 0.968892\n",
      "[80]\tvalid_0's auc: 0.96902\n",
      "[81]\tvalid_0's auc: 0.96913\n",
      "[82]\tvalid_0's auc: 0.969359\n",
      "[83]\tvalid_0's auc: 0.969591\n",
      "[84]\tvalid_0's auc: 0.969768\n",
      "[85]\tvalid_0's auc: 0.969915\n",
      "[86]\tvalid_0's auc: 0.970017\n",
      "[87]\tvalid_0's auc: 0.970074\n",
      "[88]\tvalid_0's auc: 0.97031\n",
      "[89]\tvalid_0's auc: 0.970477\n",
      "[90]\tvalid_0's auc: 0.970589\n",
      "[91]\tvalid_0's auc: 0.970694\n",
      "[92]\tvalid_0's auc: 0.970754\n",
      "[93]\tvalid_0's auc: 0.97082\n",
      "[94]\tvalid_0's auc: 0.970928\n",
      "[95]\tvalid_0's auc: 0.971038\n",
      "[96]\tvalid_0's auc: 0.971227\n",
      "[97]\tvalid_0's auc: 0.971362\n",
      "[98]\tvalid_0's auc: 0.971456\n",
      "[99]\tvalid_0's auc: 0.971534\n",
      "[100]\tvalid_0's auc: 0.971644\n",
      "[101]\tvalid_0's auc: 0.971797\n",
      "[102]\tvalid_0's auc: 0.971859\n",
      "[103]\tvalid_0's auc: 0.971918\n",
      "[104]\tvalid_0's auc: 0.972091\n",
      "[105]\tvalid_0's auc: 0.972186\n",
      "[106]\tvalid_0's auc: 0.972321\n",
      "[107]\tvalid_0's auc: 0.972409\n",
      "[108]\tvalid_0's auc: 0.972481\n",
      "[109]\tvalid_0's auc: 0.972599\n",
      "[110]\tvalid_0's auc: 0.97274\n",
      "[111]\tvalid_0's auc: 0.972768\n",
      "[112]\tvalid_0's auc: 0.972911\n",
      "[113]\tvalid_0's auc: 0.972975\n",
      "[114]\tvalid_0's auc: 0.973063\n",
      "[115]\tvalid_0's auc: 0.973152\n",
      "[116]\tvalid_0's auc: 0.97316\n",
      "[117]\tvalid_0's auc: 0.973243\n",
      "[118]\tvalid_0's auc: 0.973368\n",
      "[119]\tvalid_0's auc: 0.973423\n",
      "[120]\tvalid_0's auc: 0.973436\n",
      "[121]\tvalid_0's auc: 0.973539\n",
      "[122]\tvalid_0's auc: 0.973576\n",
      "[123]\tvalid_0's auc: 0.973675\n",
      "[124]\tvalid_0's auc: 0.973742\n",
      "[125]\tvalid_0's auc: 0.973817\n",
      "[126]\tvalid_0's auc: 0.973855\n",
      "[127]\tvalid_0's auc: 0.973954\n",
      "[128]\tvalid_0's auc: 0.974055\n",
      "[129]\tvalid_0's auc: 0.974147\n",
      "[130]\tvalid_0's auc: 0.97418\n",
      "[131]\tvalid_0's auc: 0.974212\n",
      "[132]\tvalid_0's auc: 0.974308\n",
      "[133]\tvalid_0's auc: 0.974376\n",
      "[134]\tvalid_0's auc: 0.974461\n",
      "[135]\tvalid_0's auc: 0.974498\n",
      "[136]\tvalid_0's auc: 0.974544\n",
      "[137]\tvalid_0's auc: 0.974542\n",
      "[138]\tvalid_0's auc: 0.974635\n",
      "[139]\tvalid_0's auc: 0.974729\n",
      "[140]\tvalid_0's auc: 0.974717\n",
      "[141]\tvalid_0's auc: 0.974778\n",
      "[142]\tvalid_0's auc: 0.974838\n",
      "[143]\tvalid_0's auc: 0.974821\n",
      "[144]\tvalid_0's auc: 0.974833\n",
      "[145]\tvalid_0's auc: 0.974874\n",
      "[146]\tvalid_0's auc: 0.974887\n",
      "[147]\tvalid_0's auc: 0.974962\n",
      "[148]\tvalid_0's auc: 0.974988\n",
      "[149]\tvalid_0's auc: 0.975054\n",
      "[150]\tvalid_0's auc: 0.975057\n",
      "[151]\tvalid_0's auc: 0.975075\n",
      "[152]\tvalid_0's auc: 0.975103\n",
      "[153]\tvalid_0's auc: 0.975213\n",
      "[154]\tvalid_0's auc: 0.975266\n",
      "[155]\tvalid_0's auc: 0.975266\n",
      "[156]\tvalid_0's auc: 0.975304\n",
      "[157]\tvalid_0's auc: 0.975316\n",
      "[158]\tvalid_0's auc: 0.975307\n",
      "[159]\tvalid_0's auc: 0.975345\n",
      "[160]\tvalid_0's auc: 0.975341\n",
      "[161]\tvalid_0's auc: 0.975434\n",
      "[162]\tvalid_0's auc: 0.975482\n",
      "[163]\tvalid_0's auc: 0.97551\n",
      "[164]\tvalid_0's auc: 0.975513\n",
      "[165]\tvalid_0's auc: 0.975519\n",
      "[166]\tvalid_0's auc: 0.975512\n",
      "[167]\tvalid_0's auc: 0.975524\n",
      "[168]\tvalid_0's auc: 0.975542\n",
      "[169]\tvalid_0's auc: 0.975534\n",
      "[170]\tvalid_0's auc: 0.975547\n",
      "[171]\tvalid_0's auc: 0.975572\n",
      "[172]\tvalid_0's auc: 0.975598\n",
      "[173]\tvalid_0's auc: 0.975685\n",
      "[174]\tvalid_0's auc: 0.975718\n",
      "[175]\tvalid_0's auc: 0.975747\n",
      "[176]\tvalid_0's auc: 0.975788\n",
      "[177]\tvalid_0's auc: 0.97578\n",
      "[178]\tvalid_0's auc: 0.975809\n",
      "[179]\tvalid_0's auc: 0.975808\n",
      "[180]\tvalid_0's auc: 0.975838\n",
      "[181]\tvalid_0's auc: 0.975864\n",
      "[182]\tvalid_0's auc: 0.975923\n",
      "[183]\tvalid_0's auc: 0.975908\n",
      "[184]\tvalid_0's auc: 0.975923\n",
      "[185]\tvalid_0's auc: 0.975912\n",
      "[186]\tvalid_0's auc: 0.975957\n",
      "[187]\tvalid_0's auc: 0.97603\n",
      "[188]\tvalid_0's auc: 0.976035\n",
      "[189]\tvalid_0's auc: 0.976062\n",
      "[190]\tvalid_0's auc: 0.976085\n",
      "[191]\tvalid_0's auc: 0.976082\n",
      "[192]\tvalid_0's auc: 0.976083\n",
      "[193]\tvalid_0's auc: 0.976024\n",
      "[194]\tvalid_0's auc: 0.976008\n",
      "[195]\tvalid_0's auc: 0.975979\n",
      "[196]\tvalid_0's auc: 0.975996\n",
      "[197]\tvalid_0's auc: 0.976025\n",
      "[198]\tvalid_0's auc: 0.976044\n",
      "[199]\tvalid_0's auc: 0.976088\n",
      "[200]\tvalid_0's auc: 0.976072\n",
      "[201]\tvalid_0's auc: 0.9761\n",
      "[202]\tvalid_0's auc: 0.976126\n",
      "[203]\tvalid_0's auc: 0.976132\n",
      "[204]\tvalid_0's auc: 0.976143\n",
      "[205]\tvalid_0's auc: 0.976157\n",
      "[206]\tvalid_0's auc: 0.976199\n",
      "[207]\tvalid_0's auc: 0.976153\n",
      "[208]\tvalid_0's auc: 0.976183\n",
      "[209]\tvalid_0's auc: 0.976152\n",
      "[210]\tvalid_0's auc: 0.976147\n",
      "[211]\tvalid_0's auc: 0.976184\n",
      "[212]\tvalid_0's auc: 0.976169\n",
      "[213]\tvalid_0's auc: 0.976198\n",
      "[214]\tvalid_0's auc: 0.976201\n",
      "[215]\tvalid_0's auc: 0.976194\n",
      "[216]\tvalid_0's auc: 0.976202\n",
      "[217]\tvalid_0's auc: 0.976227\n",
      "[218]\tvalid_0's auc: 0.976234\n",
      "[219]\tvalid_0's auc: 0.976255\n",
      "[220]\tvalid_0's auc: 0.976259\n",
      "[221]\tvalid_0's auc: 0.976201\n",
      "[222]\tvalid_0's auc: 0.976186\n",
      "[223]\tvalid_0's auc: 0.976205\n",
      "[224]\tvalid_0's auc: 0.976229\n",
      "[225]\tvalid_0's auc: 0.976289\n",
      "[226]\tvalid_0's auc: 0.976261\n",
      "[227]\tvalid_0's auc: 0.976282\n",
      "[228]\tvalid_0's auc: 0.976289\n",
      "[229]\tvalid_0's auc: 0.976304\n",
      "[230]\tvalid_0's auc: 0.976246\n",
      "[231]\tvalid_0's auc: 0.976289\n",
      "[232]\tvalid_0's auc: 0.976298\n",
      "[233]\tvalid_0's auc: 0.976307\n",
      "[234]\tvalid_0's auc: 0.976324\n",
      "[235]\tvalid_0's auc: 0.976386\n",
      "[236]\tvalid_0's auc: 0.976407\n",
      "[237]\tvalid_0's auc: 0.976385\n",
      "[238]\tvalid_0's auc: 0.9764\n",
      "[239]\tvalid_0's auc: 0.976443\n",
      "[240]\tvalid_0's auc: 0.976485\n",
      "[241]\tvalid_0's auc: 0.976488\n",
      "[242]\tvalid_0's auc: 0.976526\n",
      "[243]\tvalid_0's auc: 0.976559\n",
      "[244]\tvalid_0's auc: 0.976561\n",
      "[245]\tvalid_0's auc: 0.97657\n",
      "[246]\tvalid_0's auc: 0.976564\n",
      "[247]\tvalid_0's auc: 0.976549\n",
      "[248]\tvalid_0's auc: 0.976549\n",
      "[249]\tvalid_0's auc: 0.976518\n",
      "[250]\tvalid_0's auc: 0.97653\n",
      "[251]\tvalid_0's auc: 0.976552\n",
      "[252]\tvalid_0's auc: 0.976576\n",
      "[253]\tvalid_0's auc: 0.976597\n",
      "[254]\tvalid_0's auc: 0.976629\n",
      "[255]\tvalid_0's auc: 0.976599\n",
      "[256]\tvalid_0's auc: 0.976603\n",
      "[257]\tvalid_0's auc: 0.976635\n",
      "[258]\tvalid_0's auc: 0.976631\n",
      "[259]\tvalid_0's auc: 0.976583\n",
      "[260]\tvalid_0's auc: 0.976524\n",
      "[261]\tvalid_0's auc: 0.976517\n",
      "[262]\tvalid_0's auc: 0.976463\n",
      "[263]\tvalid_0's auc: 0.976486\n",
      "[264]\tvalid_0's auc: 0.97645\n",
      "[265]\tvalid_0's auc: 0.976468\n",
      "[266]\tvalid_0's auc: 0.976448\n",
      "[267]\tvalid_0's auc: 0.976409\n",
      "[268]\tvalid_0's auc: 0.976411\n",
      "[269]\tvalid_0's auc: 0.976412\n",
      "[270]\tvalid_0's auc: 0.976425\n",
      "[271]\tvalid_0's auc: 0.976448\n",
      "[272]\tvalid_0's auc: 0.976447\n",
      "[273]\tvalid_0's auc: 0.976466\n",
      "[274]\tvalid_0's auc: 0.976519\n",
      "[275]\tvalid_0's auc: 0.976517\n",
      "[276]\tvalid_0's auc: 0.976484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277]\tvalid_0's auc: 0.976481\n",
      "[278]\tvalid_0's auc: 0.976518\n",
      "[279]\tvalid_0's auc: 0.976525\n",
      "[280]\tvalid_0's auc: 0.976551\n",
      "[281]\tvalid_0's auc: 0.976569\n",
      "[282]\tvalid_0's auc: 0.976599\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's auc: 0.976635\n"
     ]
    }
   ],
   "source": [
    "lgb_ble.train(x_train, y_train['toxic'].values, valid_set_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 300000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "preds = lgb_ble.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lgbtoxicpred', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_load = np.load('lgbtoxicpred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "            dict(name=\"max_bin\", type=\"int\", bounds=dict(min=20, max=20000)),\n",
    "            dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=0.001, max=0.3)),\n",
    "            dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=4095)),\n",
    "            # dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=45000)),\n",
    "            dict(name=\"scale_pos_weight\", type=\"double\", bounds=dict(min=0.01, max=2000.0)),\n",
    "            dict(name=\"n_estimators\", type=\"int\", bounds=dict(min=10, max=10000)),\n",
    "            dict(name=\"min_child_weight\", type=\"int\", bounds=dict(min=1, max=2000)),\n",
    "            dict(name=\"subsample\", type=\"double\", bounds=dict(min=0.4, max=1)),\n",
    "            dict(name=\"bagging_fraction\", type=\"double\", bounds=dict(min=0.3, max=1)),\n",
    "            dict(name=\"max_depth\", type=\"int\", bounds=dict(min=2, max=50)),\n",
    "        ]\n",
    "# static_parameters = {'boosting_type': 'dart', 'reg_alpha': 0, 'reg_lambda': 2, 'is_unbalance': True,\n",
    "#                              'min_split_gain': 0, 'min_child_samples': 10, 'colsample_bytree': 0.8, 'subsample_freq': 3,\n",
    "#                              'subsample_for_bin': 50000,\n",
    "#                              'histogram_pool_size': detect_available_memory_for_histogram_cache()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bounds': {'max': 20000, 'min': 20}, 'name': 'max_bin', 'type': 'int'},\n",
       " {'bounds': {'max': 0.3, 'min': 0.001},\n",
       "  'name': 'learning_rate',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 4095, 'min': 100}, 'name': 'num_leaves', 'type': 'int'},\n",
       " {'bounds': {'max': 2000.0, 'min': 0.01},\n",
       "  'name': 'scale_pos_weight',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 10000, 'min': 10}, 'name': 'n_estimators', 'type': 'int'},\n",
       " {'bounds': {'max': 2000, 'min': 1},\n",
       "  'name': 'min_child_weight',\n",
       "  'type': 'int'},\n",
       " {'bounds': {'max': 1, 'min': 0.4}, 'name': 'subsample', 'type': 'double'},\n",
       " {'bounds': {'max': 1, 'min': 0.3},\n",
       "  'name': 'bagging_fraction',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 50, 'min': 2}, 'name': 'max_depth', 'type': 'int'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
