{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "import lightgbm as lgb\n",
    "\n",
    "from enum import Enum\n",
    "class ModelName(Enum):\n",
    "    XGB = 1\n",
    "    NBXGB = 2\n",
    "    LGB = 3\n",
    "    NBLGB = 4\n",
    "    LOGREG = 5\n",
    "    NBSVM = 6 # NBLOGREG\n",
    "    LSVC = 7\n",
    "    NBLSVC = 8\n",
    "    RF = 9 # random forest\n",
    "    RNN = 10\n",
    "    ONESVC = 11\n",
    "    ONELOGREG = 12\n",
    "\n",
    "\n",
    "class BaseLayerEstimator(ABC):\n",
    "    \n",
    "    def _pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def _nb(self, x_train, y_train):\n",
    "        assert isinstance(y_train, pd.DataFrame)\n",
    "        r = {}\n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            r[col] = np.log(self._pr(1, y_train[col].values, x_train) / self._pr(0, y_train[col], x_train))\n",
    "        return r\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np array\n",
    "            y_train: pd series\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, x_train):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class LightgbmBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, label_cols= None, params=None, nb=True, seed=0):\n",
    "        \"\"\"\n",
    "        constructor:\n",
    "\n",
    "            x_train: should be a np/scipy/ 2-d array or matrix. only be used when nb is true\n",
    "            y_train: should be a dataframe\n",
    "            label_cols: (list) if y_train contains multiple labels, provide the list of label names\n",
    "                e.g.: label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "            params: (dict)\n",
    "            nb: (boolean) compute naive bayes or not. (helpful for unbalanced data)\n",
    "            seed: (int) training random seed (not used currently)\n",
    "            \n",
    "        Example:\n",
    "            ll = LightgbmBLE(train_tfidf, train[label_cols], params=params, nb=True)\n",
    "            result = pd.DataFrame()\n",
    "            for col in label_cols:\n",
    "                    print(col)\n",
    "                    ll.train(train_tfidf, train[col], col)\n",
    "                    result[col] = ll.predict(test_tfidf, col)\n",
    "        \"\"\"\n",
    "        #### check naive bayes\n",
    "        if nb:\n",
    "            print('Naive Bayes is enabled')\n",
    "            self.r = self._nb(x_train, y_train)\n",
    "        else:\n",
    "            print('Naive Bayes is disabled')\n",
    "            self.r = None\n",
    "        ##### set values    \n",
    "        self.nb = nb\n",
    "        self.set_params(params)\n",
    "        self.label_cols = label_cols\n",
    "        print('LightgbmBLE is initialized')\n",
    "    \n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _pre_process(self, x, y, label=None):\n",
    "        if self.nb:\n",
    "            if label is None:\n",
    "                raise ValueError('Naive Bayes is enabled. label cannot be None.')\n",
    "            if label not in self.label_cols:\n",
    "                raise ValueError('Label not in label_cols')\n",
    "            print('apply naive bayes to feature set')\n",
    "            x = x.multiply(self.r[label])\n",
    "            if isinstance(x, csr_matrix):\n",
    "                x = x.tocsr()\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        else:\n",
    "            y = y\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train, label=None, valid_set_percent=0):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np/scipy/ 2-d array or matrix\n",
    "            y_train: should be a dataframe\n",
    "            label: (str) if not none, then it's one of the labels in the label_cols\n",
    "                    if nb is set to True when initializing, when label can not be None\n",
    "            valid_set_percent: (float, 0 to 1). \n",
    "                    0: no validation set. (imposible to use early stopping)\n",
    "                    1: use training set as validation set (to check underfitting, and early stopping)\n",
    "                    >0 and <1: use a portion of training set as validation set. (to check overfitting, and early stopping)\n",
    "        \n",
    "        \"\"\"\n",
    "        x, y = self._pre_process(x_train, y_train, label)\n",
    "        \n",
    "        if valid_set_percent != 0:\n",
    "            if valid_set_percent > 1 or valid_set_percent < 0:\n",
    "                raise ValueError('valid_set_percent must >= 0 and <= 1')\n",
    "            if valid_set_percent != 1:\n",
    "                x, x_val, y, y_val = train_test_split(x, y, test_size=valid_set_percent)\n",
    "\n",
    "\n",
    "        lgb_train = lgb.Dataset(x, y)\n",
    "        if valid_set_percent != 0:\n",
    "            if valid_set_percent == 1:\n",
    "                print('Evaluating using training set')\n",
    "                self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_train)\n",
    "            else:\n",
    "                lgb_val = lgb.Dataset(x_val, y_val)\n",
    "                print('Evaluating using validation set ({}% of training set)'.format(valid_set_percent*100))\n",
    "                self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_val)\n",
    "        else:\n",
    "            print('No evaluation set, thus not possible to use early stopping. Please train with your best params.')\n",
    "            self.model = lgb.train(self.params, lgb_train)\n",
    "        \n",
    "        \n",
    "    def predict(self, x_test, label=None):\n",
    "        x, _ = self._pre_process(x_test, y=None, label=label)\n",
    "        print('starting predicting')\n",
    "        if self.model.best_iteration > 0:\n",
    "            print('best_iteration {} is chosen.'.format(best_iteration))\n",
    "            result = self.model.predict(x, num_iteration=bst.best_iteration)\n",
    "        else:\n",
    "            result = self.model.predict(x)\n",
    "        print('predicting done')\n",
    "        return result\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tfidf_data import tfidf_data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting word\n",
      "transforming train word\n",
      "transforming test word\n",
      "fitting char\n",
      "transforming train char\n",
      "transforming test char\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, data_id = tfidf_data_process(word_ngram=(1,3), word_max=100000, char_ngram=(2, 5), char_max=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 300000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_per_label = {}\n",
    "lgb_params_per_label['toxic'] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', \n",
    "    'num_threads': 8, \n",
    "    'bagging_freq': 1, \n",
    "    'bagging_fraction': 0.9,\n",
    "    'feature_fraction': 0.6,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': -1,\n",
    "    'num_iterations': 219,\n",
    "    'num_leaves': 61, \n",
    "    'is_unbalance': False\n",
    "}\n",
    "\n",
    "lgb_params_per_label['severe_toxic'] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', \n",
    "    'num_threads': 8, \n",
    "    'bagging_freq': 1, \n",
    "    'bagging_fraction': 0.7,\n",
    "    'feature_fraction': 0.6,\n",
    "    'lambda_l1': 0.5, \n",
    "    'lambda_l2': 0.0, \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'num_iterations': 322,\n",
    "    'num_leaves': 11, \n",
    "    'is_unbalance': False\n",
    "}\n",
    "\n",
    "\n",
    "lgb_params_per_label['obscene'] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', \n",
    "    'num_threads': 8, \n",
    "    'bagging_freq': 1, \n",
    "    'bagging_fraction': 0.7,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_iterations': 274,\n",
    "    'num_leaves': 61, \n",
    "    'is_unbalance': False\n",
    "}\n",
    "\n",
    "lgb_params_per_label['threat'] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', \n",
    "    'num_threads': 8, \n",
    "    'bagging_freq': 1, \n",
    "    'bagging_fraction': 0.7,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1': 0.5, \n",
    "    'lambda_l2': 0.0, \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_iterations': 208,\n",
    "    'num_leaves': 11, \n",
    "    'is_unbalance': False\n",
    "}\n",
    "\n",
    "lgb_params_per_label['insult'] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', \n",
    "    'num_threads': 8, \n",
    "    'bagging_freq': 1, \n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.6,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.5, \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_iterations': 454,\n",
    "    'num_leaves': 11, \n",
    "    'is_unbalance': False\n",
    "}\n",
    "\n",
    "lgb_params_per_label['identity_hate'] = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', \n",
    "    'num_threads': 8, \n",
    "    'bagging_freq': 1, \n",
    "    'bagging_fraction': 0.7,\n",
    "    'feature_fraction': 0.6,\n",
    "    'lambda_l1': 0.0, \n",
    "    'lambda_l2': 0.0, \n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': -1,\n",
    "    'num_iterations': 191,\n",
    "    'num_leaves': 61, \n",
    "    'is_unbalance': False\n",
    "}\n",
    "    \n",
    "    \n",
    "#     #'learning_rate': 0.05,\n",
    "#     'is_unbalance': True,\n",
    "#     'early_stopping_round': 25,\n",
    "#     'max_depth': -1,\n",
    "#     'num_boost_round': 3000,\n",
    "#     'application': 'binary',\n",
    "#     'num_leaves': 63,\n",
    "#     'verbosity': 10,\n",
    "#     'metric': 'auc',\n",
    "#     'data_random_seed': 2,\n",
    "#     'bagging_fraction': 1,\n",
    "#     'feature_fraction': 0.6,\n",
    "#     'nthread': 4\n",
    "# #     'lambda_l1': 1,\n",
    "# #     'lambda_l2': 1\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is enabled\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "lgb_ble = LightgbmBLE(x_train, y_train, label_cols=label_cols, nb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((x_test.shape[0], len(label_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply naive bayes to feature set\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply naive bayes to feature set\n",
      "starting predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/basic.py:447: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting done\n",
      "apply naive bayes to feature set\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "apply naive bayes to feature set\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "apply naive bayes to feature set\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "apply naive bayes to feature set\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "apply naive bayes to feature set\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(label_cols):\n",
    "    lgb_ble.set_params(lgb_params_per_label[label])\n",
    "    lgb_ble.train(x_train, y_train[label].values, label)\n",
    "    preds[:, i] = lgb_ble.predict(x_test, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520650080\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = preds\n",
    "import time\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./BaseEstPreds/' + 'test123_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "      <td>153164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.203953</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>0.121976</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.095111</td>\n",
       "      <td>0.013132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.352209</td>\n",
       "      <td>0.074662</td>\n",
       "      <td>0.286152</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>0.219306</td>\n",
       "      <td>0.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.006668</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.191338</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.014694</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.032832</td>\n",
       "      <td>0.001296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967314</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>0.994893</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>0.991052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  153164.000000  153164.000000  153164.000000  153164.000000   \n",
       "mean        0.203953       0.017368       0.121976       0.003625   \n",
       "std         0.352209       0.074662       0.286152       0.037611   \n",
       "min         0.000102       0.000054       0.000103       0.000081   \n",
       "25%         0.002085       0.000337       0.000739       0.000234   \n",
       "50%         0.007873       0.000697       0.001715       0.000400   \n",
       "75%         0.191338       0.002289       0.014694       0.000780   \n",
       "max         1.000000       0.967314       0.999962       0.994893   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  153164.000000  153164.000000  \n",
       "mean        0.095111       0.013132  \n",
       "std         0.219306       0.081633  \n",
       "min         0.000374       0.000086  \n",
       "25%         0.002915       0.000260  \n",
       "50%         0.006668       0.000467  \n",
       "75%         0.032832       0.001296  \n",
       "max         0.999849       0.991052  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[label_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.870293\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[2]\tvalid_0's auc: 0.879855\n",
      "[3]\tvalid_0's auc: 0.890069\n",
      "[4]\tvalid_0's auc: 0.895269\n",
      "[5]\tvalid_0's auc: 0.900742\n",
      "[6]\tvalid_0's auc: 0.904055\n",
      "[7]\tvalid_0's auc: 0.90557\n",
      "[8]\tvalid_0's auc: 0.908777\n",
      "[9]\tvalid_0's auc: 0.911911\n",
      "[10]\tvalid_0's auc: 0.912996\n",
      "[11]\tvalid_0's auc: 0.916038\n",
      "[12]\tvalid_0's auc: 0.918541\n",
      "[13]\tvalid_0's auc: 0.921163\n",
      "[14]\tvalid_0's auc: 0.922745\n",
      "[15]\tvalid_0's auc: 0.925085\n",
      "[16]\tvalid_0's auc: 0.926786\n",
      "[17]\tvalid_0's auc: 0.927603\n",
      "[18]\tvalid_0's auc: 0.931456\n",
      "[19]\tvalid_0's auc: 0.932031\n",
      "[20]\tvalid_0's auc: 0.934025\n",
      "[21]\tvalid_0's auc: 0.93553\n",
      "[22]\tvalid_0's auc: 0.936878\n",
      "[23]\tvalid_0's auc: 0.938081\n",
      "[24]\tvalid_0's auc: 0.93911\n",
      "[25]\tvalid_0's auc: 0.940473\n",
      "[26]\tvalid_0's auc: 0.941336\n",
      "[27]\tvalid_0's auc: 0.942148\n",
      "[28]\tvalid_0's auc: 0.942864\n",
      "[29]\tvalid_0's auc: 0.943854\n",
      "[30]\tvalid_0's auc: 0.944219\n",
      "[31]\tvalid_0's auc: 0.945139\n",
      "[32]\tvalid_0's auc: 0.947068\n",
      "[33]\tvalid_0's auc: 0.947943\n",
      "[34]\tvalid_0's auc: 0.949299\n",
      "[35]\tvalid_0's auc: 0.949881\n",
      "[36]\tvalid_0's auc: 0.950709\n",
      "[37]\tvalid_0's auc: 0.951376\n",
      "[38]\tvalid_0's auc: 0.952251\n",
      "[39]\tvalid_0's auc: 0.95297\n",
      "[40]\tvalid_0's auc: 0.953703\n",
      "[41]\tvalid_0's auc: 0.954556\n",
      "[42]\tvalid_0's auc: 0.955345\n",
      "[43]\tvalid_0's auc: 0.955883\n",
      "[44]\tvalid_0's auc: 0.956224\n",
      "[45]\tvalid_0's auc: 0.956448\n",
      "[46]\tvalid_0's auc: 0.957143\n",
      "[47]\tvalid_0's auc: 0.958222\n",
      "[48]\tvalid_0's auc: 0.958521\n",
      "[49]\tvalid_0's auc: 0.958812\n",
      "[50]\tvalid_0's auc: 0.959535\n",
      "[51]\tvalid_0's auc: 0.960217\n",
      "[52]\tvalid_0's auc: 0.960455\n",
      "[53]\tvalid_0's auc: 0.960892\n",
      "[54]\tvalid_0's auc: 0.961406\n",
      "[55]\tvalid_0's auc: 0.962039\n",
      "[56]\tvalid_0's auc: 0.962328\n",
      "[57]\tvalid_0's auc: 0.962745\n",
      "[58]\tvalid_0's auc: 0.963123\n",
      "[59]\tvalid_0's auc: 0.963432\n",
      "[60]\tvalid_0's auc: 0.963828\n",
      "[61]\tvalid_0's auc: 0.964368\n",
      "[62]\tvalid_0's auc: 0.96478\n",
      "[63]\tvalid_0's auc: 0.964981\n",
      "[64]\tvalid_0's auc: 0.965402\n",
      "[65]\tvalid_0's auc: 0.965689\n",
      "[66]\tvalid_0's auc: 0.965989\n",
      "[67]\tvalid_0's auc: 0.966119\n",
      "[68]\tvalid_0's auc: 0.966557\n",
      "[69]\tvalid_0's auc: 0.966636\n",
      "[70]\tvalid_0's auc: 0.966908\n",
      "[71]\tvalid_0's auc: 0.967093\n",
      "[72]\tvalid_0's auc: 0.967389\n",
      "[73]\tvalid_0's auc: 0.967575\n",
      "[74]\tvalid_0's auc: 0.967705\n",
      "[75]\tvalid_0's auc: 0.967973\n",
      "[76]\tvalid_0's auc: 0.968204\n",
      "[77]\tvalid_0's auc: 0.968479\n",
      "[78]\tvalid_0's auc: 0.968569\n",
      "[79]\tvalid_0's auc: 0.968892\n",
      "[80]\tvalid_0's auc: 0.96902\n",
      "[81]\tvalid_0's auc: 0.96913\n",
      "[82]\tvalid_0's auc: 0.969359\n",
      "[83]\tvalid_0's auc: 0.969591\n",
      "[84]\tvalid_0's auc: 0.969768\n",
      "[85]\tvalid_0's auc: 0.969915\n",
      "[86]\tvalid_0's auc: 0.970017\n",
      "[87]\tvalid_0's auc: 0.970074\n",
      "[88]\tvalid_0's auc: 0.97031\n",
      "[89]\tvalid_0's auc: 0.970477\n",
      "[90]\tvalid_0's auc: 0.970589\n",
      "[91]\tvalid_0's auc: 0.970694\n",
      "[92]\tvalid_0's auc: 0.970754\n",
      "[93]\tvalid_0's auc: 0.97082\n",
      "[94]\tvalid_0's auc: 0.970928\n",
      "[95]\tvalid_0's auc: 0.971038\n",
      "[96]\tvalid_0's auc: 0.971227\n",
      "[97]\tvalid_0's auc: 0.971362\n",
      "[98]\tvalid_0's auc: 0.971456\n",
      "[99]\tvalid_0's auc: 0.971534\n",
      "[100]\tvalid_0's auc: 0.971644\n",
      "[101]\tvalid_0's auc: 0.971797\n",
      "[102]\tvalid_0's auc: 0.971859\n",
      "[103]\tvalid_0's auc: 0.971918\n",
      "[104]\tvalid_0's auc: 0.972091\n",
      "[105]\tvalid_0's auc: 0.972186\n",
      "[106]\tvalid_0's auc: 0.972321\n",
      "[107]\tvalid_0's auc: 0.972409\n",
      "[108]\tvalid_0's auc: 0.972481\n",
      "[109]\tvalid_0's auc: 0.972599\n",
      "[110]\tvalid_0's auc: 0.97274\n",
      "[111]\tvalid_0's auc: 0.972768\n",
      "[112]\tvalid_0's auc: 0.972911\n",
      "[113]\tvalid_0's auc: 0.972975\n",
      "[114]\tvalid_0's auc: 0.973063\n",
      "[115]\tvalid_0's auc: 0.973152\n",
      "[116]\tvalid_0's auc: 0.97316\n",
      "[117]\tvalid_0's auc: 0.973243\n",
      "[118]\tvalid_0's auc: 0.973368\n",
      "[119]\tvalid_0's auc: 0.973423\n",
      "[120]\tvalid_0's auc: 0.973436\n",
      "[121]\tvalid_0's auc: 0.973539\n",
      "[122]\tvalid_0's auc: 0.973576\n",
      "[123]\tvalid_0's auc: 0.973675\n",
      "[124]\tvalid_0's auc: 0.973742\n",
      "[125]\tvalid_0's auc: 0.973817\n",
      "[126]\tvalid_0's auc: 0.973855\n",
      "[127]\tvalid_0's auc: 0.973954\n",
      "[128]\tvalid_0's auc: 0.974055\n",
      "[129]\tvalid_0's auc: 0.974147\n",
      "[130]\tvalid_0's auc: 0.97418\n",
      "[131]\tvalid_0's auc: 0.974212\n",
      "[132]\tvalid_0's auc: 0.974308\n",
      "[133]\tvalid_0's auc: 0.974376\n",
      "[134]\tvalid_0's auc: 0.974461\n",
      "[135]\tvalid_0's auc: 0.974498\n",
      "[136]\tvalid_0's auc: 0.974544\n",
      "[137]\tvalid_0's auc: 0.974542\n",
      "[138]\tvalid_0's auc: 0.974635\n",
      "[139]\tvalid_0's auc: 0.974729\n",
      "[140]\tvalid_0's auc: 0.974717\n",
      "[141]\tvalid_0's auc: 0.974778\n",
      "[142]\tvalid_0's auc: 0.974838\n",
      "[143]\tvalid_0's auc: 0.974821\n",
      "[144]\tvalid_0's auc: 0.974833\n",
      "[145]\tvalid_0's auc: 0.974874\n",
      "[146]\tvalid_0's auc: 0.974887\n",
      "[147]\tvalid_0's auc: 0.974962\n",
      "[148]\tvalid_0's auc: 0.974988\n",
      "[149]\tvalid_0's auc: 0.975054\n",
      "[150]\tvalid_0's auc: 0.975057\n",
      "[151]\tvalid_0's auc: 0.975075\n",
      "[152]\tvalid_0's auc: 0.975103\n",
      "[153]\tvalid_0's auc: 0.975213\n",
      "[154]\tvalid_0's auc: 0.975266\n",
      "[155]\tvalid_0's auc: 0.975266\n",
      "[156]\tvalid_0's auc: 0.975304\n",
      "[157]\tvalid_0's auc: 0.975316\n",
      "[158]\tvalid_0's auc: 0.975307\n",
      "[159]\tvalid_0's auc: 0.975345\n",
      "[160]\tvalid_0's auc: 0.975341\n",
      "[161]\tvalid_0's auc: 0.975434\n",
      "[162]\tvalid_0's auc: 0.975482\n",
      "[163]\tvalid_0's auc: 0.97551\n",
      "[164]\tvalid_0's auc: 0.975513\n",
      "[165]\tvalid_0's auc: 0.975519\n",
      "[166]\tvalid_0's auc: 0.975512\n",
      "[167]\tvalid_0's auc: 0.975524\n",
      "[168]\tvalid_0's auc: 0.975542\n",
      "[169]\tvalid_0's auc: 0.975534\n",
      "[170]\tvalid_0's auc: 0.975547\n",
      "[171]\tvalid_0's auc: 0.975572\n",
      "[172]\tvalid_0's auc: 0.975598\n",
      "[173]\tvalid_0's auc: 0.975685\n",
      "[174]\tvalid_0's auc: 0.975718\n",
      "[175]\tvalid_0's auc: 0.975747\n",
      "[176]\tvalid_0's auc: 0.975788\n",
      "[177]\tvalid_0's auc: 0.97578\n",
      "[178]\tvalid_0's auc: 0.975809\n",
      "[179]\tvalid_0's auc: 0.975808\n",
      "[180]\tvalid_0's auc: 0.975838\n",
      "[181]\tvalid_0's auc: 0.975864\n",
      "[182]\tvalid_0's auc: 0.975923\n",
      "[183]\tvalid_0's auc: 0.975908\n",
      "[184]\tvalid_0's auc: 0.975923\n",
      "[185]\tvalid_0's auc: 0.975912\n",
      "[186]\tvalid_0's auc: 0.975957\n",
      "[187]\tvalid_0's auc: 0.97603\n",
      "[188]\tvalid_0's auc: 0.976035\n",
      "[189]\tvalid_0's auc: 0.976062\n",
      "[190]\tvalid_0's auc: 0.976085\n",
      "[191]\tvalid_0's auc: 0.976082\n",
      "[192]\tvalid_0's auc: 0.976083\n",
      "[193]\tvalid_0's auc: 0.976024\n",
      "[194]\tvalid_0's auc: 0.976008\n",
      "[195]\tvalid_0's auc: 0.975979\n",
      "[196]\tvalid_0's auc: 0.975996\n",
      "[197]\tvalid_0's auc: 0.976025\n",
      "[198]\tvalid_0's auc: 0.976044\n",
      "[199]\tvalid_0's auc: 0.976088\n",
      "[200]\tvalid_0's auc: 0.976072\n",
      "[201]\tvalid_0's auc: 0.9761\n",
      "[202]\tvalid_0's auc: 0.976126\n",
      "[203]\tvalid_0's auc: 0.976132\n",
      "[204]\tvalid_0's auc: 0.976143\n",
      "[205]\tvalid_0's auc: 0.976157\n",
      "[206]\tvalid_0's auc: 0.976199\n",
      "[207]\tvalid_0's auc: 0.976153\n",
      "[208]\tvalid_0's auc: 0.976183\n",
      "[209]\tvalid_0's auc: 0.976152\n",
      "[210]\tvalid_0's auc: 0.976147\n",
      "[211]\tvalid_0's auc: 0.976184\n",
      "[212]\tvalid_0's auc: 0.976169\n",
      "[213]\tvalid_0's auc: 0.976198\n",
      "[214]\tvalid_0's auc: 0.976201\n",
      "[215]\tvalid_0's auc: 0.976194\n",
      "[216]\tvalid_0's auc: 0.976202\n",
      "[217]\tvalid_0's auc: 0.976227\n",
      "[218]\tvalid_0's auc: 0.976234\n",
      "[219]\tvalid_0's auc: 0.976255\n",
      "[220]\tvalid_0's auc: 0.976259\n",
      "[221]\tvalid_0's auc: 0.976201\n",
      "[222]\tvalid_0's auc: 0.976186\n",
      "[223]\tvalid_0's auc: 0.976205\n",
      "[224]\tvalid_0's auc: 0.976229\n",
      "[225]\tvalid_0's auc: 0.976289\n",
      "[226]\tvalid_0's auc: 0.976261\n",
      "[227]\tvalid_0's auc: 0.976282\n",
      "[228]\tvalid_0's auc: 0.976289\n",
      "[229]\tvalid_0's auc: 0.976304\n",
      "[230]\tvalid_0's auc: 0.976246\n",
      "[231]\tvalid_0's auc: 0.976289\n",
      "[232]\tvalid_0's auc: 0.976298\n",
      "[233]\tvalid_0's auc: 0.976307\n",
      "[234]\tvalid_0's auc: 0.976324\n",
      "[235]\tvalid_0's auc: 0.976386\n",
      "[236]\tvalid_0's auc: 0.976407\n",
      "[237]\tvalid_0's auc: 0.976385\n",
      "[238]\tvalid_0's auc: 0.9764\n",
      "[239]\tvalid_0's auc: 0.976443\n",
      "[240]\tvalid_0's auc: 0.976485\n",
      "[241]\tvalid_0's auc: 0.976488\n",
      "[242]\tvalid_0's auc: 0.976526\n",
      "[243]\tvalid_0's auc: 0.976559\n",
      "[244]\tvalid_0's auc: 0.976561\n",
      "[245]\tvalid_0's auc: 0.97657\n",
      "[246]\tvalid_0's auc: 0.976564\n",
      "[247]\tvalid_0's auc: 0.976549\n",
      "[248]\tvalid_0's auc: 0.976549\n",
      "[249]\tvalid_0's auc: 0.976518\n",
      "[250]\tvalid_0's auc: 0.97653\n",
      "[251]\tvalid_0's auc: 0.976552\n",
      "[252]\tvalid_0's auc: 0.976576\n",
      "[253]\tvalid_0's auc: 0.976597\n",
      "[254]\tvalid_0's auc: 0.976629\n",
      "[255]\tvalid_0's auc: 0.976599\n",
      "[256]\tvalid_0's auc: 0.976603\n",
      "[257]\tvalid_0's auc: 0.976635\n",
      "[258]\tvalid_0's auc: 0.976631\n",
      "[259]\tvalid_0's auc: 0.976583\n",
      "[260]\tvalid_0's auc: 0.976524\n",
      "[261]\tvalid_0's auc: 0.976517\n",
      "[262]\tvalid_0's auc: 0.976463\n",
      "[263]\tvalid_0's auc: 0.976486\n",
      "[264]\tvalid_0's auc: 0.97645\n",
      "[265]\tvalid_0's auc: 0.976468\n",
      "[266]\tvalid_0's auc: 0.976448\n",
      "[267]\tvalid_0's auc: 0.976409\n",
      "[268]\tvalid_0's auc: 0.976411\n",
      "[269]\tvalid_0's auc: 0.976412\n",
      "[270]\tvalid_0's auc: 0.976425\n",
      "[271]\tvalid_0's auc: 0.976448\n",
      "[272]\tvalid_0's auc: 0.976447\n",
      "[273]\tvalid_0's auc: 0.976466\n",
      "[274]\tvalid_0's auc: 0.976519\n",
      "[275]\tvalid_0's auc: 0.976517\n",
      "[276]\tvalid_0's auc: 0.976484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277]\tvalid_0's auc: 0.976481\n",
      "[278]\tvalid_0's auc: 0.976518\n",
      "[279]\tvalid_0's auc: 0.976525\n",
      "[280]\tvalid_0's auc: 0.976551\n",
      "[281]\tvalid_0's auc: 0.976569\n",
      "[282]\tvalid_0's auc: 0.976599\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's auc: 0.976635\n"
     ]
    }
   ],
   "source": [
    "lgb_ble.train(x_train, y_train['toxic'].values, valid_set_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 300000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "preds = lgb_ble.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('lgbtoxicpred', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_load = np.load('lgbtoxicpred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "            dict(name=\"max_bin\", type=\"int\", bounds=dict(min=20, max=20000)),\n",
    "            dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=0.001, max=0.3)),\n",
    "            dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=4095)),\n",
    "            # dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=45000)),\n",
    "            dict(name=\"scale_pos_weight\", type=\"double\", bounds=dict(min=0.01, max=2000.0)),\n",
    "            dict(name=\"n_estimators\", type=\"int\", bounds=dict(min=10, max=10000)),\n",
    "            dict(name=\"min_child_weight\", type=\"int\", bounds=dict(min=1, max=2000)),\n",
    "            dict(name=\"subsample\", type=\"double\", bounds=dict(min=0.4, max=1)),\n",
    "            dict(name=\"bagging_fraction\", type=\"double\", bounds=dict(min=0.3, max=1)),\n",
    "            dict(name=\"max_depth\", type=\"int\", bounds=dict(min=2, max=50)),\n",
    "        ]\n",
    "# static_parameters = {'boosting_type': 'dart', 'reg_alpha': 0, 'reg_lambda': 2, 'is_unbalance': True,\n",
    "#                              'min_split_gain': 0, 'min_child_samples': 10, 'colsample_bytree': 0.8, 'subsample_freq': 3,\n",
    "#                              'subsample_for_bin': 50000,\n",
    "#                              'histogram_pool_size': detect_available_memory_for_histogram_cache()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bounds': {'max': 20000, 'min': 20}, 'name': 'max_bin', 'type': 'int'},\n",
       " {'bounds': {'max': 0.3, 'min': 0.001},\n",
       "  'name': 'learning_rate',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 4095, 'min': 100}, 'name': 'num_leaves', 'type': 'int'},\n",
       " {'bounds': {'max': 2000.0, 'min': 0.01},\n",
       "  'name': 'scale_pos_weight',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 10000, 'min': 10}, 'name': 'n_estimators', 'type': 'int'},\n",
       " {'bounds': {'max': 2000, 'min': 1},\n",
       "  'name': 'min_child_weight',\n",
       "  'type': 'int'},\n",
       " {'bounds': {'max': 1, 'min': 0.4}, 'name': 'subsample', 'type': 'double'},\n",
       " {'bounds': {'max': 1, 'min': 0.3},\n",
       "  'name': 'bagging_fraction',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 50, 'min': 2}, 'name': 'max_depth', 'type': 'int'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
