{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from enum import Enum\n",
    "class ModelName(Enum):\n",
    "    XGB = 1\n",
    "    NBXGB = 2\n",
    "    LGB = 3\n",
    "    NBLGB = 4\n",
    "    LOGREG = 5\n",
    "    NBSVM = 6\n",
    "    NBLSVC = 7\n",
    "    RF = 8 # random forest\n",
    "    RNN = 9\n",
    "    ONESVC = 10\n",
    "    ONELOGREG = 11\n",
    "\n",
    "\n",
    "class BaseLayerEstimator(ABC):\n",
    "    \n",
    "    def _pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def _nb(self, x_train, y_train):\n",
    "        assert isinstance(y_train, pd.DataFrame)\n",
    "        r = {}\n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            r[col] = np.log(self._pr(1, y_train[col].values, x_train) / self._pr(0, y_train[col], x_train))\n",
    "        return r\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np array\n",
    "            y_train: pd series\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, x_train):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "class OneVSOneRegBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, model='logistic'):\n",
    "        \"\"\"\n",
    "        x_train: sparse matrix, raw tfidf\n",
    "        y_train: dataframe, with only label columns. should be 6 columns in total\n",
    "        model: only support logistic or svc\n",
    "        \"\"\"\n",
    "        self.r = {}\n",
    "        self.setModelName(model)\n",
    "        assert self.model_name in ['logistic', 'svc']\n",
    "        self.param = {}\n",
    "        self.param['logistic'] = {'identity_hate': 9.0,\n",
    "                                     'insult': 1.5,\n",
    "                                     'obscene': 1.0,\n",
    "                                     'severe_toxic': 4.0,\n",
    "                                     'threat': 9.0,\n",
    "                                     'toxic': 2.7}\n",
    "        self.param['svc'] = {'identity_hate': 0.9,\n",
    "                             'insult': 0.15,\n",
    "                             'obscene': 0.15,\n",
    "                             'severe_toxic': 0.15,\n",
    "                             'threat': 1.0,\n",
    "                             'toxic': 0.29}\n",
    "        \n",
    "        \n",
    "        \n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            self.r[col] = np.log(self.pr(1, y_train[col].values, x_train) / self.pr(0, y_train[col], x_train))\n",
    "        print('initializing done')\n",
    "        print('OneVsOne is using {} kernel'.format(self.model_name))\n",
    "        \n",
    "    def setModelName(self, name):\n",
    "        self.model_name = name\n",
    "        assert self.model_name in ['logistic', 'svc']\n",
    "        print('OneVsOne is using {} kernel'.format(self.model_name))\n",
    "        \n",
    "    def pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def oneVsOneSplit(self, x_train, y_train, label):\n",
    "        print('Starting One vs One dataset splitting')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        model_train = x_train[np.array(y_train == 1)]\n",
    "        y_model_train = y_train[np.array(y_train == 1)]\n",
    "        non_model_train = x_train[np.array(y_train == 0)]\n",
    "        non_model_train = non_model_train[:model_train.shape[0]]\n",
    "        y_non_model_train = y_train[np.array(y_train == 0)]\n",
    "        y_non_model_train = y_non_model_train[:model_train.shape[0]]\n",
    "        x_model_stack = vstack([model_train, non_model_train])\n",
    "        y_model_stack = np.concatenate([y_model_train, y_non_model_train])\n",
    "        x_nb = x_model_stack.multiply(self.r[label]).tocsr()\n",
    "        y_nb = y_model_stack\n",
    "        print('splitting done!')\n",
    "        return (x_nb, y_nb)\n",
    "    \n",
    "    def train(self, x_train, y_train, label):\n",
    "        ### construct one vs one\n",
    "        x_nb, y_nb = self.oneVsOneSplit(x_train, y_train, label)\n",
    "        ### start training\n",
    "        if self.model_name is 'logistic':\n",
    "            print('start training logistic regression')\n",
    "            self.model = LogisticRegression(C=self.param['logistic'][label])\n",
    "            self.model.fit(x_nb, y_nb)\n",
    "            print('training done')\n",
    "            \n",
    "        else:\n",
    "            print('start training linear svc regression')\n",
    "            lsvc = LinearSVC(C=self.param['svc'][label])\n",
    "            self.model = CalibratedClassifierCV(lsvc) \n",
    "            self.model.fit(x_nb, y_nb)\n",
    "            print('training done')\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self, x_test, label):\n",
    "        print('applying naive bayes to dataset')\n",
    "        x_nb_test = x_test.multiply(self.r[label]).tocsr()\n",
    "        print('predicting')\n",
    "        pred = self.model.predict_proba(x_nb_test)[:,1]\n",
    "        print('predicting done')\n",
    "        return pred\n",
    "    \n",
    "##### example        \n",
    "# aa = OneVSOneReg(train_tfidf, train[label_cols], model='logistic')\n",
    "# aa.setModelName('svc')\n",
    "# aa.train(train_tfidf,train['toxic'], 'toxic')\n",
    "# aa.predict(test_tfidf, 'toxic')\n",
    "\n",
    "\n",
    "\n",
    "class SklearnBLE(BaseLayerEstimator):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "class NbSvmBLE(BaseLayerEstimator, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, mode=ModelName.NBSVM, seed=0, params=None):\n",
    "        self._mode = mode\n",
    "        params['random_state'] = seed\n",
    "        self._params = params\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        #return self._clf.predict(x.multiply(self._r))\n",
    "        return self._clf.predict_proba(x.multiply(self._r))[:,1] # chance of being 1 ([:,0] chance of being 0)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        #self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        self._clf = LogisticRegression(**self._params).fit(x_nb, y)\n",
    "        if self._mode == ModelName.NBLSVC:\n",
    "            self._clf = CalibratedClassifierCV(LinearSVC(**self._params)).fit(x_nb, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.fit(x_train, y_train)\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        return self._clf.feature_importance\n",
    "\n",
    "    \n",
    "class XgbBLE(BaseLayerEstimator):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict_proba(xgb.DMatrix(x))[:,1]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "class LightgbmBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, params=None, nb=True, seed=0):\n",
    "        \"\"\"\n",
    "        constructor:\n",
    "\n",
    "            x_train: should be a np/scipy/ 2-d array or matrix. only be used when nb is true\n",
    "            y_train: should be a dataframe\n",
    "            \n",
    "        Example:\n",
    "            ll = LightgbmBLE(train_tfidf, train[label_cols], params=params, nb=True)\n",
    "            result = pd.DataFrame()\n",
    "            for col in label_cols:\n",
    "                    print(col)\n",
    "                    ll.train(train_tfidf, train[col], col)\n",
    "                    result[col] = ll.predict(test_tfidf, col)\n",
    "        \"\"\"\n",
    "        #### check naive bayes\n",
    "        if nb:\n",
    "            print('Naive Bayes is enabled')\n",
    "            self.r = self._nb(x_train, y_train)\n",
    "        else:\n",
    "            print('Naive Bayes is disabled')\n",
    "            self.r = None\n",
    "        ##### set values    \n",
    "        self.nb = nb\n",
    "        self.set_params(params)\n",
    "        print('LightgbmBLE is initialized')\n",
    "    \n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _pre_process(self, x_train, y_train, label=None):\n",
    "        if self.nb:\n",
    "            if label is None:\n",
    "                raise ValueError('Naive Bayes is enabled. label cannot be None.')\n",
    "            print('apply naive bayes to feature set')\n",
    "            x = x_train.multiply(self.r[label])\n",
    "            if isinstance(x_train, csr_matrix):\n",
    "                x = x.tocsr()\n",
    "        else:\n",
    "            x = x_train\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y = y_train.values\n",
    "        else:\n",
    "            y = y_train\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train, label=None, valid_set_percent=0):\n",
    "        x, y = self._pre_process(x_train, y_train, label)\n",
    "        \n",
    "        if valid_set_percent != 0:\n",
    "            if valid_set_percent > 1 or valid_set_percent < 0:\n",
    "                raise ValueError('valid_set_percent must >= 0 and <= 1')\n",
    "            x, x_val, y, y_val = train_test_split(x, y, test_size=valid_set_percent)\n",
    "\n",
    "\n",
    "        lgb_train = lgb.Dataset(x, y)\n",
    "        if valid_set_percent != 0:\n",
    "            lgb_val = lgb.Dataset(x_val, y_val)\n",
    "            self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_val)\n",
    "        else:\n",
    "            self.model = lgb.train(self.params, lgb_train)\n",
    "        \n",
    "        \n",
    "    def predict(self, x_train, label=None):\n",
    "        x, _ = self._pre_process(x_train, y_train=None, label=label)\n",
    "        print('starting predicting')\n",
    "        result = self.model.predict(x)\n",
    "        print('predicting done')\n",
    "        return result\n",
    "        \n",
    "            \n",
    "\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "class RnnBLE(BaseLayerEstimator):\n",
    "    def __init__(self, window_length, n_features, label_cols, rnn_units=50, dense_units=50, dropout=0.1, mode='LSTM', bidirection=True, batch_size=32, epochs=2):\n",
    "        self._model = RnnBLE.get_lstm_model(window_length, n_features, label_cols, rnn_units, dense_units, dropout, mode, bidirection)\n",
    "        self._batch_size = batch_size\n",
    "        self._epochs = epochs\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_lstm_model(window_length, n_features, label_cols, rnn_units, dense_units, dropout, mode, bidirection):\n",
    "        input = Input(shape=(window_length, n_features))\n",
    "        rnn_layer = LSTM(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)\n",
    "        if mode == 'GRU':\n",
    "            rnn_layer = GRU(rnn_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)\n",
    "        if bidirection:\n",
    "            x = Bidirectional(rnn_layer)(input)\n",
    "        else:\n",
    "            x = rnn_layer(input)\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "        x = Dense(dense_units, activation='relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Dense(len(label_cols), activation='sigmoid')(x)\n",
    "        model = Model(inputs=input, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self._model.fit(x_train, y_train, batch_size=self._batch_size, epochs=self._epochs)\n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self._model.predict(x)#, batch_size=1024)\n",
    "    \n",
    "    \n",
    "    \n",
    "class BaseLayerDataRepo():\n",
    "    def __init__(self):\n",
    "        self._data_repo = {}\n",
    "    \n",
    "    def add_data(self, data_id, x_train, x_test, y_train, label_cols, compatible_model=[ModelName.LOGREG], rnn_data=False):\n",
    "        \"\"\"\n",
    "        x_train, x_test: ndarray\n",
    "        y_train: pd df\n",
    "        \"\"\"\n",
    "        temp = {}\n",
    "        \n",
    "        temp['data_id'] = data_id\n",
    "        temp['x_train'] = x_train\n",
    "        temp['x_test'] = x_test\n",
    "        temp['labes_cols'] = label_cols\n",
    "        temp['compatible_model'] = set(compatible_model)\n",
    "        \n",
    "        if rnn_data: \n",
    "            temp['y_train'] = y_train # here y_train is a df\n",
    "        else:\n",
    "            label_dict = {}\n",
    "            for col in label_cols:\n",
    "                label_dict[col] = y_train[col]\n",
    "            temp['y_train'] = label_dict # hence y_train is a dict with labels as keys\n",
    "        \n",
    "        self._data_repo[data_id] = temp\n",
    "    \n",
    "    def get_data(self, data_id):\n",
    "        return self._data_repo[data_id]\n",
    "    \n",
    "    def remove_data(self, data_id):\n",
    "        self._data_repo.pop(data_id, None)\n",
    "        \n",
    "    def get_compatible_model(self, data_id):\n",
    "        return self._data_repo[data_id]['compatible_model']\n",
    "    \n",
    "    def remove_compatible_model(self, data_id, model_name):\n",
    "        return self._data_repo[data_id]['compatible_model'].discard(model_name)\n",
    "    \n",
    "    def add_compatible_model(self, data_id, model_name):\n",
    "        return self._data_repo[data_id]['compatible_model'].add(model_name)\n",
    "                  \n",
    "    def get_data_by_compatible_model(self, model_name):\n",
    "        data_to_return = []\n",
    "        for data_id in self._data_repo.keys():\n",
    "            data = self._data_repo[data_id]\n",
    "            if model_name in data['compatible_model']:\n",
    "                data_to_return.append(data)\n",
    "        return data_to_return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data_repo)\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = ''\n",
    "        for data_id in self._data_repo.keys():\n",
    "            output+='data_id: {:20} \\n\\tx_train: {}\\tx_test: {}\\n\\ty_train type: {}\\n\\tcompatible_model: {}\\n '\\\n",
    "            .format(data_id, self._data_repo[data_id]['x_train'].shape, \\\n",
    "                    self._data_repo[data_id]['x_test'].shape, \\\n",
    "                    type(self._data_repo[data_id]['y_train']), \\\n",
    "                    self._data_repo[data_id]['compatible_model'])\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "import copy\n",
    "class BaseLayerResultsRepo:\n",
    "    def __init__(self, label_cols=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], load_from_file=True):\n",
    "        self._layer1_oof_train = {}\n",
    "        self._layer1_oof_test = {}\n",
    "        for label in label_cols:\n",
    "            self._layer1_oof_train[label] = []\n",
    "            self._layer1_oof_test[label] = []\n",
    "        self._base_layer_est_preds = {}\n",
    "        self._model_data_id_list = []\n",
    "        self._base_layer_est_scores = {}\n",
    "        self._label_cols = label_cols\n",
    "        self._save_lock = False # will be set to True if remove() is invoked successfully\n",
    "        if load_from_file:\n",
    "            print('load from file')\n",
    "            self._layer1_oof_train = load_obj('13models_layer1_oof_train')\n",
    "            self._layer1_oof_test = load_obj('13models_layer1_oof_test')\n",
    "            self._base_layer_est_preds = load_obj('13models_base_layer_est_preds')\n",
    "            self._model_data_id_list = load_obj('13models_model_data_id_list')\n",
    "            self._base_layer_est_scores = load_obj('13models_base_layer_est_scores')\n",
    "\n",
    "    def get_model_data_id_list(self):\n",
    "        return self._model_data_id_list\n",
    "    \n",
    "    def add(self, layer1_oof_train, layer1_oof_test, base_layer_est_preds, model_data_id_list):\n",
    "        assert type(layer1_oof_train) == dict\n",
    "        assert len(list(layer1_oof_train)) == len(self._label_cols)\n",
    "        assert set(list(layer1_oof_train)) - set(self._label_cols) == set()\n",
    "        assert type(layer1_oof_test) == dict\n",
    "        assert len(list(layer1_oof_test)) == len(self._label_cols)\n",
    "        assert set(list(layer1_oof_test)) - set(self._label_cols) == set()\n",
    "        for label in self._label_cols:\n",
    "            len(layer1_oof_train[label]) == len(layer1_oof_test[label]) == len(list(base_layer_est_preds))\n",
    "        assert type(base_layer_est_preds) == dict\n",
    "        assert type(model_data_id_list) == list\n",
    "        assert set(list(base_layer_est_preds)) - set(model_data_id_list) == set()\n",
    "        for model_data_id in model_data_id_list:\n",
    "            if model_data_id in set(self._model_data_id_list):\n",
    "                raise ValueError('{} is already in the repo'.format(model_data_id))\n",
    "        for model_data_id in model_data_id_list:\n",
    "            if model_data_id not in set(self._model_data_id_list):\n",
    "                self._model_data_id_list.append(model_data_id)\n",
    "                self._base_layer_est_scores[model_data_id] = 0\n",
    "        for (key, values) in base_layer_est_preds.items():\n",
    "            self._base_layer_est_preds[key] = values\n",
    "        for label in self._label_cols:\n",
    "            self._layer1_oof_train[label] += layer1_oof_train[label]\n",
    "            self._layer1_oof_test[label] += layer1_oof_test[label]\n",
    "    \n",
    "    def add_score(self, model_data_id, score):\n",
    "        assert score <= 1 and score >= 0\n",
    "        if model_data_id not in set(self._model_data_id_list):\n",
    "            raise ValueError('{} not in the repo. please add it first'.format(model_data_id))\n",
    "        if model_data_id in set(self._model_data_id_list):\n",
    "            print('{} already existed in the repo. score: {} update to {}'\\\n",
    "                  .format(model_data_id, self._base_layer_est_scores[model_data_id], score))\n",
    "        self._base_layer_est_scores[model_data_id] = score\n",
    "    \n",
    "    def show_scores(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of (name, score) tuple in sorted order by score\n",
    "        \"\"\"\n",
    "        sorted_list_from_dict = sorted(self._base_layer_est_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "        for key, value in sorted_list_from_dict:\n",
    "            print('{}\\t{}'.format(value, key))\n",
    "        return sorted_list_from_dict\n",
    "    \n",
    "    def get_results(self, threshold=None, chosen_ones=None):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            Note: threshold and chosen_ones can NOT both be not-None\n",
    "            threshold: if not None, then return only ones that score >= threshold\n",
    "            chosen_ones: list of model_data_id\n",
    "        Returns: \n",
    "            layer1_oof_train, layer1_oof_test, base_layer_est_preds\n",
    "        \"\"\"\n",
    "        if threshold != None and chosen_ones != None:\n",
    "            raise ValueError('threshold and chosen_ones can NOT both be not-None')\n",
    "        if threshold == None and chosen_ones == None:\n",
    "            return self._layer1_oof_train, self._layer1_oof_test, self._base_layer_est_preds\n",
    "        else:\n",
    "            layer1_oof_train_temp = copy.deepcopy(self._layer1_oof_train) # copy only keep the keys, not the value reference\n",
    "            layer1_oof_test_temp = copy.deepcopy(self._layer1_oof_test)   # deepcopy also keep the value reference\n",
    "            base_layer_est_preds_temp = self._base_layer_est_preds.copy()\n",
    "            base_layer_est_scores_temp = self._base_layer_est_scores.copy()\n",
    "            model_data_id_list_temp = self._model_data_id_list.copy()\n",
    "            if threshold != None:\n",
    "                assert threshold <= 1 and threshold >= 0\n",
    "                for (key, value) in base_layer_est_scores_temp.items():\n",
    "                    if value < threshold:\n",
    "                        self.remove(key)\n",
    "            else: # chosen_ones != None\n",
    "                assert type(chosen_ones) == list\n",
    "                for model_data_id in chosen_ones:\n",
    "                    self.remove(model_data_id)\n",
    "            \n",
    "            self._save_lock = False # not actually removed, so set it back to True\n",
    "\n",
    "            r1, r2, r3 = self._layer1_oof_train, self._layer1_oof_test, self._base_layer_est_preds\n",
    "\n",
    "            self._layer1_oof_train = layer1_oof_train_temp\n",
    "            self._layer1_oof_test = layer1_oof_test_temp\n",
    "            self._base_layer_est_preds = base_layer_est_preds_temp\n",
    "            self._base_layer_est_scores = base_layer_est_scores_temp\n",
    "            self._model_data_id_list = model_data_id_list_temp\n",
    "            return r1, r2, r3\n",
    "    \n",
    "    def remove(self, model_data_id):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        mdid_index = self._model_data_id_list.index(model_data_id)\n",
    "        self._model_data_id_list.pop(mdid_index)\n",
    "        self._base_layer_est_preds.pop(model_data_id)\n",
    "        self._base_layer_est_scores.pop(model_data_id)\n",
    "        for label in self._label_cols:\n",
    "            self._layer1_oof_train[label].pop(mdid_index)\n",
    "            self._layer1_oof_test[label].pop(mdid_index)\n",
    "        self._save_lock = True\n",
    "            \n",
    "    def unlock_save(self):\n",
    "        self._save_lock = False\n",
    "            \n",
    "    def save(self):\n",
    "        if self._save_lock:\n",
    "            print('save function is locked due to some results removed from the repo. \\\n",
    "            Call unlock_save() to unlock the save function and save again.')\n",
    "        else:\n",
    "            save_obj(self._model_data_id_list, '13models_model_data_id_list')\n",
    "            save_obj(self._layer1_oof_train, '13models_layer1_oof_train')\n",
    "            save_obj(self._layer1_oof_test, '13models_layer1_oof_test')\n",
    "            save_obj(self._base_layer_est_preds, '13models_base_layer_est_preds')\n",
    "            save_obj(self._base_layer_est_scores, '13models_base_layer_est_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tfidf_data import tfidf_data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, data_id = tfidf_data_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    #'learning_rate': 0.05,\n",
    "    'is_unbalance': True,\n",
    "    'early_stopping_round': 25,\n",
    "    'max_depth': -1,\n",
    "    'num_boost_round': 3000,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': 1,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 4\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 1\n",
    "}\n",
    "lgb_ble = LightgbmBLE(x_train, y_train, params=lgb_params, nb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_ble.train(x_train, y_train['toxic'].values, valid_set_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
