{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from enum import Enum\n",
    "class ModelName(Enum):\n",
    "    XGB = 1\n",
    "    NBXGB = 2\n",
    "    LGB = 3\n",
    "    NBLGB = 4\n",
    "    LOGREG = 5\n",
    "    NBSVM = 6\n",
    "    NBLSVC = 7\n",
    "    RF = 8 # random forest\n",
    "    RNN = 9\n",
    "    ONESVC = 10\n",
    "    ONELOGREG = 11\n",
    "\n",
    "\n",
    "class BaseLayerEstimator(ABC):\n",
    "    \n",
    "    def _pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def _nb(self, x_train, y_train):\n",
    "        assert isinstance(y_train, pd.DataFrame)\n",
    "        r = {}\n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            r[col] = np.log(self._pr(1, y_train[col].values, x_train) / self._pr(0, y_train[col], x_train))\n",
    "        return r\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np array\n",
    "            y_train: pd series\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, x_train):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "class OneVSOneRegBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, model='logistic'):\n",
    "        \"\"\"\n",
    "        x_train: sparse matrix, raw tfidf\n",
    "        y_train: dataframe, with only label columns. should be 6 columns in total\n",
    "        model: only support logistic or svc\n",
    "        \"\"\"\n",
    "        self.r = {}\n",
    "        self.setModelName(model)\n",
    "        assert self.model_name in ['logistic', 'svc']\n",
    "        self.param = {}\n",
    "        self.param['logistic'] = {'identity_hate': 9.0,\n",
    "                                     'insult': 1.5,\n",
    "                                     'obscene': 1.0,\n",
    "                                     'severe_toxic': 4.0,\n",
    "                                     'threat': 9.0,\n",
    "                                     'toxic': 2.7}\n",
    "        self.param['svc'] = {'identity_hate': 0.9,\n",
    "                             'insult': 0.15,\n",
    "                             'obscene': 0.15,\n",
    "                             'severe_toxic': 0.15,\n",
    "                             'threat': 1.0,\n",
    "                             'toxic': 0.29}\n",
    "        \n",
    "        \n",
    "        \n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            self.r[col] = np.log(self.pr(1, y_train[col].values, x_train) / self.pr(0, y_train[col], x_train))\n",
    "        print('initializing done')\n",
    "        print('OneVsOne is using {} kernel'.format(self.model_name))\n",
    "        \n",
    "    def setModelName(self, name):\n",
    "        self.model_name = name\n",
    "        assert self.model_name in ['logistic', 'svc']\n",
    "        print('OneVsOne is using {} kernel'.format(self.model_name))\n",
    "        \n",
    "    def pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def oneVsOneSplit(self, x_train, y_train, label):\n",
    "        print('Starting One vs One dataset splitting')\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.values\n",
    "        model_train = x_train[np.array(y_train == 1)]\n",
    "        y_model_train = y_train[np.array(y_train == 1)]\n",
    "        non_model_train = x_train[np.array(y_train == 0)]\n",
    "        non_model_train = non_model_train[:model_train.shape[0]]\n",
    "        y_non_model_train = y_train[np.array(y_train == 0)]\n",
    "        y_non_model_train = y_non_model_train[:model_train.shape[0]]\n",
    "        x_model_stack = vstack([model_train, non_model_train])\n",
    "        y_model_stack = np.concatenate([y_model_train, y_non_model_train])\n",
    "        x_nb = x_model_stack.multiply(self.r[label]).tocsr()\n",
    "        y_nb = y_model_stack\n",
    "        print('splitting done!')\n",
    "        return (x_nb, y_nb)\n",
    "    \n",
    "    def train(self, x_train, y_train, label):\n",
    "        ### construct one vs one\n",
    "        x_nb, y_nb = self.oneVsOneSplit(x_train, y_train, label)\n",
    "        ### start training\n",
    "        if self.model_name is 'logistic':\n",
    "            print('start training logistic regression')\n",
    "            self.model = LogisticRegression(C=self.param['logistic'][label])\n",
    "            self.model.fit(x_nb, y_nb)\n",
    "            print('training done')\n",
    "            \n",
    "        else:\n",
    "            print('start training linear svc regression')\n",
    "            lsvc = LinearSVC(C=self.param['svc'][label])\n",
    "            self.model = CalibratedClassifierCV(lsvc) \n",
    "            self.model.fit(x_nb, y_nb)\n",
    "            print('training done')\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self, x_test, label):\n",
    "        print('applying naive bayes to dataset')\n",
    "        x_nb_test = x_test.multiply(self.r[label]).tocsr()\n",
    "        print('predicting')\n",
    "        pred = self.model.predict_proba(x_nb_test)[:,1]\n",
    "        print('predicting done')\n",
    "        return pred\n",
    "    \n",
    "##### example        \n",
    "# aa = OneVSOneReg(train_tfidf, train[label_cols], model='logistic')\n",
    "# aa.setModelName('svc')\n",
    "# aa.train(train_tfidf,train['toxic'], 'toxic')\n",
    "# aa.predict(test_tfidf, 'toxic')\n",
    "\n",
    "\n",
    "\n",
    "class SklearnBLE(BaseLayerEstimator):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "class NbSvmBLE(BaseLayerEstimator, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, mode=ModelName.NBSVM, seed=0, params=None):\n",
    "        self._mode = mode\n",
    "        params['random_state'] = seed\n",
    "        self._params = params\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        #return self._clf.predict(x.multiply(self._r))\n",
    "        return self._clf.predict_proba(x.multiply(self._r))[:,1] # chance of being 1 ([:,0] chance of being 0)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        #self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        self._clf = LogisticRegression(**self._params).fit(x_nb, y)\n",
    "        if self._mode == ModelName.NBLSVC:\n",
    "            self._clf = CalibratedClassifierCV(LinearSVC(**self._params)).fit(x_nb, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.fit(x_train, y_train)\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        return self._clf.feature_importance\n",
    "\n",
    "    \n",
    "class XgbBLE(BaseLayerEstimator):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict_proba(xgb.DMatrix(x))[:,1]\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "class LightgbmBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, label_cols= None, params=None, nb=True, seed=0):\n",
    "        \"\"\"\n",
    "        constructor:\n",
    "\n",
    "            x_train: should be a np/scipy/ 2-d array or matrix. only be used when nb is true\n",
    "            y_train: should be a dataframe\n",
    "            label_cols: (list) if y_train contains multiple labels, provide the list of label names\n",
    "                e.g.: label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "            params: (dict)\n",
    "            nb: (boolean) compute naive bayes or not. (helpful for unbalanced data)\n",
    "            seed: (int) training random seed (not used currently)\n",
    "            \n",
    "        Example:\n",
    "            ll = LightgbmBLE(train_tfidf, train[label_cols], params=params, nb=True)\n",
    "            result = pd.DataFrame()\n",
    "            for col in label_cols:\n",
    "                    print(col)\n",
    "                    ll.train(train_tfidf, train[col], col)\n",
    "                    result[col] = ll.predict(test_tfidf, col)\n",
    "        \"\"\"\n",
    "        #### check naive bayes\n",
    "        if nb:\n",
    "            print('Naive Bayes is enabled')\n",
    "            self.r = self._nb(x_train, y_train)\n",
    "        else:\n",
    "            print('Naive Bayes is disabled')\n",
    "            self.r = None\n",
    "        ##### set values    \n",
    "        self.nb = nb\n",
    "        self.set_params(params)\n",
    "        self.label_cols = label_cols\n",
    "        print('LightgbmBLE is initialized')\n",
    "    \n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _pre_process(self, x_train, y_train, label=None):\n",
    "        if self.nb:\n",
    "            if label is None:\n",
    "                raise ValueError('Naive Bayes is enabled. label cannot be None.')\n",
    "            if label not in self.label_cols:\n",
    "                raise ValueError('Label not in label_cols')\n",
    "            print('apply naive bayes to feature set')\n",
    "            x = x_train.multiply(self.r[label])\n",
    "            if isinstance(x_train, csr_matrix):\n",
    "                x = x.tocsr()\n",
    "        else:\n",
    "            x = x_train\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y = y_train.values\n",
    "        else:\n",
    "            y = y_train\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train, label=None, valid_set_percent=0):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np/scipy/ 2-d array or matrix\n",
    "            y_train: should be a dataframe\n",
    "            label: (str) if not none, then it's one of the labels in the label_cols\n",
    "                    if nb is set to True when initializing, when label can not be None\n",
    "            valid_set_percent: (float, 0 to 1). \n",
    "                    0: no validation set. (imposible to use early stopping)\n",
    "                    1: use training set as validation set (to check underfitting, and early stopping)\n",
    "                    >0 and <1: use a portion of training set as validation set. (to check overfitting, and early stopping)\n",
    "        \n",
    "        \"\"\"\n",
    "        x, y = self._pre_process(x_train, y_train, label)\n",
    "        \n",
    "        if valid_set_percent != 0:\n",
    "            if valid_set_percent > 1 or valid_set_percent < 0:\n",
    "                raise ValueError('valid_set_percent must >= 0 and <= 1')\n",
    "            if valid_set_percent != 1:\n",
    "                x, x_val, y, y_val = train_test_split(x, y, test_size=valid_set_percent)\n",
    "\n",
    "\n",
    "        lgb_train = lgb.Dataset(x, y)\n",
    "        if valid_set_percent != 0:\n",
    "            if valid_set_percent == 1:\n",
    "                print('Evaluating using training set')\n",
    "                self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_train)\n",
    "            else:\n",
    "                lgb_val = lgb.Dataset(x_val, y_val)\n",
    "                print('Evaluating using validation set ({}% of training set)'.format(valid_set_percent*100))\n",
    "                self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_val)\n",
    "        else:\n",
    "            print('No evaluation set, thus not possible to use early stopping. Please train with your best params.')\n",
    "            self.model = lgb.train(self.params, lgb_train)\n",
    "        \n",
    "        \n",
    "    def predict(self, x_train, label=None):\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        x, _ = self._pre_process(x_train, y_train=None, label=label)\n",
    "        print('starting predicting')\n",
    "        if self.model.best_iteration > 0:\n",
    "            print('best_iteration {} is chosen.'.format(best_iteration))\n",
    "            result = self.model.predict(x, num_iteration=bst.best_iteration)\n",
    "        else:\n",
    "            result = self.model.predict(x)\n",
    "        print('predicting done')\n",
    "        return result\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tfidf_data import tfidf_data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data done!\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, data_id = tfidf_data_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    #'learning_rate': 0.05,\n",
    "    'is_unbalance': True,\n",
    "    'early_stopping_round': 25,\n",
    "    'max_depth': -1,\n",
    "    'num_boost_round': 3000,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 63,\n",
    "    'verbosity': 10,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 1,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 4\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 1\n",
    "}\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "lgb_ble = LightgbmBLE(x_train, y_train, label_cols=label_cols, params=lgb_params, nb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "lgb_ble.train(x_train, y_train['threat'].values, valid_set_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-17-a1ac4d0d46cd>(318)predict()\n",
      "-> x, _ = self._pre_process(x_train, y_train=None, label=label)\n",
      "(Pdb) self.model.best_iteration\n",
      "0\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2c91015ff970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_ble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-a1ac4d0d46cd>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_train, label)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting predicting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-a1ac4d0d46cd>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_train, label)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting predicting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgb_ble.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:99: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/lightgbm/engine.py:104: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.870293\n",
      "Training until validation scores don't improve for 25 rounds.\n",
      "[2]\tvalid_0's auc: 0.879855\n",
      "[3]\tvalid_0's auc: 0.890069\n",
      "[4]\tvalid_0's auc: 0.895269\n",
      "[5]\tvalid_0's auc: 0.900742\n",
      "[6]\tvalid_0's auc: 0.904055\n",
      "[7]\tvalid_0's auc: 0.90557\n",
      "[8]\tvalid_0's auc: 0.908777\n",
      "[9]\tvalid_0's auc: 0.911911\n",
      "[10]\tvalid_0's auc: 0.912996\n",
      "[11]\tvalid_0's auc: 0.916038\n",
      "[12]\tvalid_0's auc: 0.918541\n",
      "[13]\tvalid_0's auc: 0.921163\n",
      "[14]\tvalid_0's auc: 0.922745\n",
      "[15]\tvalid_0's auc: 0.925085\n",
      "[16]\tvalid_0's auc: 0.926786\n",
      "[17]\tvalid_0's auc: 0.927603\n",
      "[18]\tvalid_0's auc: 0.931456\n",
      "[19]\tvalid_0's auc: 0.932031\n",
      "[20]\tvalid_0's auc: 0.934025\n",
      "[21]\tvalid_0's auc: 0.93553\n",
      "[22]\tvalid_0's auc: 0.936878\n",
      "[23]\tvalid_0's auc: 0.938081\n",
      "[24]\tvalid_0's auc: 0.93911\n",
      "[25]\tvalid_0's auc: 0.940473\n",
      "[26]\tvalid_0's auc: 0.941336\n",
      "[27]\tvalid_0's auc: 0.942148\n",
      "[28]\tvalid_0's auc: 0.942864\n",
      "[29]\tvalid_0's auc: 0.943854\n",
      "[30]\tvalid_0's auc: 0.944219\n",
      "[31]\tvalid_0's auc: 0.945139\n",
      "[32]\tvalid_0's auc: 0.947068\n",
      "[33]\tvalid_0's auc: 0.947943\n",
      "[34]\tvalid_0's auc: 0.949299\n",
      "[35]\tvalid_0's auc: 0.949881\n",
      "[36]\tvalid_0's auc: 0.950709\n",
      "[37]\tvalid_0's auc: 0.951376\n",
      "[38]\tvalid_0's auc: 0.952251\n",
      "[39]\tvalid_0's auc: 0.95297\n",
      "[40]\tvalid_0's auc: 0.953703\n",
      "[41]\tvalid_0's auc: 0.954556\n",
      "[42]\tvalid_0's auc: 0.955345\n",
      "[43]\tvalid_0's auc: 0.955883\n",
      "[44]\tvalid_0's auc: 0.956224\n",
      "[45]\tvalid_0's auc: 0.956448\n",
      "[46]\tvalid_0's auc: 0.957143\n",
      "[47]\tvalid_0's auc: 0.958222\n",
      "[48]\tvalid_0's auc: 0.958521\n",
      "[49]\tvalid_0's auc: 0.958812\n",
      "[50]\tvalid_0's auc: 0.959535\n",
      "[51]\tvalid_0's auc: 0.960217\n",
      "[52]\tvalid_0's auc: 0.960455\n",
      "[53]\tvalid_0's auc: 0.960892\n",
      "[54]\tvalid_0's auc: 0.961406\n",
      "[55]\tvalid_0's auc: 0.962039\n",
      "[56]\tvalid_0's auc: 0.962328\n",
      "[57]\tvalid_0's auc: 0.962745\n",
      "[58]\tvalid_0's auc: 0.963123\n",
      "[59]\tvalid_0's auc: 0.963432\n",
      "[60]\tvalid_0's auc: 0.963828\n",
      "[61]\tvalid_0's auc: 0.964368\n",
      "[62]\tvalid_0's auc: 0.96478\n",
      "[63]\tvalid_0's auc: 0.964981\n",
      "[64]\tvalid_0's auc: 0.965402\n",
      "[65]\tvalid_0's auc: 0.965689\n",
      "[66]\tvalid_0's auc: 0.965989\n",
      "[67]\tvalid_0's auc: 0.966119\n",
      "[68]\tvalid_0's auc: 0.966557\n",
      "[69]\tvalid_0's auc: 0.966636\n",
      "[70]\tvalid_0's auc: 0.966908\n",
      "[71]\tvalid_0's auc: 0.967093\n",
      "[72]\tvalid_0's auc: 0.967389\n",
      "[73]\tvalid_0's auc: 0.967575\n",
      "[74]\tvalid_0's auc: 0.967705\n",
      "[75]\tvalid_0's auc: 0.967973\n",
      "[76]\tvalid_0's auc: 0.968204\n",
      "[77]\tvalid_0's auc: 0.968479\n",
      "[78]\tvalid_0's auc: 0.968569\n",
      "[79]\tvalid_0's auc: 0.968892\n",
      "[80]\tvalid_0's auc: 0.96902\n",
      "[81]\tvalid_0's auc: 0.96913\n",
      "[82]\tvalid_0's auc: 0.969359\n",
      "[83]\tvalid_0's auc: 0.969591\n",
      "[84]\tvalid_0's auc: 0.969768\n",
      "[85]\tvalid_0's auc: 0.969915\n",
      "[86]\tvalid_0's auc: 0.970017\n",
      "[87]\tvalid_0's auc: 0.970074\n",
      "[88]\tvalid_0's auc: 0.97031\n",
      "[89]\tvalid_0's auc: 0.970477\n",
      "[90]\tvalid_0's auc: 0.970589\n",
      "[91]\tvalid_0's auc: 0.970694\n",
      "[92]\tvalid_0's auc: 0.970754\n",
      "[93]\tvalid_0's auc: 0.97082\n",
      "[94]\tvalid_0's auc: 0.970928\n",
      "[95]\tvalid_0's auc: 0.971038\n",
      "[96]\tvalid_0's auc: 0.971227\n",
      "[97]\tvalid_0's auc: 0.971362\n",
      "[98]\tvalid_0's auc: 0.971456\n",
      "[99]\tvalid_0's auc: 0.971534\n",
      "[100]\tvalid_0's auc: 0.971644\n",
      "[101]\tvalid_0's auc: 0.971797\n",
      "[102]\tvalid_0's auc: 0.971859\n",
      "[103]\tvalid_0's auc: 0.971918\n",
      "[104]\tvalid_0's auc: 0.972091\n",
      "[105]\tvalid_0's auc: 0.972186\n",
      "[106]\tvalid_0's auc: 0.972321\n",
      "[107]\tvalid_0's auc: 0.972409\n",
      "[108]\tvalid_0's auc: 0.972481\n",
      "[109]\tvalid_0's auc: 0.972599\n",
      "[110]\tvalid_0's auc: 0.97274\n",
      "[111]\tvalid_0's auc: 0.972768\n",
      "[112]\tvalid_0's auc: 0.972911\n",
      "[113]\tvalid_0's auc: 0.972975\n",
      "[114]\tvalid_0's auc: 0.973063\n",
      "[115]\tvalid_0's auc: 0.973152\n",
      "[116]\tvalid_0's auc: 0.97316\n",
      "[117]\tvalid_0's auc: 0.973243\n",
      "[118]\tvalid_0's auc: 0.973368\n",
      "[119]\tvalid_0's auc: 0.973423\n",
      "[120]\tvalid_0's auc: 0.973436\n",
      "[121]\tvalid_0's auc: 0.973539\n",
      "[122]\tvalid_0's auc: 0.973576\n",
      "[123]\tvalid_0's auc: 0.973675\n",
      "[124]\tvalid_0's auc: 0.973742\n",
      "[125]\tvalid_0's auc: 0.973817\n",
      "[126]\tvalid_0's auc: 0.973855\n",
      "[127]\tvalid_0's auc: 0.973954\n",
      "[128]\tvalid_0's auc: 0.974055\n",
      "[129]\tvalid_0's auc: 0.974147\n",
      "[130]\tvalid_0's auc: 0.97418\n",
      "[131]\tvalid_0's auc: 0.974212\n",
      "[132]\tvalid_0's auc: 0.974308\n",
      "[133]\tvalid_0's auc: 0.974376\n",
      "[134]\tvalid_0's auc: 0.974461\n",
      "[135]\tvalid_0's auc: 0.974498\n",
      "[136]\tvalid_0's auc: 0.974544\n",
      "[137]\tvalid_0's auc: 0.974542\n",
      "[138]\tvalid_0's auc: 0.974635\n",
      "[139]\tvalid_0's auc: 0.974729\n",
      "[140]\tvalid_0's auc: 0.974717\n",
      "[141]\tvalid_0's auc: 0.974778\n",
      "[142]\tvalid_0's auc: 0.974838\n",
      "[143]\tvalid_0's auc: 0.974821\n",
      "[144]\tvalid_0's auc: 0.974833\n",
      "[145]\tvalid_0's auc: 0.974874\n",
      "[146]\tvalid_0's auc: 0.974887\n",
      "[147]\tvalid_0's auc: 0.974962\n",
      "[148]\tvalid_0's auc: 0.974988\n",
      "[149]\tvalid_0's auc: 0.975054\n",
      "[150]\tvalid_0's auc: 0.975057\n",
      "[151]\tvalid_0's auc: 0.975075\n",
      "[152]\tvalid_0's auc: 0.975103\n",
      "[153]\tvalid_0's auc: 0.975213\n",
      "[154]\tvalid_0's auc: 0.975266\n",
      "[155]\tvalid_0's auc: 0.975266\n",
      "[156]\tvalid_0's auc: 0.975304\n",
      "[157]\tvalid_0's auc: 0.975316\n",
      "[158]\tvalid_0's auc: 0.975307\n",
      "[159]\tvalid_0's auc: 0.975345\n",
      "[160]\tvalid_0's auc: 0.975341\n",
      "[161]\tvalid_0's auc: 0.975434\n",
      "[162]\tvalid_0's auc: 0.975482\n",
      "[163]\tvalid_0's auc: 0.97551\n",
      "[164]\tvalid_0's auc: 0.975513\n",
      "[165]\tvalid_0's auc: 0.975519\n",
      "[166]\tvalid_0's auc: 0.975512\n",
      "[167]\tvalid_0's auc: 0.975524\n",
      "[168]\tvalid_0's auc: 0.975542\n",
      "[169]\tvalid_0's auc: 0.975534\n",
      "[170]\tvalid_0's auc: 0.975547\n",
      "[171]\tvalid_0's auc: 0.975572\n",
      "[172]\tvalid_0's auc: 0.975598\n",
      "[173]\tvalid_0's auc: 0.975685\n",
      "[174]\tvalid_0's auc: 0.975718\n",
      "[175]\tvalid_0's auc: 0.975747\n",
      "[176]\tvalid_0's auc: 0.975788\n",
      "[177]\tvalid_0's auc: 0.97578\n",
      "[178]\tvalid_0's auc: 0.975809\n",
      "[179]\tvalid_0's auc: 0.975808\n",
      "[180]\tvalid_0's auc: 0.975838\n",
      "[181]\tvalid_0's auc: 0.975864\n",
      "[182]\tvalid_0's auc: 0.975923\n",
      "[183]\tvalid_0's auc: 0.975908\n",
      "[184]\tvalid_0's auc: 0.975923\n",
      "[185]\tvalid_0's auc: 0.975912\n",
      "[186]\tvalid_0's auc: 0.975957\n",
      "[187]\tvalid_0's auc: 0.97603\n",
      "[188]\tvalid_0's auc: 0.976035\n",
      "[189]\tvalid_0's auc: 0.976062\n",
      "[190]\tvalid_0's auc: 0.976085\n",
      "[191]\tvalid_0's auc: 0.976082\n",
      "[192]\tvalid_0's auc: 0.976083\n",
      "[193]\tvalid_0's auc: 0.976024\n",
      "[194]\tvalid_0's auc: 0.976008\n",
      "[195]\tvalid_0's auc: 0.975979\n",
      "[196]\tvalid_0's auc: 0.975996\n",
      "[197]\tvalid_0's auc: 0.976025\n",
      "[198]\tvalid_0's auc: 0.976044\n",
      "[199]\tvalid_0's auc: 0.976088\n",
      "[200]\tvalid_0's auc: 0.976072\n",
      "[201]\tvalid_0's auc: 0.9761\n",
      "[202]\tvalid_0's auc: 0.976126\n",
      "[203]\tvalid_0's auc: 0.976132\n",
      "[204]\tvalid_0's auc: 0.976143\n",
      "[205]\tvalid_0's auc: 0.976157\n",
      "[206]\tvalid_0's auc: 0.976199\n",
      "[207]\tvalid_0's auc: 0.976153\n",
      "[208]\tvalid_0's auc: 0.976183\n",
      "[209]\tvalid_0's auc: 0.976152\n",
      "[210]\tvalid_0's auc: 0.976147\n",
      "[211]\tvalid_0's auc: 0.976184\n",
      "[212]\tvalid_0's auc: 0.976169\n",
      "[213]\tvalid_0's auc: 0.976198\n",
      "[214]\tvalid_0's auc: 0.976201\n",
      "[215]\tvalid_0's auc: 0.976194\n",
      "[216]\tvalid_0's auc: 0.976202\n",
      "[217]\tvalid_0's auc: 0.976227\n",
      "[218]\tvalid_0's auc: 0.976234\n",
      "[219]\tvalid_0's auc: 0.976255\n",
      "[220]\tvalid_0's auc: 0.976259\n",
      "[221]\tvalid_0's auc: 0.976201\n",
      "[222]\tvalid_0's auc: 0.976186\n",
      "[223]\tvalid_0's auc: 0.976205\n",
      "[224]\tvalid_0's auc: 0.976229\n",
      "[225]\tvalid_0's auc: 0.976289\n",
      "[226]\tvalid_0's auc: 0.976261\n",
      "[227]\tvalid_0's auc: 0.976282\n",
      "[228]\tvalid_0's auc: 0.976289\n",
      "[229]\tvalid_0's auc: 0.976304\n",
      "[230]\tvalid_0's auc: 0.976246\n",
      "[231]\tvalid_0's auc: 0.976289\n",
      "[232]\tvalid_0's auc: 0.976298\n",
      "[233]\tvalid_0's auc: 0.976307\n",
      "[234]\tvalid_0's auc: 0.976324\n",
      "[235]\tvalid_0's auc: 0.976386\n",
      "[236]\tvalid_0's auc: 0.976407\n",
      "[237]\tvalid_0's auc: 0.976385\n",
      "[238]\tvalid_0's auc: 0.9764\n",
      "[239]\tvalid_0's auc: 0.976443\n",
      "[240]\tvalid_0's auc: 0.976485\n",
      "[241]\tvalid_0's auc: 0.976488\n",
      "[242]\tvalid_0's auc: 0.976526\n",
      "[243]\tvalid_0's auc: 0.976559\n",
      "[244]\tvalid_0's auc: 0.976561\n",
      "[245]\tvalid_0's auc: 0.97657\n",
      "[246]\tvalid_0's auc: 0.976564\n",
      "[247]\tvalid_0's auc: 0.976549\n",
      "[248]\tvalid_0's auc: 0.976549\n",
      "[249]\tvalid_0's auc: 0.976518\n",
      "[250]\tvalid_0's auc: 0.97653\n",
      "[251]\tvalid_0's auc: 0.976552\n",
      "[252]\tvalid_0's auc: 0.976576\n",
      "[253]\tvalid_0's auc: 0.976597\n",
      "[254]\tvalid_0's auc: 0.976629\n",
      "[255]\tvalid_0's auc: 0.976599\n",
      "[256]\tvalid_0's auc: 0.976603\n",
      "[257]\tvalid_0's auc: 0.976635\n",
      "[258]\tvalid_0's auc: 0.976631\n",
      "[259]\tvalid_0's auc: 0.976583\n",
      "[260]\tvalid_0's auc: 0.976524\n",
      "[261]\tvalid_0's auc: 0.976517\n",
      "[262]\tvalid_0's auc: 0.976463\n",
      "[263]\tvalid_0's auc: 0.976486\n",
      "[264]\tvalid_0's auc: 0.97645\n",
      "[265]\tvalid_0's auc: 0.976468\n",
      "[266]\tvalid_0's auc: 0.976448\n",
      "[267]\tvalid_0's auc: 0.976409\n",
      "[268]\tvalid_0's auc: 0.976411\n",
      "[269]\tvalid_0's auc: 0.976412\n",
      "[270]\tvalid_0's auc: 0.976425\n",
      "[271]\tvalid_0's auc: 0.976448\n",
      "[272]\tvalid_0's auc: 0.976447\n",
      "[273]\tvalid_0's auc: 0.976466\n",
      "[274]\tvalid_0's auc: 0.976519\n",
      "[275]\tvalid_0's auc: 0.976517\n",
      "[276]\tvalid_0's auc: 0.976484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[277]\tvalid_0's auc: 0.976481\n",
      "[278]\tvalid_0's auc: 0.976518\n",
      "[279]\tvalid_0's auc: 0.976525\n",
      "[280]\tvalid_0's auc: 0.976551\n",
      "[281]\tvalid_0's auc: 0.976569\n",
      "[282]\tvalid_0's auc: 0.976599\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid_0's auc: 0.976635\n"
     ]
    }
   ],
   "source": [
    "lgb_ble.train(x_train, y_train['toxic'].values, valid_set_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 300000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "preds = lgb_ble.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('lgbtoxicpred', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_load = np.load('lgbtoxicpred.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_load.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "            dict(name=\"max_bin\", type=\"int\", bounds=dict(min=20, max=20000)),\n",
    "            dict(name=\"learning_rate\", type=\"double\", bounds=dict(min=0.001, max=0.3)),\n",
    "            dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=4095)),\n",
    "            # dict(name=\"num_leaves\", type=\"int\", bounds=dict(min=100, max=45000)),\n",
    "            dict(name=\"scale_pos_weight\", type=\"double\", bounds=dict(min=0.01, max=2000.0)),\n",
    "            dict(name=\"n_estimators\", type=\"int\", bounds=dict(min=10, max=10000)),\n",
    "            dict(name=\"min_child_weight\", type=\"int\", bounds=dict(min=1, max=2000)),\n",
    "            dict(name=\"subsample\", type=\"double\", bounds=dict(min=0.4, max=1)),\n",
    "            dict(name=\"bagging_fraction\", type=\"double\", bounds=dict(min=0.3, max=1)),\n",
    "            dict(name=\"max_depth\", type=\"int\", bounds=dict(min=2, max=50)),\n",
    "        ]\n",
    "# static_parameters = {'boosting_type': 'dart', 'reg_alpha': 0, 'reg_lambda': 2, 'is_unbalance': True,\n",
    "#                              'min_split_gain': 0, 'min_child_samples': 10, 'colsample_bytree': 0.8, 'subsample_freq': 3,\n",
    "#                              'subsample_for_bin': 50000,\n",
    "#                              'histogram_pool_size': detect_available_memory_for_histogram_cache()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bounds': {'max': 20000, 'min': 20}, 'name': 'max_bin', 'type': 'int'},\n",
       " {'bounds': {'max': 0.3, 'min': 0.001},\n",
       "  'name': 'learning_rate',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 4095, 'min': 100}, 'name': 'num_leaves', 'type': 'int'},\n",
       " {'bounds': {'max': 2000.0, 'min': 0.01},\n",
       "  'name': 'scale_pos_weight',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 10000, 'min': 10}, 'name': 'n_estimators', 'type': 'int'},\n",
       " {'bounds': {'max': 2000, 'min': 1},\n",
       "  'name': 'min_child_weight',\n",
       "  'type': 'int'},\n",
       " {'bounds': {'max': 1, 'min': 0.4}, 'name': 'subsample', 'type': 'double'},\n",
       " {'bounds': {'max': 1, 'min': 0.3},\n",
       "  'name': 'bagging_fraction',\n",
       "  'type': 'double'},\n",
       " {'bounds': {'max': 50, 'min': 2}, 'name': 'max_depth', 'type': 'int'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
