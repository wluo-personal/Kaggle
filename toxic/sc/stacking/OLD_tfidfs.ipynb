{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bldr = BaseLayerDataRepo()\n",
    "\n",
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,1),(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        #min_df = i\n",
    "        #word_ngram_range = (1,1)\n",
    "        #char_ngram_range = (1,5)\n",
    "        word_max_features = 200000\n",
    "        #char_max_features = 100000\n",
    "        token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "        data_id = 'tfidf_word_df%d_ng%s_wmf%s'%(min_df,str(word_ngram_range),str(word_max_features))\n",
    "\n",
    "        word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                  min_df=1,\n",
    "                                  ngram_range=word_ngram_range,\n",
    "                                  max_features=word_max_features,\n",
    "                                  token_pattern=token_pattern,\n",
    "                                  stop_words='english',\n",
    "                                  strip_accents='unicode',\n",
    "                                  sublinear_tf=True)\n",
    "        \n",
    "        train_term_doc = word_vec.fit_transform(train.comment_text)\n",
    "        test_term_doc = word_vec.transform(test.comment_text)\n",
    "        #pdb.set_trace()\n",
    "        #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_doc)\n",
    "        #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_doc)\n",
    "    \n",
    "        compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "        bldr.add_data(data_id, train_term_doc, test_term_doc, train[label_cols], label_cols, compatible_models)\n",
    "\n",
    "for min_df in [2]:\n",
    "    for word_ngram_range in [(1,2)]:#,(4,4),(5,6)]:#,(1,3),(4,4),(5,6)]:\n",
    "        for char_max_df in [0.3]:\n",
    "            #min_df = i\n",
    "            #word_ngram_range = (1,1)\n",
    "            #char_ngram_range = (1,5)\n",
    "            word_max_features = 100000\n",
    "            char_max_features = 100000\n",
    "            token_pattern = r'\\w{%d,}'%3\n",
    "\n",
    "            data_id = 'tfidf_wordchar_charmaxdf%f_ng%s_wmf%s_cmf%s'%(char_max_df,str(word_ngram_range),str(word_max_features),str(char_max_features))\n",
    "\n",
    "            word_vec = TfidfVectorizer(analyzer='word',\n",
    "                                      min_df=1,\n",
    "                                      ngram_range=word_ngram_range,\n",
    "                                      max_features=word_max_features,\n",
    "                                      token_pattern=token_pattern,\n",
    "                                      stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "\n",
    "            char_vec = TfidfVectorizer(analyzer='char',\n",
    "                                      min_df = 1,\n",
    "                                      max_df = char_max_df,\n",
    "                                      ngram_range=(2,7), \n",
    "                                      max_features=char_max_features, \n",
    "                                      #stop_words='english',\n",
    "                                      strip_accents='unicode',\n",
    "                                      sublinear_tf=True)\n",
    "\n",
    "            train_word_doc = word_vec.fit_transform(train.comment_text)\n",
    "            test_word_doc = word_vec.transform(test.comment_text)\n",
    "\n",
    "            train_char_doc = char_vec.fit_transform(train.comment_text)\n",
    "            test_char_doc = char_vec.transform(test.comment_text)\n",
    "\n",
    "            train_term_tfidf = hstack((train_word_doc, train_char_doc), format='csr')\n",
    "            test_term_tfidf = hstack((test_word_doc, test_char_doc), format='csr')\n",
    "\n",
    "            #np.save(DATA_PATH + data_id+'_x_train.npy', train_term_tfidf)\n",
    "            #np.save(DATA_PATH + data_id+'_x_test.npy', test_term_tfidf)\n",
    "\n",
    "            compatible_models = [ModelName.LGB, ModelName.LOGREG, ModelName.NBSVM, ModelName.NBLSVC, ModelName.RF, ModelName.XGB]\n",
    "            bldr.add_data(data_id, train_term_tfidf, test_term_tfidf, train[label_cols], label_cols, compatible_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
