{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_layer_utils import BaseLayerDataRepo, BaseLayerResultsRepo, ModelName\n",
    "from base_layer_utils import LightgbmBLE, SklearnBLE, XGBoostBLE\n",
    "\n",
    "#from fast_text_data import fasttext_data_process\n",
    "#from tfidf_data import tfidf_data_process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.cross_validation import KFold # replace with model_selection?\n",
    "#from sklearn.model_selection import KFold\n",
    "import time, re, gc\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n"
     ]
    }
   ],
   "source": [
    "PATH = '~/data/toxic/data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select layer2 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09854\tModelName.LOGREG_layer2\n",
      "0.09854\tModelName.XGB_layer2\n",
      "0.09853\tModelName.XGB_layer2_usingthebadhalf\n",
      "0.09827\tModelName.LGB_layer2\n",
      "0.09825\tModelName.RNN_rnn_data_001\n",
      "0\tModelName.LOGREG_wordtfidf_word_(1, 1)_100000_1_1.0_char_(0, 0)_100000_1_1.0\n",
      "0\tModelName.XGB_layer2_UsingAll14\n",
      "0\tModelName.RNN_rnn_data_fasttext_200_300\n",
      "0\tModelName.NBLGB_wordtfidf_word_(1, 1)_100000_1_1.0_char_(0, 0)_100000_1_1.0\n"
     ]
    }
   ],
   "source": [
    "_ = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.remove('ModelName.LGB_layer2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = ['ModelName.XGB_layer2', 'ModelName.LOGREG_layer2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer2_oof_train_loaded, layer2_oof_test_loaded, layer2_est_preds_loaded = base_layer_results_repo.get_results(chosen_ones=selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(layer2_oof_train_loaded['toxic']) == len(layer2_est_preds_loaded) == len(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before we choose which models to assemble, we can do:\n",
    "#### 1. scatter plot analysis to check the diversity\n",
    "#### 2. submit to check if the models have similar performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_layer_oof_per_label(layer1_oof_dict, label):\n",
    "    x = None\n",
    "    data_list = layer1_oof_dict[label]\n",
    "    for i in range(len(data_list)):\n",
    "        if i == 0:\n",
    "            x = data_list[0]\n",
    "        else:\n",
    "            x = np.concatenate((x, data_list[i]), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. simple blend of two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0 0.9910814282997946\n",
      "toxic 1 0.990771077509489\n",
      "toxic 0.9910856078589341 0.9550000000000001\n",
      "severe_toxic 0 0.9925828223897508\n",
      "severe_toxic 1 0.9919322096940085\n",
      "severe_toxic 0.9928764823290488 0.128\n",
      "obscene 0 0.9962181243081879\n",
      "obscene 1 0.9959665500533149\n",
      "obscene 0.9962206810311406 0.984\n",
      "threat 0 0.9946855970430916\n",
      "threat 1 0.99310288682231\n",
      "threat 0.9948137226753532 0.294\n",
      "insult 0 0.991952202527335\n",
      "insult 1 0.991528941423655\n",
      "insult 0.9919546286846996 0.936\n",
      "identity_hate 0 0.9933174155555204\n",
      "identity_hate 1 0.9925218214135401\n",
      "identity_hate 0.9934864460389672 0.203\n"
     ]
    }
   ],
   "source": [
    "result = np.empty((test.shape[0],len(label_cols)))\n",
    "\n",
    "# mix the first two models\n",
    "for i, label in enumerate(label_cols):\n",
    "    x_train = combine_layer_oof_per_label(layer2_oof_train_loaded, label)\n",
    "    x_test = combine_layer_oof_per_label(layer2_oof_test_loaded, label)\n",
    "    for j in range(x_train.shape[1]):\n",
    "        roc = roc_auc_score(train[label], x_train[:,j])\n",
    "        print(label, j, roc) # print out roc for meta feature on meta label (which is just the original train label)\n",
    "    \n",
    "    roc_scores_of_a_label = []\n",
    "    alphas = np.linspace(0,1,1001)\n",
    "    best_roc = 0\n",
    "    best_alpha = 0\n",
    "    for alpha in alphas:\n",
    "        roc = roc_auc_score(train[label], alpha*x_train[:,0] + (1-alpha)*x_train[:,1])\n",
    "        if roc > best_roc:\n",
    "            best_roc = roc\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    print(label, best_roc, best_alpha)\n",
    "    result[:,i] = best_alpha*x_test[:,0] + (1-best_alpha)*x_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520889439\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(PATH + 'sample_submission.csv')#.head(1000)\n",
    "submission[label_cols] = result\n",
    "sub_id = int(time.time())\n",
    "print(sub_id)\n",
    "submission.to_csv('./StackPreds/layer3_mix_9854xgb_9853logreg_' + str(sub_id) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from file\n"
     ]
    }
   ],
   "source": [
    "base_layer_results_repo = BaseLayerResultsRepo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add_score('ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.9819)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded = base_layer_results_repo.get_results(threshold=0.9793)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ths = [score for _, score in scores][3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9815, 0.9803, 0.9794, 0.9793, 0.9786, 0.9774, 0.9768]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real already existed in the repo. score: 0 update to 0.9818\n"
     ]
    }
   ],
   "source": [
    "#base_layer_results_repo.add_score('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',0.9818)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = open('./xgb_search.csv', 'a')\n",
    "# header = 'time,id,th,amt,csbt,lr,md,ss,ga,a,rounds,folds,tops,sps,ops,\\\n",
    "# thps,inps,ihps,tobr,sbr,obr,thbr,inbr,ihbr,sth1,sth2,sth3,sth4,sth5,\\\n",
    "# to_auc,s_auc,o_auc,th_auc,in_auc,ih_auc,avg_auc\\n'\n",
    "header = 'time,id,threshold,num_models,colsample_bytree,lr,max_depth,subsample,\\\n",
    "gamma,alpha,cv_num_round,cv_nfolds,toxic_pos_scale,severe_toxic_pos_scale,obscene_pos_scale,\\\n",
    "threat_pos_scale,insult_pos_scale,identity_hate_pos_scale,sth1,sth2,sth3,sth4,sth5,\\\n",
    "toxic_best_round,severe_toxic_best_round,obscene_best_round,threat_best_round,\\\n",
    "insult_best_round,identity_hate_best_round,toxic_auc,severe_toxic_auc,obscene_auc,\\\n",
    "threat_auc,insult_auc,identity_hate_auc,avg_auc\\n'\n",
    "f.write(header)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    from datetime import datetime\n",
    "    from dateutil import tz\n",
    "\n",
    "    # METHOD 1: Hardcode zones:\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "\n",
    "    utc = datetime.utcnow()\n",
    "\n",
    "    # Tell the datetime object that it's in UTC time zone since \n",
    "    # datetime objects are 'naive' by default\n",
    "    utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "    # Convert time zone\n",
    "    est = utc.astimezone(to_zone)\n",
    "    \n",
    "    return est.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_stacker_params = {\n",
    "    'max_depth':3, \n",
    "    'metric':\"auc\", \n",
    "    'n_estimators':125, \n",
    "    'num_leaves':10, \n",
    "    'boosting_type':\"gbdt\", \n",
    "    'learning_rate':0.1, \n",
    "    'feature_fraction':0.45, \n",
    "    'colsample_bytree':0.45, \n",
    "    'bagging_fraction':0.8, \n",
    "    'bagging_freq':5, \n",
    "    'reg_lambda':0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "lgb_stacker = LightgbmBLE(None, None, params=lgb_stacker_params, nb=False, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is disabled\n",
      "XGBoostBase is initialized\n"
     ]
    }
   ],
   "source": [
    "xgb_stacker = XGBoostBLE(None, None, params={}, nb=False, seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_pool = {}\n",
    "model_pool[ModelName.LGB] = lgb_stacker\n",
    "model_pool[ModelName.XGB] = xgb_stacker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer1_oof_train_loaded, layer1_oof_test_loaded, base_layer_est_preds_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer2_inputs = {}\n",
    "# let lGB use half of the base layer model+data, and XGB use another half\n",
    "layer2_inputs[ModelName.LGB] = base_layer_results_repo.get_results(chosen_ones=selected[:int(len(selected)/2)])\n",
    "layer2_inputs[ModelName.XGB] = base_layer_results_repo.get_results(chosen_ones=selected[int(len(selected)/2):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "No evaluation set, thus not possible to use early stopping. Please train with your best params.\n",
      "starting predicting\n",
      "predicting done\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "xgb grid search file loaded. shape:(81, 36)\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n",
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "# import lightgbm as lgb\n",
    "# stacker = lgb.LGBMClassifier(max_depth=3, metric=\"auc\", n_estimators=125, num_leaves=10, \n",
    "#                              boosting_type=\"gbdt\", learning_rate=0.1, feature_fraction=0.45, \n",
    "#                              colsample_bytree=0.45, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0.2)\n",
    "\n",
    "layer2_est_preds = {} # directly preditions from the base layer estimators\n",
    "\n",
    "layer2_oof_train = {}\n",
    "layer2_oof_test = {}\n",
    "\n",
    "model_data_id_list = []\n",
    "\n",
    "# result = np.empty((test.shape[0],len(label_cols)))\n",
    "for model_name in model_pool.keys():\n",
    "    for i, label in enumerate(label_cols):\n",
    "        assert train.shape == (159571, 27)\n",
    "\n",
    "        model = model_pool[model_name]\n",
    "        if model_name == ModelName.XGB: # Done some grid search on xgb, so it has per label based best params\n",
    "            model.set_params(get_best_xgb_params(label=label))\n",
    "\n",
    "        x_train = combine_layer_oof_per_label(layer1_oof_train_loaded, label)\n",
    "        x_test = combine_layer_oof_per_label(layer1_oof_test_loaded, label)\n",
    "\n",
    "        # add engineered features to layer 2\n",
    "        x_train = np.hstack([F_train[features].as_matrix(), x_train])\n",
    "        x_test = np.hstack([F_test[features].as_matrix(), x_test])  \n",
    "\n",
    "        SEED = 1001\n",
    "        NFOLDS = 2 # set folds for out-of-fold prediction\n",
    "\n",
    "        oof_train, oof_test = get_oof(model,  x_train, train[label], x_test, NFOLDS, SEED)\n",
    "\n",
    "        if label not in layer2_oof_train:\n",
    "            layer2_oof_train[label] = []\n",
    "            layer2_oof_test[label] = []\n",
    "        layer2_oof_train[label].append(oof_train)\n",
    "        layer2_oof_test[label].append(oof_test)\n",
    "\n",
    "    #     stacker.fit(x_train, train[label])\n",
    "    #     result[:,i] = stacker.predict_proba(x_test)[:,1]\n",
    "        model_data_id = '{}_{}'.format(model_name, 'layer2')\n",
    "        model.train(x_train, train[label])\n",
    "        est_preds = model.predict(x_test)\n",
    "\n",
    "        if model_data_id not in layer2_est_preds:\n",
    "            layer2_est_preds[model_data_id] = np.empty((x_test.shape[0],len(label_cols)))\n",
    "            model_data_id_list.append(model_data_id)\n",
    "        layer2_est_preds[model_data_id][:,i] = est_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer2_oof_train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_oof_train['toxic'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.XGB_layer2', 'ModelName.LGB_layer2']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(layer2_est_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_est_preds['ModelName.LGB_layer2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModelName.LGB_layer2', 'ModelName.XGB_layer2']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_data_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_layer_results_repo.add(layer2_oof_train, layer2_oof_test, layer2_est_preds, model_data_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9827\tModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9826\tModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9819\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9818\tModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real\n",
      "0.9815\tModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9803\tModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9794\tModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9793\tModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000\n",
      "0.9786\tModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w\n",
      "0.9774\tModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9768\tModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9765\tModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9761\tModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.976\tModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.9752\tModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9726\tModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000\n",
      "0.9723\tModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000\n",
      "0.09839\tModelName.XGB_layer2\n",
      "0.09837\tModelName.LGB_layer2\n",
      "0.09825\tModelName.RNN_rnn_data_001\n",
      "0\tModelName.RNN_rnn_data_fasttext_200_300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ModelName.NBLSVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', 0.9827),\n",
       " ('ModelName.NBSVM_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', 0.9826),\n",
       " ('ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real', 0.9819),\n",
       " ('ModelName.ONELOGREG_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w_real',\n",
       "  0.9818),\n",
       " ('ModelName.NBLSVC_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9815),\n",
       " ('ModelName.NBSVM_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9803),\n",
       " ('ModelName.LGB_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9794),\n",
       " ('ModelName.LOGREG_tfidf_wordchar_charmaxdf0.300000_ng(1, 2)_wmf100000_cmf100000',\n",
       "  0.9793),\n",
       " ('ModelName.ONESVC_wordtfidf_ng13_mf10w_chartfidf_ng25_mf20w', 0.9786),\n",
       " ('ModelName.NBLSVC_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9774),\n",
       " ('ModelName.NBSVM_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9768),\n",
       " ('ModelName.NBLSVC_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9765),\n",
       " ('ModelName.NBSVM_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9761),\n",
       " ('ModelName.LOGREG_tfidf_word_df2_ng(1, 1)_wmf200000', 0.976),\n",
       " ('ModelName.LOGREG_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9752),\n",
       " ('ModelName.LGB_tfidf_word_df2_ng(1, 2)_wmf200000', 0.9726),\n",
       " ('ModelName.LGB_tfidf_word_df2_ng(1, 1)_wmf200000', 0.9723),\n",
       " ('ModelName.XGB_layer2', 0.09839),\n",
       " ('ModelName.LGB_layer2', 0.09837),\n",
       " ('ModelName.RNN_rnn_data_001', 0.09825),\n",
       " ('ModelName.RNN_rnn_data_fasttext_200_300', 0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer_results_repo.show_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1520883170\n"
     ]
    }
   ],
   "source": [
    "sub_title = 'xgb_topn_w_addedfeatures_'\n",
    "submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "submission[label_cols] = layer2_est_preds['ModelName.XGB_layer2']\n",
    "tempid = int(time.time())\n",
    "print(tempid)\n",
    "submission.to_csv('./StackPreds/TopN_XGB/{}_{}.csv'.format(sub_title, tempid), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_xgb_params(xgb_grid_search_res_path='~/data/kaggle/toxic/sc/stacking/xgb_search.csv', idx = 0, label='toxic'):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        xgb_grid_search_res_path: (str).\n",
    "        idx: load the nth best params. e.g. idx=0, load the best. idx=1, load the 2nd best\n",
    "    Returns:\n",
    "        (dict) the best xgb grid search params.\n",
    "    \"\"\"       \n",
    "    xgb_search = pd.read_csv(xgb_grid_search_res_path).sort_values(by='avg_auc', ascending=False)\n",
    "    \n",
    "    print('xgb grid search file loaded. shape:{}'.format(xgb_search.shape))\n",
    "    \n",
    "    i=idx\n",
    "\n",
    "    search_id = xgb_search['id'].values[i]\n",
    "    model_threshold = xgb_search['threshold'].values[i]\n",
    "    num_models = xgb_search['num_models'].values[i]\n",
    "    xgb_colsample_bytree = xgb_search['colsample_bytree'].values[i]\n",
    "    xgb_learning_rate = xgb_search['lr'].values[i]\n",
    "    xgb_max_depth = xgb_search['max_depth'].values[i]\n",
    "    xgb_subsample = xgb_search['subsample'].values[i]\n",
    "    xgb_gamma = xgb_search['gamma'].values[i]\n",
    "    xgb_alpha = xgb_search['alpha'].values[i]\n",
    "    xgb_cv_num_round = xgb_search['cv_num_round'].values[i]\n",
    "    xgb_cv_nfolds = xgb_search['cv_nfolds'].values[i]\n",
    "    \n",
    "    # per label based params \n",
    "    best_nrounds = {}\n",
    "    best_nrounds['toxic'] = xgb_search['toxic_best_round'].values[i]\n",
    "    best_nrounds['severe_toxic'] = xgb_search['severe_toxic_best_round'].values[i]\n",
    "    best_nrounds['obscene'] = xgb_search['obscene_best_round'].values[i]\n",
    "    best_nrounds['threat'] = xgb_search['threat_best_round'].values[i]\n",
    "    best_nrounds['insult'] = xgb_search['insult_best_round'].values[i]\n",
    "    best_nrounds['identity_hate'] = xgb_search['identity_hate_best_round'].values[i]\n",
    "    \n",
    "    scale_pos_weights = {}\n",
    "    scale_pos_weights['toxic'] = xgb_search['toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['severe_toxic'] = xgb_search['severe_toxic_pos_scale'].values[i]\n",
    "    scale_pos_weights['obscene'] = xgb_search['obscene_pos_scale'].values[i]\n",
    "    scale_pos_weights['threat'] = xgb_search['threat_pos_scale'].values[i]\n",
    "    scale_pos_weights['insult'] = xgb_search['insult_pos_scale'].values[i]\n",
    "    scale_pos_weights['identity_hate'] = xgb_search['identity_hate_pos_scale'].values[i]\n",
    "\n",
    "    metric_dict_fromcsv = {}\n",
    "    metric_dict_fromcsv['avg_auc'] = xgb_search['avg_auc'].values[i]\n",
    "    \n",
    "    xgb_params = {\n",
    "        'seed': 0,\n",
    "        'colsample_bytree': xgb_colsample_bytree,\n",
    "        'silent': 1,\n",
    "        'subsample': xgb_subsample,\n",
    "        'learning_rate': xgb_learning_rate,\n",
    "        'max_depth': xgb_max_depth,\n",
    "        'gamma': xgb_gamma,\n",
    "        'alpha': xgb_alpha,\n",
    "        'nthread': 7,\n",
    "        'min_child_weight': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'auc'\n",
    "    }\n",
    "\n",
    "    xgb_params['scale_pos_weight'] = scale_pos_weights[label]\n",
    "    xgb_params['num_boost_round'] = best_nrounds[label]\n",
    "    \n",
    "    return xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb grid search file loaded. shape:(81, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0,\n",
       " 'colsample_bytree': 0.9,\n",
       " 'eval_metric': 'auc',\n",
       " 'gamma': 2,\n",
       " 'learning_rate': 0.0966631,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'nthread': 7,\n",
       " 'num_boost_round': 119,\n",
       " 'objective': 'binary:logistic',\n",
       " 'scale_pos_weight': 120,\n",
       " 'seed': 0,\n",
       " 'silent': 1,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_xgb_params(label='threat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance({}, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "def get_oof(clf, x_train, y_train, x_test, nfolds, stratified=False, shuffle=True, seed=1001):\n",
    "    #pdb.set_trace()\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((nfolds, ntest))\n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits=nfolds, shuffle=shuffle, random_state=seed)\n",
    "    else:\n",
    "        kf = KFold(n_splits=nfolds, shuffle=shuffle, random_state=seed)\n",
    "\n",
    "    for i, (tr_index, te_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_tr, x_te = x_train[tr_index], x_train[te_index]\n",
    "        y_tr, y_te = y_train.iloc[tr_index], y_train.iloc[te_index]\n",
    "        \n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[te_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
