{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramCountVectorizer(CountVectorizer):\n",
    "    \"\"\"\n",
    "    To vectorize text with skip-grams in scikit-learn simply passing the skip gram tokens as the vocabulary \n",
    "    to CountVectorizer will not work. You need to modify the way tokens are processed which can be done with \n",
    "    a custom analyzer. Below is an example vectorizer that produces 1-skip-2-grams\n",
    "    \"\"\"\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)\n",
    "    \n",
    "# \"\"\"\n",
    "# examples:\n",
    "# text = ['the rain in Spain falls mainly on the plain']\n",
    "\n",
    "# vect = SkipGramVectorizer()\n",
    "# vect.fit(text)\n",
    "# vect.get_feature_names()\n",
    "# \"\"\"\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "class SkipGramTfidfVectorizer(SkipGramCountVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "\n",
    "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : string {'filename', 'file', 'content'}\n",
    "        If 'filename', the sequence passed as an argument to fit is\n",
    "        expected to be a list of filenames that need reading to fetch\n",
    "        the raw content to analyze.\n",
    "\n",
    "        If 'file', the sequence items must have a 'read' method (file-like\n",
    "        object) that is called to fetch the bytes in memory.\n",
    "\n",
    "        Otherwise the input is expected to be the sequence strings or\n",
    "        bytes items are expected to be analyzed directly.\n",
    "\n",
    "    encoding : string, 'utf-8' by default.\n",
    "        If bytes or files are given to analyze, this encoding is used to\n",
    "        decode.\n",
    "\n",
    "    decode_error : {'strict', 'ignore', 'replace'}\n",
    "        Instruction on what to do if a byte sequence is given to analyze that\n",
    "        contains characters not of the given `encoding`. By default, it is\n",
    "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "        values are 'ignore' and 'replace'.\n",
    "\n",
    "    strip_accents : {'ascii', 'unicode', None}\n",
    "        Remove accents during the preprocessing step.\n",
    "        'ascii' is a fast method that only works on characters that have\n",
    "        an direct ASCII mapping.\n",
    "        'unicode' is a slightly slower method that works on any characters.\n",
    "        None (default) does nothing.\n",
    "\n",
    "    analyzer : string, {'word', 'char'} or callable\n",
    "        Whether the feature should be made of word or character n-grams.\n",
    "\n",
    "        If a callable is passed it is used to extract the sequence of features\n",
    "        out of the raw, unprocessed input.\n",
    "\n",
    "    preprocessor : callable or None (default)\n",
    "        Override the preprocessing (string transformation) stage while\n",
    "        preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "    tokenizer : callable or None (default)\n",
    "        Override the string tokenization step while preserving the\n",
    "        preprocessing and n-grams generation steps.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    ngram_range : tuple (min_n, max_n)\n",
    "        The lower and upper boundary of the range of n-values for different\n",
    "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "        will be used.\n",
    "\n",
    "    stop_words : string {'english'}, list, or None (default)\n",
    "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "        list is returned. 'english' is currently the only supported string\n",
    "        value.\n",
    "\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "\n",
    "    lowercase : boolean, default True\n",
    "        Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "    token_pattern : string\n",
    "        Regular expression denoting what constitutes a \"token\", only used\n",
    "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "        or more alphanumeric characters (punctuation is completely ignored\n",
    "        and always treated as a token separator).\n",
    "\n",
    "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    min_df : float in range [0.0, 1.0] or int, default=1\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly lower than the given threshold. This value is also\n",
    "        called cut-off in the literature.\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    max_features : int or None, default=None\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus.\n",
    "\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    vocabulary : Mapping or iterable, optional\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents.\n",
    "\n",
    "    binary : boolean, default=False\n",
    "        If True, all non-zero term counts are set to 1. This does not mean\n",
    "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
    "\n",
    "    dtype : type, optional\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "    norm : 'l1', 'l2' or None, optional\n",
    "        Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "    use_idf : boolean, default=True\n",
    "        Enable inverse-document-frequency reweighting.\n",
    "\n",
    "    smooth_idf : boolean, default=True\n",
    "        Smooth idf weights by adding one to document frequencies, as if an\n",
    "        extra document was seen containing every term in the collection\n",
    "        exactly once. Prevents zero divisions.\n",
    "\n",
    "    sublinear_tf : boolean, default=False\n",
    "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "\n",
    "    idf_ : array, shape = [n_features], or None\n",
    "        The learned idf vector (global term weights)\n",
    "        when ``use_idf`` is set to True, None otherwise.\n",
    "\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "\n",
    "        This is only available if no vocabulary was given.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    CountVectorizer\n",
    "        Tokenize the documents and count the occurrences of token and return\n",
    "        them as a sparse matrix\n",
    "\n",
    "    TfidfTransformer\n",
    "        Apply Term Frequency Inverse Document Frequency normalization to a\n",
    "        sparse matrix of occurrence counts.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "\n",
    "        super(SkipGramTfidfVectorizer, self).__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,\n",
    "                                       smooth_idf=smooth_idf,\n",
    "                                       sublinear_tf=sublinear_tf)\n",
    "\n",
    "    # Broadcast the TF-IDF parameters to the underlying transformer instance\n",
    "    # for easy grid search and repr\n",
    "\n",
    "    @property\n",
    "    def norm(self):\n",
    "        return self._tfidf.norm\n",
    "\n",
    "    @norm.setter\n",
    "    def norm(self, value):\n",
    "        self._tfidf.norm = value\n",
    "\n",
    "    @property\n",
    "    def use_idf(self):\n",
    "        return self._tfidf.use_idf\n",
    "\n",
    "    @use_idf.setter\n",
    "    def use_idf(self, value):\n",
    "        self._tfidf.use_idf = value\n",
    "\n",
    "    @property\n",
    "    def smooth_idf(self):\n",
    "        return self._tfidf.smooth_idf\n",
    "\n",
    "    @smooth_idf.setter\n",
    "    def smooth_idf(self, value):\n",
    "        self._tfidf.smooth_idf = value\n",
    "\n",
    "    @property\n",
    "    def sublinear_tf(self):\n",
    "        return self._tfidf.sublinear_tf\n",
    "\n",
    "    @sublinear_tf.setter\n",
    "    def sublinear_tf(self, value):\n",
    "        self._tfidf.sublinear_tf = value\n",
    "\n",
    "    @property\n",
    "    def idf_(self):\n",
    "        return self._tfidf.idf_\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf from training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : TfidfVectorizer\n",
    "        \"\"\"\n",
    "        X = super(SkipGramTfidfVectorizer, self).fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf, return term-document matrix.\n",
    "\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        X = super(SkipGramTfidfVectorizer, self).fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        # X is already a transformed view of raw_documents so\n",
    "        # we set copy to False\n",
    "        return self._tfidf.transform(X, copy=False)\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "\n",
    "        Uses the vocabulary and document frequencies (df) learned by fit (or\n",
    "        fit_transform).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        copy : boolean, default True\n",
    "            Whether to copy X and operate on the copy or perform in-place\n",
    "            operations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n",
    "\n",
    "        X = super(SkipGramTfidfVectorizer, self).transform(raw_documents)\n",
    "        return self._tfidf.transform(X, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 30)\n",
      "(153164, 24)\n"
     ]
    }
   ],
   "source": [
    "PATH = '../data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "# train_sentence = train['comment_text_cleaned']\n",
    "# test_sentence = test['comment_text_cleaned']\n",
    "train_sentence = train['comment_text_cleaned_polarity']\n",
    "test_sentence = test['comment_text_cleaned_polarity']\n",
    "# train_sentence = train['comment_text_cleaned_features']\n",
    "# test_sentence = test['comment_text_cleaned_features']\n",
    "\n",
    "train_sentence_retain_punctuation = train['comment_text_cleaned_retain_punctuation']\n",
    "test_sentence_retain_punctuation = test['comment_text_cleaned_retain_punctuation']\n",
    "# text = pd.concat([train_sentence, test_sentence])\n",
    "text = train_sentence\n",
    "# text_retain_punctuation = pd.concat([train_sentence_retain_punctuation, test_sentence_retain_punctuation])\n",
    "text_retain_punctuation = train_sentence_retain_punctuation\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set done\n"
     ]
    }
   ],
   "source": [
    "# from textblob import TextBlob\n",
    "# # x = my_series.apply(my_function, args = (arg1,))\n",
    "# pol = pd.DataFrame()\n",
    "# pol_test = pd.DataFrame()\n",
    "# def add_polarity_phrase(x, col):\n",
    "#     score = int (TextBlob(x).sentiment.polarity * 20) \n",
    "#     if score > 0:\n",
    "#         senti = 'positive'\n",
    "#     elif score < 0:\n",
    "#         senti = 'negative'\n",
    "#         score = score * (-1)\n",
    "#     else:\n",
    "#         senti = 'neutral'\n",
    "#     return ' {}_{}_{} '.format(col,senti,score)\n",
    "\n",
    "# pol['cleaned'] = train['comment_text_cleaned'].apply(add_polarity_phrase, args=('cleaned',))\n",
    "# pol['original'] = train['comment_text'].apply(add_polarity_phrase, args=('original',))\n",
    "\n",
    "# print('train set done')\n",
    "\n",
    "# pol_test['cleaned'] = test['comment_text_cleaned'].apply(add_polarity_phrase, args=('cleaned',))\n",
    "# pol_test['original'] = test['comment_text'].apply(add_polarity_phrase, args=('original',))\n",
    "\n",
    "# train['comment_text_cleaned_polarity'] = train['comment_text_cleaned'] + pol['cleaned'] + pol['original']\n",
    "# test['comment_text_cleaned_polarity'] = test['comment_text_cleaned'] + pol_test['cleaned'] + pol_test['original']\n",
    "\n",
    "# train.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "# test.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting skip gram tfidf\n",
      "fitting skip 1 n-gram 2\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train skip gram\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test skip gram\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<159571x350000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 205858421 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('getting skip gram tfidf')\n",
    "\n",
    "skip_vectorizer = SkipGramTfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=10000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "char_vectorizer = TfidfVectorizer(ngram_range=(2,5), \n",
    "                                  strip_accents='unicode', \n",
    "                                  max_features=200000, \n",
    "                                  analyzer='char', \n",
    "                                  sublinear_tf=True)\n",
    "print('fitting skip 1 n-gram 2')\n",
    "# skip_vectorizer.fit(text.values)\n",
    "print('fitting char')\n",
    "char_vectorizer.fit(text_retain_punctuation.values)\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(text.values)\n",
    "\n",
    "print('transforming train skip gram')\n",
    "# train_skip = skip_vectorizer.transform(train_sentence.values)\n",
    "print('transforming train char')\n",
    "train_char = char_vectorizer.transform(train_sentence_retain_punctuation.values)\n",
    "print('transforming train phrase')\n",
    "train_phrase = phrase_vectorizer.transform(train_sentence.values)\n",
    "\n",
    "print('transforming test skip gram')\n",
    "# test_skip = skip_vectorizer.transform(test_sentence.values)\n",
    "print('transforming test char')\n",
    "test_char = char_vectorizer.transform(test_sentence_retain_punctuation.values)\n",
    "print('transforming test phrase')\n",
    "test_phrase = phrase_vectorizer.transform(test_sentence.values)\n",
    "\n",
    "# train_tfidf = hstack((train_skip, train_char, train_phrase), format='csr')\n",
    "# test_tfidf = hstack((test_skip, test_char, test_phrase), format='csr')\n",
    "\n",
    "train_tfidf = hstack((train_char, train_phrase), format='csr')\n",
    "test_tfidf = hstack((test_char, test_phrase), format='csr')\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def pr(y_i, y, train_features):\n",
    "    p = train_features[y==y_i].sum(0)\n",
    "    return (p + 1) / ((y == y_i).sum() + 1)\n",
    "\n",
    "def get_nblogreg_model(label_cols, train_features, train, test_features, cc):\n",
    "    preds = np.zeros((test.shape[0], len(label_cols)))\n",
    "    train_preds = np.zeros((train.shape[0], len(label_cols)))\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fit', j)\n",
    "        y = train[j].values\n",
    "        r = np.log(pr(1, y, train_features) / pr(0, y, train_features))\n",
    "        model = LogisticRegression(C=cc, max_iter = 300, n_jobs=10)\n",
    "        x_nb = train_features.multiply(r).tocsr()\n",
    "        model.fit(x_nb, y)\n",
    "        preds[:, i] = model.predict_proba(test_features.multiply(r))[:, 1]\n",
    "        train_preds[:, i] = model.predict_proba(x_nb)[:, 1]\n",
    "        print('accuracy is {}'.format(roc_auc_score(y, train_preds[:, i])))\n",
    "    return preds, train_preds\n",
    "\n",
    "\n",
    "\n",
    "def get_nbsvc_model(label_cols, train_features, train, test_features, cc):\n",
    "    preds = np.zeros((test.shape[0], len(label_cols)))\n",
    "    train_preds = np.zeros((train.shape[0], len(label_cols)))\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fit', j)\n",
    "        y = train[j].values\n",
    "        r = np.log(pr(1, y, train_features) / pr(0, y, train_features))\n",
    "#         model = SVC(C=cc, probability=True, kernel='linear')\n",
    "        lsvc = LinearSVC(C=cc)\n",
    "        model = CalibratedClassifierCV(lsvc) \n",
    "        x_nb = train_features.multiply(r).tocsr()\n",
    "        model.fit(x_nb, y)\n",
    "        preds[:, i] = model.predict_proba(test_features.multiply(r))[:, 1]\n",
    "        train_preds[:, i] = model.predict_proba(x_nb)[:, 1]\n",
    "        print('accuracy is {}'.format(roc_auc_score(y, train_preds[:, i])))\n",
    "    return preds, train_preds\n",
    "\n",
    "def save(model_name, y_test, label_cols, path, is_train=False):\n",
    "    if is_train:\n",
    "        submission = pd.read_csv(path + 'sample_train.csv')\n",
    "        file_name = 'train_' + model_name\n",
    "    else:\n",
    "        submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "        file_name = model_name\n",
    "    submission[label_cols] = y_test\n",
    "    submission.to_csv(path + file_name + '.csv', index=False)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n",
      "predicting C 0.2\n",
      "fit toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.9866589133929896\n",
      "fit severe_toxic\n",
      "accuracy is 0.9966293518971098\n",
      "fit obscene\n",
      "accuracy is 0.9959793007741083\n",
      "fit threat\n",
      "accuracy is 0.9996495826090721\n",
      "fit insult\n",
      "accuracy is 0.9912361463255193\n",
      "fit identity_hate\n",
      "accuracy is 0.9972277493221569\n",
      "total score is 0.9945635073868261\n",
      "saving files\n",
      "nblogreg_best_char25_c_0.2\n",
      "predicting C 0.25\n",
      "fit toxic\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f50d0aef99eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predicting C %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_nblogreg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total score is {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d5f43444279d>\u001b[0m in \u001b[0;36mget_nblogreg_model\u001b[0;34m(label_cols, train_features, train, test_features, cc)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "path_save = PATH = '../data/'\n",
    "for cc in [ 0.20, 0.25, 0.15 ]:\n",
    "    print('predicting C %s' % cc)\n",
    "    y_test, y_train = get_nblogreg_model(label_cols, train_tfidf, train, test_tfidf, cc)\n",
    "    print('total score is {}'.format(roc_auc_score(train[label_cols], y_train)))\n",
    "    ########################################\n",
    "    print('saving files')\n",
    "    model_name = 'nblogreg_best_char25_c_{}'.format(cc)\n",
    "    print(model_name)\n",
    "    save(model_name, y_test, label_cols, PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n",
      "predicting C 0.035\n",
      "fit toxic\n",
      "accuracy is 0.9998737219226554\n",
      "fit insult\n",
      "accuracy is 0.9928838226854799\n",
      "fit identity_hate\n",
      "accuracy is 0.9989625049550399\n",
      "total score is 0.9959041108606087\n",
      "saving files\n",
      "nbsvm_best_c_0.025\n",
      "predicting C 0.015\n",
      "fit toxic\n",
      "accuracy is 0.9866429701139954\n",
      "fit severe_toxic\n",
      "accuracy is 0.9971267053302648\n",
      "fit obscene\n",
      "accuracy is 0.9963451576863208\n",
      "fit threat\n",
      "accuracy is 0.999780081790533\n",
      "fit insult\n",
      "accuracy is 0.9913971246702713\n",
      "fit identity_hate\n",
      "accuracy is 0.9981750715260507\n",
      "total score is 0.9949111851862393\n",
      "saving files\n",
      "nbsvm_best_c_0.015\n",
      "predicting C 0.017\n",
      "fit toxic\n",
      "accuracy is 0.9871294096169617\n",
      "fit severe_toxic\n",
      "accuracy is 0.9973901277492568\n",
      "fit obscene\n",
      "accuracy is 0.9964859900847332\n",
      "fit threat\n",
      "accuracy is 0.999809156124492\n",
      "fit insult\n",
      "accuracy is 0.9917571483462866\n",
      "fit identity_hate\n",
      "accuracy is 0.9983956695256386\n",
      "total score is 0.9951612502412281\n",
      "saving files\n",
      "nbsvm_best_c_0.017\n",
      "predicting C 0.023\n",
      "fit toxic\n",
      "accuracy is 0.9883203902323011\n",
      "fit severe_toxic\n",
      "accuracy is 0.9979798645657537\n",
      "fit obscene\n",
      "accuracy is 0.996829148591489\n",
      "fit threat\n",
      "accuracy is 0.999861992250158\n",
      "fit insult\n",
      "accuracy is 0.9926390352075839\n",
      "fit identity_hate\n",
      "accuracy is 0.9988526559532054\n",
      "total score is 0.9957471811334152\n",
      "saving files\n",
      "nbsvm_best_c_0.023\n"
     ]
    }
   ],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "path_save = PATH = '../data/'\n",
    "for cc in [ 0.035,0.03,0.025,0.015,0.017,0.023 ]:\n",
    "    print('predicting C %s' % cc)\n",
    "    y_test, y_train = get_nbsvc_model(label_cols, train_tfidf, train, test_tfidf, cc)\n",
    "    print('total score is {}'.format(roc_auc_score(train[label_cols], y_train)))\n",
    "    ########################################\n",
    "    print('saving files')\n",
    "    model_name = 'nbsvm_best_c_{}'.format(cc)\n",
    "    print(model_name)\n",
    "    save(model_name, y_test, label_cols, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = []\n",
    "for word in phrase_vectorizer.vocabulary_:\n",
    "    if len(word.split(' ')) == 1:\n",
    "        one.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "original = []\n",
    "for each in one:\n",
    "    if each[:7] == 'cleaned':\n",
    "        cleaned.append(each)\n",
    "    elif each[:8] == 'original':\n",
    "        original.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
