{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramCountVectorizer(CountVectorizer):\n",
    "    \"\"\"\n",
    "    To vectorize text with skip-grams in scikit-learn simply passing the skip gram tokens as the vocabulary \n",
    "    to CountVectorizer will not work. You need to modify the way tokens are processed which can be done with \n",
    "    a custom analyzer. Below is an example vectorizer that produces 1-skip-2-grams\n",
    "    \"\"\"\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)\n",
    "    \n",
    "# \"\"\"\n",
    "# examples:\n",
    "# text = ['the rain in Spain falls mainly on the plain']\n",
    "\n",
    "# vect = SkipGramVectorizer()\n",
    "# vect.fit(text)\n",
    "# vect.get_feature_names()\n",
    "# \"\"\"\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "class SkipGramTfidfVectorizer(SkipGramCountVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "\n",
    "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : string {'filename', 'file', 'content'}\n",
    "        If 'filename', the sequence passed as an argument to fit is\n",
    "        expected to be a list of filenames that need reading to fetch\n",
    "        the raw content to analyze.\n",
    "\n",
    "        If 'file', the sequence items must have a 'read' method (file-like\n",
    "        object) that is called to fetch the bytes in memory.\n",
    "\n",
    "        Otherwise the input is expected to be the sequence strings or\n",
    "        bytes items are expected to be analyzed directly.\n",
    "\n",
    "    encoding : string, 'utf-8' by default.\n",
    "        If bytes or files are given to analyze, this encoding is used to\n",
    "        decode.\n",
    "\n",
    "    decode_error : {'strict', 'ignore', 'replace'}\n",
    "        Instruction on what to do if a byte sequence is given to analyze that\n",
    "        contains characters not of the given `encoding`. By default, it is\n",
    "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "        values are 'ignore' and 'replace'.\n",
    "\n",
    "    strip_accents : {'ascii', 'unicode', None}\n",
    "        Remove accents during the preprocessing step.\n",
    "        'ascii' is a fast method that only works on characters that have\n",
    "        an direct ASCII mapping.\n",
    "        'unicode' is a slightly slower method that works on any characters.\n",
    "        None (default) does nothing.\n",
    "\n",
    "    analyzer : string, {'word', 'char'} or callable\n",
    "        Whether the feature should be made of word or character n-grams.\n",
    "\n",
    "        If a callable is passed it is used to extract the sequence of features\n",
    "        out of the raw, unprocessed input.\n",
    "\n",
    "    preprocessor : callable or None (default)\n",
    "        Override the preprocessing (string transformation) stage while\n",
    "        preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "    tokenizer : callable or None (default)\n",
    "        Override the string tokenization step while preserving the\n",
    "        preprocessing and n-grams generation steps.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    ngram_range : tuple (min_n, max_n)\n",
    "        The lower and upper boundary of the range of n-values for different\n",
    "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "        will be used.\n",
    "\n",
    "    stop_words : string {'english'}, list, or None (default)\n",
    "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "        list is returned. 'english' is currently the only supported string\n",
    "        value.\n",
    "\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "\n",
    "    lowercase : boolean, default True\n",
    "        Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "    token_pattern : string\n",
    "        Regular expression denoting what constitutes a \"token\", only used\n",
    "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "        or more alphanumeric characters (punctuation is completely ignored\n",
    "        and always treated as a token separator).\n",
    "\n",
    "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    min_df : float in range [0.0, 1.0] or int, default=1\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly lower than the given threshold. This value is also\n",
    "        called cut-off in the literature.\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    max_features : int or None, default=None\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus.\n",
    "\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    vocabulary : Mapping or iterable, optional\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents.\n",
    "\n",
    "    binary : boolean, default=False\n",
    "        If True, all non-zero term counts are set to 1. This does not mean\n",
    "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
    "\n",
    "    dtype : type, optional\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "    norm : 'l1', 'l2' or None, optional\n",
    "        Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "    use_idf : boolean, default=True\n",
    "        Enable inverse-document-frequency reweighting.\n",
    "\n",
    "    smooth_idf : boolean, default=True\n",
    "        Smooth idf weights by adding one to document frequencies, as if an\n",
    "        extra document was seen containing every term in the collection\n",
    "        exactly once. Prevents zero divisions.\n",
    "\n",
    "    sublinear_tf : boolean, default=False\n",
    "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "\n",
    "    idf_ : array, shape = [n_features], or None\n",
    "        The learned idf vector (global term weights)\n",
    "        when ``use_idf`` is set to True, None otherwise.\n",
    "\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "\n",
    "        This is only available if no vocabulary was given.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    CountVectorizer\n",
    "        Tokenize the documents and count the occurrences of token and return\n",
    "        them as a sparse matrix\n",
    "\n",
    "    TfidfTransformer\n",
    "        Apply Term Frequency Inverse Document Frequency normalization to a\n",
    "        sparse matrix of occurrence counts.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "\n",
    "        super(SkipGramTfidfVectorizer, self).__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,\n",
    "                                       smooth_idf=smooth_idf,\n",
    "                                       sublinear_tf=sublinear_tf)\n",
    "\n",
    "    # Broadcast the TF-IDF parameters to the underlying transformer instance\n",
    "    # for easy grid search and repr\n",
    "\n",
    "    @property\n",
    "    def norm(self):\n",
    "        return self._tfidf.norm\n",
    "\n",
    "    @norm.setter\n",
    "    def norm(self, value):\n",
    "        self._tfidf.norm = value\n",
    "\n",
    "    @property\n",
    "    def use_idf(self):\n",
    "        return self._tfidf.use_idf\n",
    "\n",
    "    @use_idf.setter\n",
    "    def use_idf(self, value):\n",
    "        self._tfidf.use_idf = value\n",
    "\n",
    "    @property\n",
    "    def smooth_idf(self):\n",
    "        return self._tfidf.smooth_idf\n",
    "\n",
    "    @smooth_idf.setter\n",
    "    def smooth_idf(self, value):\n",
    "        self._tfidf.smooth_idf = value\n",
    "\n",
    "    @property\n",
    "    def sublinear_tf(self):\n",
    "        return self._tfidf.sublinear_tf\n",
    "\n",
    "    @sublinear_tf.setter\n",
    "    def sublinear_tf(self, value):\n",
    "        self._tfidf.sublinear_tf = value\n",
    "\n",
    "    @property\n",
    "    def idf_(self):\n",
    "        return self._tfidf.idf_\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf from training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : TfidfVectorizer\n",
    "        \"\"\"\n",
    "        X = super(SkipGramTfidfVectorizer, self).fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf, return term-document matrix.\n",
    "\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        X = super(SkipGramTfidfVectorizer, self).fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        # X is already a transformed view of raw_documents so\n",
    "        # we set copy to False\n",
    "        return self._tfidf.transform(X, copy=False)\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "\n",
    "        Uses the vocabulary and document frequencies (df) learned by fit (or\n",
    "        fit_transform).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        copy : boolean, default True\n",
    "            Whether to copy X and operate on the copy or perform in-place\n",
    "            operations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n",
    "\n",
    "        X = super(SkipGramTfidfVectorizer, self).transform(raw_documents)\n",
    "        return self._tfidf.transform(X, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 30)\n",
      "(153164, 24)\n"
     ]
    }
   ],
   "source": [
    "PATH = '../data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "# train_sentence = train['comment_text_cleaned']\n",
    "# test_sentence = test['comment_text_cleaned']\n",
    "train_sentence = train['comment_text_cleaned_polarity']\n",
    "test_sentence = test['comment_text_cleaned_polarity']\n",
    "\n",
    "\n",
    "train_sentence_retain_punctuation = train['comment_text_cleaned_retain_punctuation']\n",
    "test_sentence_retain_punctuation = test['comment_text_cleaned_retain_punctuation']\n",
    "# text = pd.concat([train_sentence, test_sentence])\n",
    "text = train_sentence\n",
    "# text_retain_punctuation = pd.concat([train_sentence_retain_punctuation, test_sentence_retain_punctuation])\n",
    "text_retain_punctuation = train_sentence_retain_punctuation\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set done\n"
     ]
    }
   ],
   "source": [
    "# from textblob import TextBlob\n",
    "# # x = my_series.apply(my_function, args = (arg1,))\n",
    "# pol = pd.DataFrame()\n",
    "# pol_test = pd.DataFrame()\n",
    "# def add_polarity_phrase(x, col):\n",
    "#     score = int (TextBlob(x).sentiment.polarity * 20) \n",
    "#     if score > 0:\n",
    "#         senti = 'positive'\n",
    "#     elif score < 0:\n",
    "#         senti = 'negative'\n",
    "#         score = score * (-1)\n",
    "#     else:\n",
    "#         senti = 'neutral'\n",
    "#     return ' {}_{}_{} '.format(col,senti,score)\n",
    "\n",
    "# pol['cleaned'] = train['comment_text_cleaned'].apply(add_polarity_phrase, args=('cleaned',))\n",
    "# pol['original'] = train['comment_text'].apply(add_polarity_phrase, args=('original',))\n",
    "\n",
    "# print('train set done')\n",
    "\n",
    "# pol_test['cleaned'] = test['comment_text_cleaned'].apply(add_polarity_phrase, args=('cleaned',))\n",
    "# pol_test['original'] = test['comment_text'].apply(add_polarity_phrase, args=('original',))\n",
    "\n",
    "# train['comment_text_cleaned_polarity'] = train['comment_text_cleaned'] + pol['cleaned'] + pol['original']\n",
    "# test['comment_text_cleaned_polarity'] = test['comment_text_cleaned'] + pol_test['cleaned'] + pol_test['original']\n",
    "\n",
    "# train.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "# test.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting skip gram tfidf\n",
      "fitting skip 1 n-gram 2\n",
      "fitting char\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-91fed55fc548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# skip_vectorizer.fit(text.values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fitting char'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mchar_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_retain_punctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fitting phrase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mphrase_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('getting skip gram tfidf')\n",
    "\n",
    "skip_vectorizer = SkipGramTfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=10000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "char_vectorizer = TfidfVectorizer(ngram_range=(1,5), \n",
    "                                  strip_accents='unicode', \n",
    "                                  max_features=200000, \n",
    "                                  analyzer='char', \n",
    "                                  sublinear_tf=True)\n",
    "print('fitting skip 1 n-gram 2')\n",
    "# skip_vectorizer.fit(text.values)\n",
    "print('fitting char')\n",
    "char_vectorizer.fit(text_retain_punctuation.values)\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(text.values)\n",
    "\n",
    "print('transforming train skip gram')\n",
    "# train_skip = skip_vectorizer.transform(train_sentence.values)\n",
    "print('transforming train char')\n",
    "train_char = char_vectorizer.transform(train_sentence_retain_punctuation.values)\n",
    "print('transforming train phrase')\n",
    "train_phrase = phrase_vectorizer.transform(train_sentence.values)\n",
    "\n",
    "print('transforming test skip gram')\n",
    "# test_skip = skip_vectorizer.transform(test_sentence.values)\n",
    "print('transforming test char')\n",
    "test_char = char_vectorizer.transform(test_sentence_retain_punctuation.values)\n",
    "print('transforming test phrase')\n",
    "test_phrase = phrase_vectorizer.transform(test_sentence.values)\n",
    "\n",
    "# train_tfidf = hstack((train_skip, train_char, train_phrase), format='csr')\n",
    "# test_tfidf = hstack((test_skip, test_char, test_phrase), format='csr')\n",
    "\n",
    "train_tfidf = hstack((train_char, train_phrase), format='csr')\n",
    "test_tfidf = hstack((test_char, test_phrase), format='csr')\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "getting appearance\n",
      "severe_toxic\n",
      "getting appearance\n",
      "obscene\n",
      "getting appearance\n",
      "threat\n",
      "getting appearance\n",
      "insult\n",
      "getting appearance\n",
      "identity_hate\n",
      "getting appearance\n"
     ]
    }
   ],
   "source": [
    "##### getting naive bayes matrix\n",
    "def pr(y_i, y, train_features):\n",
    "    p = train_features[y==y_i].sum(0)\n",
    "    return (p + 1) / ((y == y_i).sum() + 1)\n",
    "\n",
    "def get_conditional_matrix(m_train,m_test,y):\n",
    "    x_train = (m_train > 0) * 1\n",
    "    x_test = (m_test > 0 ) * 1\n",
    "    count = x_train.sum(0)\n",
    "    appearance = x_train[y == 1].sum(0)\n",
    "    x_train = x_train.multiply(appearance / count).tocsr()\n",
    "    x_test = x_test.multiply(appearance / count).tocsr()\n",
    "    return x_train, x_test\n",
    "    # conditional probality\n",
    "\n",
    "# train_tfidf_nb = {}\n",
    "# test_tfidf_nb = {}\n",
    "# train_appearance_cp = {}\n",
    "# test_appearance_cp = {}\n",
    "train_model = {}\n",
    "test_model = {}\n",
    "for col in label_cols:\n",
    "    print(col)\n",
    "    y = train[col].values\n",
    "    r = np.log(pr(1, y, train_tfidf) / pr(0, y, train_tfidf))\n",
    "    train_tfidf_nb = train_tfidf.multiply(r).tocsr()\n",
    "    test_tfidf_nb = test_tfidf.multiply(r).tocsr()\n",
    "    print('getting appearance')\n",
    "    train_appearance_cp,test_appearance_cp = get_conditional_matrix(train_tfidf,test_tfidf,y)\n",
    "    \n",
    "    train_model[col] = hstack((train_tfidf_nb, train_appearance_cp), format='csr')\n",
    "    test_model[col] = hstack((test_tfidf_nb, test_appearance_cp), format='csr')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n",
      "0.15\n",
      "toxic\n",
      "accuracy is 0.9894004626373385\n",
      "severe_toxic\n",
      "accuracy is 0.9958259680887998\n",
      "obscene\n",
      "accuracy is 0.9959957320951517\n",
      "threat\n",
      "accuracy is 0.9993480563866923\n",
      "insult\n",
      "accuracy is 0.9912481775218893\n",
      "identity_hate\n",
      "accuracy is 0.9959097457993027\n",
      "0.1\n",
      "toxic\n",
      "accuracy is 0.9869235967498293\n",
      "severe_toxic\n",
      "accuracy is 0.9946969445618739\n",
      "obscene\n",
      "accuracy is 0.9952001217346296\n",
      "threat\n",
      "accuracy is 0.998815737023057\n",
      "insult\n",
      "accuracy is 0.9897287820357448\n",
      "identity_hate\n",
      "accuracy is 0.9940579276972978\n",
      "0.08\n",
      "toxic\n",
      "accuracy is 0.9855126097609673\n",
      "severe_toxic\n",
      "accuracy is 0.9940842646944665\n",
      "obscene\n",
      "accuracy is 0.9947330375466854\n",
      "threat\n",
      "accuracy is 0.9984193214321342\n",
      "insult\n",
      "accuracy is 0.9888819928590162\n",
      "identity_hate\n",
      "accuracy is 0.992961669668828\n",
      "0.07\n",
      "toxic\n",
      "accuracy is 0.9846491697183067\n",
      "severe_toxic\n",
      "accuracy is 0.9937224383752272\n",
      "obscene\n",
      "accuracy is 0.994446665779438\n",
      "threat\n",
      "accuracy is 0.9981407417103235\n",
      "insult\n",
      "accuracy is 0.9883692334128905\n",
      "identity_hate\n",
      "accuracy is 0.9922976076803492\n",
      "0.05\n",
      "toxic\n",
      "accuracy is 0.9824232038196066\n",
      "severe_toxic\n",
      "accuracy is 0.9928346323944608\n",
      "obscene\n",
      "accuracy is 0.9937090725495281\n",
      "threat\n",
      "accuracy is 0.9973060939830278\n",
      "insult\n",
      "accuracy is 0.9870603286281787\n",
      "identity_hate\n",
      "accuracy is 0.9906372187102132\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "#### train model\n",
    "for cc in [ 0.15, 0.10, 0.08, 0.07, 0.05 ]:\n",
    "    print(cc)\n",
    "    preds_train = pd.DataFrame()\n",
    "    preds_test = pd.DataFrame()\n",
    "    for col in label_cols:\n",
    "        print(col)\n",
    "        model = LogisticRegression(C=cc)\n",
    "        model.fit(train_model[col], train[col].values)\n",
    "        preds_test[col] = model.predict_proba(test_model[col])[:, 1]\n",
    "        preds_train[col] = model.predict_proba(train_model[col])[:, 1]\n",
    "        print('accuracy is {}'.format(roc_auc_score(train[col].values, preds_train[col])))\n",
    "    preds_test['id'] = test['id']\n",
    "    preds_test.to_csv(PATH + 'nblogreg_ori_trainOntrain_char_punctuation_polarity_appearance_c_{}.csv'.format(cc), index= False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = pd.read_csv('../data/nblogreg_ori_trainOntrain_char_punctuation_polarity_appearance_c_0.2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate',\n",
       "       'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa.to_csv('../data/nblogreg_ori_trainOntrain_char_punctuation_polarity_appearance_c_0.2.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_nblogreg_model(label_cols, train_features, train, test_features, cc):\n",
    "    preds = np.zeros((test.shape[0], len(label_cols)))\n",
    "    train_preds = np.zeros((train.shape[0], len(label_cols)))\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fit', j)\n",
    "        y = train[j].values\n",
    "        r = np.log(pr(1, y, train_features) / pr(0, y, train_features))\n",
    "        model = LogisticRegression(C=cc, max_iter = 300, n_jobs=10)\n",
    "        x_nb = train_features.multiply(r).tocsr()\n",
    "        model.fit(x_nb, y)\n",
    "        preds[:, i] = model.predict_proba(test_features.multiply(r))[:, 1]\n",
    "        train_preds[:, i] = model.predict_proba(x_nb)[:, 1]\n",
    "        print('accuracy is {}'.format(roc_auc_score(y, train_preds[:, i])))\n",
    "    return preds, train_preds\n",
    "\n",
    "def save(model_name, y_test, label_cols, path, is_train=False):\n",
    "    if is_train:\n",
    "        submission = pd.read_csv(path + 'sample_train.csv')\n",
    "        file_name = 'train_' + model_name\n",
    "    else:\n",
    "        submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "        file_name = model_name\n",
    "    submission[label_cols] = y_test\n",
    "    submission.to_csv(path + file_name + '.csv', index=False)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n",
      "predicting C 0.2\n",
      "fit toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.986595103115305\n",
      "fit severe_toxic\n",
      "accuracy is 0.9965187402776788\n",
      "fit obscene\n",
      "accuracy is 0.9960058109640987\n",
      "fit threat\n",
      "accuracy is 0.9995928672755734\n",
      "fit insult\n",
      "accuracy is 0.9911548001731012\n",
      "fit identity_hate\n",
      "accuracy is 0.9970560571007812\n",
      "total score is 0.9944872298177563\n",
      "saving files\n",
      "nblogreg_ori_trainOntrain_char_punctuation_polarity_c_0.2\n",
      "predicting C 0.25\n",
      "fit toxic\n",
      "accuracy is 0.9875469413436043\n",
      "fit severe_toxic\n",
      "accuracy is 0.9971036392496745\n",
      "fit obscene\n",
      "accuracy is 0.9963109589225135\n",
      "fit threat\n",
      "accuracy is 0.9997286526995723\n",
      "fit insult\n",
      "accuracy is 0.9918339267281295\n",
      "fit identity_hate\n",
      "accuracy is 0.9977976267377627\n",
      "total score is 0.9950536242802094\n",
      "saving files\n",
      "nblogreg_ori_trainOntrain_char_punctuation_polarity_c_0.25\n",
      "predicting C 0.15\n",
      "fit toxic\n",
      "accuracy is 0.9853769137852342\n",
      "fit severe_toxic\n",
      "accuracy is 0.9957561626360292\n",
      "fit obscene\n",
      "accuracy is 0.9956171694955567\n",
      "fit threat\n",
      "accuracy is 0.9993460707582762\n",
      "fit insult\n",
      "accuracy is 0.9903076310462371\n",
      "fit identity_hate\n",
      "accuracy is 0.995935656681797\n",
      "total score is 0.9937232674005219\n",
      "saving files\n",
      "nblogreg_ori_trainOntrain_char_punctuation_polarity_c_0.15\n",
      "predicting C 0.1\n",
      "fit toxic\n",
      "accuracy is 0.9836864567053927\n",
      "fit severe_toxic\n",
      "accuracy is 0.9946885190131655\n",
      "fit obscene\n",
      "accuracy is 0.9950722307637158\n",
      "fit threat\n",
      "accuracy is 0.9988087018495301\n",
      "fit insult\n",
      "accuracy is 0.9891666926759554\n",
      "fit identity_hate\n",
      "accuracy is 0.9941203964140024\n",
      "total score is 0.9925904995702935\n",
      "saving files\n",
      "nblogreg_ori_trainOntrain_char_punctuation_polarity_c_0.1\n",
      "predicting C 0.08\n",
      "fit toxic\n",
      "accuracy is 0.98277002225476\n",
      "fit severe_toxic\n",
      "accuracy is 0.9941145299956679\n",
      "fit obscene\n",
      "accuracy is 0.9947733146461751\n",
      "fit threat\n",
      "accuracy is 0.9984097088866235\n",
      "fit insult\n",
      "accuracy is 0.9885635017499356\n",
      "fit identity_hate\n",
      "accuracy is 0.9930473402803119\n",
      "total score is 0.9919464029689125\n",
      "saving files\n",
      "nblogreg_ori_trainOntrain_char_punctuation_polarity_c_0.08\n",
      "predicting C 0.07\n",
      "fit toxic\n",
      "accuracy is 0.9822240945619931\n",
      "fit severe_toxic\n",
      "accuracy is 0.993780587758023\n",
      "fit obscene\n",
      "accuracy is 0.9946002453742898\n",
      "fit threat\n",
      "accuracy is 0.9981279205996904\n",
      "fit insult\n",
      "accuracy is 0.9882102301393288\n",
      "fit identity_hate\n",
      "accuracy is 0.9923915852541608\n",
      "total score is 0.9915557772812478\n",
      "saving files\n",
      "nblogreg_ori_trainOntrain_char_punctuation_polarity_c_0.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_save = PATH = '../data/'\n",
    "for cc in [ 0.20, 0.25, 0.15, 0.10, 0.08, 0.07 ]:\n",
    "    print('predicting C %s' % cc)\n",
    "    y_test, y_train = get_nblogreg_model(label_cols, train_tfidf, train, test_tfidf, cc)\n",
    "    print('total score is {}'.format(roc_auc_score(train[label_cols], y_train)))\n",
    "    ########################################\n",
    "    print('saving files')\n",
    "    model_name = 'nblogreg_ori_trainOntrain_char_punctuation_polarity_c_{}'.format(cc)\n",
    "    print(model_name)\n",
    "    save(model_name, y_test, label_cols, PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = []\n",
    "for word in phrase_vectorizer.vocabulary_:\n",
    "    if len(word.split(' ')) == 1:\n",
    "        one.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "original = []\n",
    "for each in one:\n",
    "    if each[:7] == 'cleaned':\n",
    "        cleaned.append(each)\n",
    "    elif each[:8] == 'original':\n",
    "        original.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
