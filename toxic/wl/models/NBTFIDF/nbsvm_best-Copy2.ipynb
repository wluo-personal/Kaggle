{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from toolz import itertoolz, compose\n",
    "from toolz.curried import map as cmap, sliding_window, pluck\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class SkipGramCountVectorizer(CountVectorizer):\n",
    "    \"\"\"\n",
    "    To vectorize text with skip-grams in scikit-learn simply passing the skip gram tokens as the vocabulary \n",
    "    to CountVectorizer will not work. You need to modify the way tokens are processed which can be done with \n",
    "    a custom analyzer. Below is an example vectorizer that produces 1-skip-2-grams\n",
    "    \"\"\"\n",
    "    def build_analyzer(self):    \n",
    "        preprocess = self.build_preprocessor()\n",
    "        stop_words = self.get_stop_words()\n",
    "        tokenize = self.build_tokenizer()\n",
    "        return lambda doc: self._word_skip_grams(\n",
    "                compose(tokenize, preprocess, self.decode)(doc),\n",
    "                stop_words)\n",
    "\n",
    "    def _word_skip_grams(self, tokens, stop_words=None):\n",
    "        # handle stop words\n",
    "        if stop_words is not None:\n",
    "            tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "        return compose(cmap(' '.join), pluck([0, 2]), sliding_window(3))(tokens)\n",
    "    \n",
    "# \"\"\"\n",
    "# examples:\n",
    "# text = ['the rain in Spain falls mainly on the plain']\n",
    "\n",
    "# vect = SkipGramVectorizer()\n",
    "# vect.fit(text)\n",
    "# vect.get_feature_names()\n",
    "# \"\"\"\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "class SkipGramTfidfVectorizer(SkipGramCountVectorizer):\n",
    "    \"\"\"Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "\n",
    "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : string {'filename', 'file', 'content'}\n",
    "        If 'filename', the sequence passed as an argument to fit is\n",
    "        expected to be a list of filenames that need reading to fetch\n",
    "        the raw content to analyze.\n",
    "\n",
    "        If 'file', the sequence items must have a 'read' method (file-like\n",
    "        object) that is called to fetch the bytes in memory.\n",
    "\n",
    "        Otherwise the input is expected to be the sequence strings or\n",
    "        bytes items are expected to be analyzed directly.\n",
    "\n",
    "    encoding : string, 'utf-8' by default.\n",
    "        If bytes or files are given to analyze, this encoding is used to\n",
    "        decode.\n",
    "\n",
    "    decode_error : {'strict', 'ignore', 'replace'}\n",
    "        Instruction on what to do if a byte sequence is given to analyze that\n",
    "        contains characters not of the given `encoding`. By default, it is\n",
    "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "        values are 'ignore' and 'replace'.\n",
    "\n",
    "    strip_accents : {'ascii', 'unicode', None}\n",
    "        Remove accents during the preprocessing step.\n",
    "        'ascii' is a fast method that only works on characters that have\n",
    "        an direct ASCII mapping.\n",
    "        'unicode' is a slightly slower method that works on any characters.\n",
    "        None (default) does nothing.\n",
    "\n",
    "    analyzer : string, {'word', 'char'} or callable\n",
    "        Whether the feature should be made of word or character n-grams.\n",
    "\n",
    "        If a callable is passed it is used to extract the sequence of features\n",
    "        out of the raw, unprocessed input.\n",
    "\n",
    "    preprocessor : callable or None (default)\n",
    "        Override the preprocessing (string transformation) stage while\n",
    "        preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "    tokenizer : callable or None (default)\n",
    "        Override the string tokenization step while preserving the\n",
    "        preprocessing and n-grams generation steps.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    ngram_range : tuple (min_n, max_n)\n",
    "        The lower and upper boundary of the range of n-values for different\n",
    "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "        will be used.\n",
    "\n",
    "    stop_words : string {'english'}, list, or None (default)\n",
    "        If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "        list is returned. 'english' is currently the only supported string\n",
    "        value.\n",
    "\n",
    "        If a list, that list is assumed to contain stop words, all of which\n",
    "        will be removed from the resulting tokens.\n",
    "        Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "        If None, no stop words will be used. max_df can be set to a value\n",
    "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "        words based on intra corpus document frequency of terms.\n",
    "\n",
    "    lowercase : boolean, default True\n",
    "        Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "    token_pattern : string\n",
    "        Regular expression denoting what constitutes a \"token\", only used\n",
    "        if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "        or more alphanumeric characters (punctuation is completely ignored\n",
    "        and always treated as a token separator).\n",
    "\n",
    "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly higher than the given threshold (corpus-specific\n",
    "        stop words).\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    min_df : float in range [0.0, 1.0] or int, default=1\n",
    "        When building the vocabulary ignore terms that have a document\n",
    "        frequency strictly lower than the given threshold. This value is also\n",
    "        called cut-off in the literature.\n",
    "        If float, the parameter represents a proportion of documents, integer\n",
    "        absolute counts.\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    max_features : int or None, default=None\n",
    "        If not None, build a vocabulary that only consider the top\n",
    "        max_features ordered by term frequency across the corpus.\n",
    "\n",
    "        This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "    vocabulary : Mapping or iterable, optional\n",
    "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "        indices in the feature matrix, or an iterable over terms. If not\n",
    "        given, a vocabulary is determined from the input documents.\n",
    "\n",
    "    binary : boolean, default=False\n",
    "        If True, all non-zero term counts are set to 1. This does not mean\n",
    "        outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
    "\n",
    "    dtype : type, optional\n",
    "        Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "    norm : 'l1', 'l2' or None, optional\n",
    "        Norm used to normalize term vectors. None for no normalization.\n",
    "\n",
    "    use_idf : boolean, default=True\n",
    "        Enable inverse-document-frequency reweighting.\n",
    "\n",
    "    smooth_idf : boolean, default=True\n",
    "        Smooth idf weights by adding one to document frequencies, as if an\n",
    "        extra document was seen containing every term in the collection\n",
    "        exactly once. Prevents zero divisions.\n",
    "\n",
    "    sublinear_tf : boolean, default=False\n",
    "        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    vocabulary_ : dict\n",
    "        A mapping of terms to feature indices.\n",
    "\n",
    "    idf_ : array, shape = [n_features], or None\n",
    "        The learned idf vector (global term weights)\n",
    "        when ``use_idf`` is set to True, None otherwise.\n",
    "\n",
    "    stop_words_ : set\n",
    "        Terms that were ignored because they either:\n",
    "\n",
    "          - occurred in too many documents (`max_df`)\n",
    "          - occurred in too few documents (`min_df`)\n",
    "          - were cut off by feature selection (`max_features`).\n",
    "\n",
    "        This is only available if no vocabulary was given.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    CountVectorizer\n",
    "        Tokenize the documents and count the occurrences of token and return\n",
    "        them as a sparse matrix\n",
    "\n",
    "    TfidfTransformer\n",
    "        Apply Term Frequency Inverse Document Frequency normalization to a\n",
    "        sparse matrix of occurrence counts.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``stop_words_`` attribute can get large and increase the model size\n",
    "    when pickling. This attribute is provided only for introspection and can\n",
    "    be safely removed using delattr or set to None before pickling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "\n",
    "        super(SkipGramTfidfVectorizer, self).__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,\n",
    "                                       smooth_idf=smooth_idf,\n",
    "                                       sublinear_tf=sublinear_tf)\n",
    "\n",
    "    # Broadcast the TF-IDF parameters to the underlying transformer instance\n",
    "    # for easy grid search and repr\n",
    "\n",
    "    @property\n",
    "    def norm(self):\n",
    "        return self._tfidf.norm\n",
    "\n",
    "    @norm.setter\n",
    "    def norm(self, value):\n",
    "        self._tfidf.norm = value\n",
    "\n",
    "    @property\n",
    "    def use_idf(self):\n",
    "        return self._tfidf.use_idf\n",
    "\n",
    "    @use_idf.setter\n",
    "    def use_idf(self, value):\n",
    "        self._tfidf.use_idf = value\n",
    "\n",
    "    @property\n",
    "    def smooth_idf(self):\n",
    "        return self._tfidf.smooth_idf\n",
    "\n",
    "    @smooth_idf.setter\n",
    "    def smooth_idf(self, value):\n",
    "        self._tfidf.smooth_idf = value\n",
    "\n",
    "    @property\n",
    "    def sublinear_tf(self):\n",
    "        return self._tfidf.sublinear_tf\n",
    "\n",
    "    @sublinear_tf.setter\n",
    "    def sublinear_tf(self, value):\n",
    "        self._tfidf.sublinear_tf = value\n",
    "\n",
    "    @property\n",
    "    def idf_(self):\n",
    "        return self._tfidf.idf_\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf from training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : TfidfVectorizer\n",
    "        \"\"\"\n",
    "        X = super(SkipGramTfidfVectorizer, self).fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn vocabulary and idf, return term-document matrix.\n",
    "\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        X = super(SkipGramTfidfVectorizer, self).fit_transform(raw_documents)\n",
    "        self._tfidf.fit(X)\n",
    "        # X is already a transformed view of raw_documents so\n",
    "        # we set copy to False\n",
    "        return self._tfidf.transform(X, copy=False)\n",
    "\n",
    "    def transform(self, raw_documents, copy=True):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "\n",
    "        Uses the vocabulary and document frequencies (df) learned by fit (or\n",
    "        fit_transform).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        copy : boolean, default True\n",
    "            Whether to copy X and operate on the copy or perform in-place\n",
    "            operations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Tf-idf-weighted document-term matrix.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n",
    "\n",
    "        X = super(SkipGramTfidfVectorizer, self).transform(raw_documents)\n",
    "        return self._tfidf.transform(X, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = '../../dataset/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "train_sentence_retain_punctuation = train['comment_text_cleaned_retain_punctuation']\n",
    "test_sentence_retain_punctuation = test['comment_text_cleaned_retain_punctuation']\n",
    "text = pd.concat([train_sentence, test_sentence])\n",
    "text = train_sentence\n",
    "text_retain_punctuation = pd.concat([train_sentence_retain_punctuation, test_sentence_retain_punctuation])\n",
    "text_retain_punctuation = train_sentence_retain_punctuation\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('getting skip gram tfidf')\n",
    "\n",
    "skip_vectorizer = SkipGramTfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=10000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "\n",
    "phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "char_vectorizer = TfidfVectorizer(ngram_range=(1,5), \n",
    "                                  strip_accents='unicode', \n",
    "                                  max_features=200000, \n",
    "                                  analyzer='char', \n",
    "                                  sublinear_tf=True)\n",
    "print('fitting skip 1 n-gram 2')\n",
    "# skip_vectorizer.fit(text.values)\n",
    "print('fitting char')\n",
    "char_vectorizer.fit(text_retain_punctuation.values)\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(text.values)\n",
    "\n",
    "print('transforming train skip gram')\n",
    "# train_skip = skip_vectorizer.transform(train_sentence.values)\n",
    "print('transforming train char')\n",
    "train_char = char_vectorizer.transform(train_sentence_retain_punctuation.values)\n",
    "print('transforming train phrase')\n",
    "train_phrase = phrase_vectorizer.transform(train_sentence.values)\n",
    "\n",
    "print('transforming test skip gram')\n",
    "# test_skip = skip_vectorizer.transform(test_sentence.values)\n",
    "print('transforming test char')\n",
    "test_char = char_vectorizer.transform(test_sentence_retain_punctuation.values)\n",
    "print('transforming test phrase')\n",
    "test_phrase = phrase_vectorizer.transform(test_sentence.values)\n",
    "\n",
    "# train_tfidf = hstack((train_skip, train_char, train_phrase), format='csr')\n",
    "# test_tfidf = hstack((test_skip, test_char, test_phrase), format='csr')\n",
    "\n",
    "train_tfidf = hstack((train_char, train_phrase), format='csr')\n",
    "test_tfidf = hstack((test_char, test_phrase), format='csr')\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(y_i, y, train_features):\n",
    "    p = train_features[y==y_i].sum(0)\n",
    "    return (p + 1) / ((y == y_i).sum() + 1)\n",
    "\n",
    "def get_nblogreg_model(label_cols, train_features, train, test_features, cc):\n",
    "    preds = np.zeros((test.shape[0], len(label_cols)))\n",
    "    train_preds = np.zeros((train.shape[0], len(label_cols)))\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fit', j)\n",
    "        y = train[j].values\n",
    "        r = np.log(pr(1, y, train_features) / pr(0, y, train_features))\n",
    "        model = LogisticRegression(C=cc, max_iter = 300, n_jobs=10)\n",
    "        x_nb = train_features.multiply(r).tocsr()\n",
    "        model.fit(x_nb, y)\n",
    "        preds[:, i] = model.predict_proba(test_features.multiply(r))[:, 1]\n",
    "        train_preds[:, i] = model.predict_proba(x_nb)[:, 1]\n",
    "        print('accuracy is {}'.format(roc_auc_score(y, train_preds[:, i])))\n",
    "    return preds, train_preds\n",
    "\n",
    "def save(model_name, y_test, label_cols, path, is_train=False):\n",
    "    if is_train:\n",
    "        submission = pd.read_csv(path + 'sample_train.csv')\n",
    "        file_name = 'train_' + model_name\n",
    "    else:\n",
    "        submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "        file_name = model_name\n",
    "    submission[label_cols] = y_test\n",
    "    submission.to_csv(path + file_name + '.csv', index=False)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "path_save = PATH = '../data/'\n",
    "for cc in [ 0.25, 0.23, 0.21, 0.18, 0.17, 0.15, 0.13 ]:\n",
    "    print('predicting C %s' % cc)\n",
    "    y_test, y_train = get_nblogreg_model(label_cols, train_tfidf, train, test_tfidf, cc)\n",
    "    print('total score is {}'.format(roc_auc_score(train[label_cols], y_train)))\n",
    "    ########################################\n",
    "    print('saving files')\n",
    "    model_name = 'nblogreg_ori_trainOntrain_char_punctuation_w100k_c200k_c_{}'.format(cc)\n",
    "    print(model_name)\n",
    "    save(model_name, y_test, label_cols, PATH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
