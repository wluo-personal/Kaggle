{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.calibration import CalibratedClassifierCV \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from scipy.sparse import csr_matrix, hstack \n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 30)\n",
      "(153164, 24)\n",
      "fitting char\n",
      "fitting phrase\n",
      "transforming train skip gram\n",
      "transforming train char\n",
      "transforming train phrase\n",
      "transforming test char\n",
      "transforming test phrase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = '../data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "\n",
    "train_sentence = train['comment_text_cleaned_polarity']\n",
    "test_sentence = test['comment_text_cleaned_polarity']\n",
    "\n",
    "\n",
    "train_sentence_retain_punctuation = train['comment_text_cleaned_retain_punctuation']\n",
    "test_sentence_retain_punctuation = test['comment_text_cleaned_retain_punctuation']\n",
    "\n",
    "text = train_sentence\n",
    "\n",
    "text_retain_punctuation = train_sentence_retain_punctuation\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "###########################\n",
    "\n",
    "\n",
    "phrase_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                    strip_accents='unicode', \n",
    "                                    max_features=100000, \n",
    "                                    analyzer='word',\n",
    "                                    sublinear_tf=True,\n",
    "                                    token_pattern=r'\\w{1,}')\n",
    "char_vectorizer = TfidfVectorizer(ngram_range=(2,5), \n",
    "                                  strip_accents='unicode', \n",
    "                                  max_features=200000, \n",
    "                                  analyzer='char', \n",
    "                                  sublinear_tf=True)\n",
    "\n",
    "print('fitting char')\n",
    "char_vectorizer.fit(text_retain_punctuation.values)\n",
    "print('fitting phrase')\n",
    "phrase_vectorizer.fit(text.values)\n",
    "\n",
    "print('transforming train skip gram')\n",
    "\n",
    "print('transforming train char')\n",
    "train_char = char_vectorizer.transform(train_sentence_retain_punctuation.values)\n",
    "print('transforming train phrase')\n",
    "train_phrase = phrase_vectorizer.transform(train_sentence.values)\n",
    "\n",
    "\n",
    "print('transforming test char')\n",
    "test_char = char_vectorizer.transform(test_sentence_retain_punctuation.values)\n",
    "print('transforming test phrase')\n",
    "test_phrase = phrase_vectorizer.transform(test_sentence.values)\n",
    "\n",
    "\n",
    "train_tfidf = hstack((train_char, train_phrase), format='csr')\n",
    "test_tfidf = hstack((test_char, test_phrase), format='csr')\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#######################\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train, x_val, y_train_df, y_val_df = train_test_split(train_tfidf, train, test_size=0.33)\n",
    "# # Split the dataset\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "split_index = round(len(train) * 0.9) #################################\n",
    "# shuffled_train = train#.sample(frac=1)\n",
    "x_train = train_tfidf[:split_index]\n",
    "y_train_df = train.iloc[:split_index]\n",
    "#######\n",
    "x_val = train_tfidf[split_index:]\n",
    "y_val_df = train.iloc[split_index:]\n",
    "# Get test data ready\n",
    "x_test = test_tfidf\n",
    "\n",
    "\n",
    "# train toxic\n",
    "def pr(y_i, y, train_features):\n",
    "    p = train_features[y==y_i].sum(0)\n",
    "    return (p + 1) / ((y == y_i).sum() + 1)\n",
    "r_dict = {label: np.log(pr(1, y_train_df[label].values, x_train) / pr(0,  y_train_df[label].values, x_train)) for label in label_cols}\n",
    "\n",
    "\n",
    "train_set = {label: x_train.multiply(r_dict[label]).tocsr() for label in r_dict }\n",
    "val_set = {label: x_val.multiply(r_dict[label]).tocsr() for label in r_dict }\n",
    "test_set = {label: x_test.multiply(r_dict[label]).tocsr() for label in r_dict }\n",
    "\n",
    "# del r_dict, x_train, x_val\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseLayerEstimator(ABC):\n",
    "    \n",
    "    def _pr(self, y_i, y, train_features):\n",
    "        p = train_features[np.array(y==y_i)].sum(0)\n",
    "        return (p + 1) / (np.array(y == y_i).sum() + 1)\n",
    "    \n",
    "    def _nb(self, x_train, y_train):\n",
    "        assert isinstance(y_train, pd.DataFrame)\n",
    "        r = {}\n",
    "        for col in y_train.columns:\n",
    "            print('calculating naive bayes for {}'.format(col))\n",
    "            r[col] = np.log(self._pr(1, y_train[col].values, x_train) / self._pr(0, y_train[col], x_train))\n",
    "        return r\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            x_train: np array\n",
    "            y_train: pd series\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, x_train):\n",
    "        pass\n",
    "    \n",
    "class LightgbmBLE(BaseLayerEstimator):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        #self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('num_iterations', 100)\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = lgb.Dataset(x_train, label=y_train)\n",
    "        self.gbdt = lgb.train(self.param, dtrain, self.nrounds, verbose_eval=10)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LightgbmBLE(BaseLayerEstimator):\n",
    "    def __init__(self, x_train, y_train, params=None, nb=True, seed=0):\n",
    "        \"\"\"\n",
    "        constructor:\n",
    "\n",
    "            x_train: should be a np/scipy/ 2-d array or matrix. only be used when nb is true\n",
    "            y_train: should be a dataframe\n",
    "        \"\"\"\n",
    "        #### check naive bayes\n",
    "        if nb:\n",
    "            print('Naive Bayes is enabled')\n",
    "            self.r = self._nb(x_train, y_train)\n",
    "        else:\n",
    "            print('Naive Bayes is disabled')\n",
    "            self.r = None\n",
    "        ##### set values    \n",
    "        self.nb = nb\n",
    "        self.set_params(params)\n",
    "        print('LightgbmBLE is initialized')\n",
    "    \n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _pre_process(self, x_train, y_train, label=None):\n",
    "        if self.nb:\n",
    "            assert label is not None\n",
    "            print('apply naive bayes to feature set')\n",
    "            x = x_train.multiply(self.r[label])\n",
    "            if isinstance(x_train, csr_matrix):\n",
    "                x = x.tocsr()\n",
    "        else:\n",
    "            x = x_train\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y = y_train.values\n",
    "        else:\n",
    "            y = y_train\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    def train(self, x_train, y_train, label=None):\n",
    "        x, y = self._pre_process(x_train, y_train, label)\n",
    "        lgb_train = lgb.Dataset(x, y)\n",
    "        lgb_eval = lgb.Dataset(x, y, reference=lgb_train)\n",
    "        self.model = lgb.train(self.params, lgb_train, valid_sets=lgb_eval, verbose_eval=20)\n",
    "        \n",
    "        \n",
    "    def predict(self, x_train, label=None):\n",
    "        x, _ = self._pre_process(x_train, y_train=None, label=label)\n",
    "        print('starting predicting')\n",
    "        result = self.model.predict(x)\n",
    "        print('predicting done')\n",
    "        return result\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes is enabled\n",
      "calculating naive bayes for toxic\n",
      "calculating naive bayes for severe_toxic\n",
      "calculating naive bayes for obscene\n",
      "calculating naive bayes for threat\n",
      "calculating naive bayes for insult\n",
      "calculating naive bayes for identity_hate\n",
      "LightgbmBLE is initialized\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'application': 'binary',\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': -1,\n",
    "    'metric': 'auc',\n",
    "    'data_random_seed': 2,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.6,\n",
    "    'nthread': 4,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1\n",
    "} \n",
    "ll = LightgbmBLE(train_tfidf, train[label_cols], params=params, nb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "apply naive bayes to feature set\n",
      "[20]\tvalid_0's auc: 0.961558\n",
      "[40]\tvalid_0's auc: 0.980041\n",
      "[60]\tvalid_0's auc: 0.986581\n",
      "[80]\tvalid_0's auc: 0.990192\n",
      "[100]\tvalid_0's auc: 0.992662\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "severe_toxic\n",
      "apply naive bayes to feature set\n",
      "[20]\tvalid_0's auc: 0.990618\n",
      "[40]\tvalid_0's auc: 0.997012\n",
      "[60]\tvalid_0's auc: 0.998851\n",
      "[80]\tvalid_0's auc: 0.999541\n",
      "[100]\tvalid_0's auc: 0.999816\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "obscene\n",
      "apply naive bayes to feature set\n",
      "[20]\tvalid_0's auc: 0.990877\n",
      "[40]\tvalid_0's auc: 0.995789\n",
      "[60]\tvalid_0's auc: 0.997536\n",
      "[80]\tvalid_0's auc: 0.998457\n",
      "[100]\tvalid_0's auc: 0.999016\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "threat\n",
      "apply naive bayes to feature set\n",
      "[20]\tvalid_0's auc: 0.990854\n",
      "[40]\tvalid_0's auc: 0.999744\n",
      "[60]\tvalid_0's auc: 0.999992\n",
      "[80]\tvalid_0's auc: 0.999999\n",
      "[100]\tvalid_0's auc: 0.999999\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "insult\n",
      "apply naive bayes to feature set\n",
      "[20]\tvalid_0's auc: 0.977008\n",
      "[40]\tvalid_0's auc: 0.988609\n",
      "[60]\tvalid_0's auc: 0.99258\n",
      "[80]\tvalid_0's auc: 0.994829\n",
      "[100]\tvalid_0's auc: 0.996169\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n",
      "identity_hate\n",
      "apply naive bayes to feature set\n",
      "[20]\tvalid_0's auc: 0.979272\n",
      "[40]\tvalid_0's auc: 0.996621\n",
      "[60]\tvalid_0's auc: 0.999009\n",
      "[80]\tvalid_0's auc: 0.999696\n",
      "[100]\tvalid_0's auc: 0.999897\n",
      "apply naive bayes to feature set\n",
      "starting predicting\n",
      "predicting done\n"
     ]
    }
   ],
   "source": [
    "ll = LightgbmBLE(train_tfidf, train[label_cols], params=params, nb=True)\n",
    "result = pd.DataFrame()\n",
    "for col in label_cols:\n",
    "    print(col)\n",
    "    ll.train(train_tfidf, train[col], col)\n",
    "    result[col] = ll.predict(test_tfidf, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv(PATH + 'lightgbmtest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
