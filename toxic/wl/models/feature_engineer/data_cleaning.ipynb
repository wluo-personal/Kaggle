{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### add polarity\n",
    "### retain punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set done\n"
     ]
    }
   ],
   "source": [
    "# run by python 3.5 kernel\n",
    "\n",
    "train = pd.read_csv('/home/kai/data/kaggle/toxic/dataset/training/emoji_train.csv')\n",
    "test = pd.read_csv('/home/kai/data/kaggle/toxic/dataset/training/emoji_test.csv')\n",
    "\n",
    "train['comment_text'].fillna('na', inplace=True)\n",
    "test['comment_text'].fillna('na', inplace=True)\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "# x = my_series.apply(my_function, args = (arg1,))\n",
    "pol = pd.DataFrame()\n",
    "pol_test = pd.DataFrame()\n",
    "def add_polarity_phrase(x, col):\n",
    "    score = int (TextBlob(x).sentiment.polarity * 20) \n",
    "    if score > 0:\n",
    "        senti = 'positive'\n",
    "    elif score < 0:\n",
    "        senti = 'negative'\n",
    "        score = score * (-1)\n",
    "    else:\n",
    "        senti = 'neutral'\n",
    "    return ' {}_{}_{} '.format(col,senti,score)\n",
    "\n",
    "pol['cleaned'] = train['comment_text_cleaned'].apply(add_polarity_phrase, args=('cleaned',))\n",
    "pol['original'] = train['comment_text'].apply(add_polarity_phrase, args=('original',))\n",
    "\n",
    "print('train set done')\n",
    "\n",
    "pol_test['cleaned'] = test['comment_text_cleaned'].apply(add_polarity_phrase, args=('cleaned',))\n",
    "pol_test['original'] = test['comment_text'].apply(add_polarity_phrase, args=('original',))\n",
    "\n",
    "train['comment_text_cleaned_polarity'] = train['comment_text_cleaned'] + pol['cleaned'] + pol['original']\n",
    "test['comment_text_cleaned_polarity'] = test['comment_text_cleaned'] + pol_test['cleaned'] + pol_test['original']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = '../data/'\n",
    "train.to_csv(PATH + 'cleaned_train_emoji.csv', index=False)\n",
    "test.to_csv(PATH + 'cleaned_test_emoji.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment text cleaning\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import nltk \n",
    "import nltk.tokenize.toktok as tok\n",
    "\n",
    "\n",
    "PATH = '../data/'\n",
    "train = pd.read_csv(PATH + 'cleaned_train_emoji.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test_emoji.csv')\n",
    "APO = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"i would\",\n",
    "    \"i'd\" : \"i had\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"i'm\" : \"i am\",\n",
    "    \"im\" : \"i am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"i have\",\n",
    "    \"ive\" : \"i have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"u\" : \"you\",\n",
    "    \"r\" : \"are\",\n",
    "    \"ur\" : \"you are\",\n",
    "    \"fuckin\" : \"fucking\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean(comment):\n",
    "    comment = comment.lower()\n",
    "#     comment = re.sub(r'\\n+', ' ', comment)\n",
    "    comment = re.sub('\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', '',comment) # remove leaky elements like ip,user\n",
    "    comment = re.sub('\\[\\[.*\\]', '',comment)    #removing usernames\n",
    "    comment = re.sub('[=\",~]', '', comment)\n",
    "    text = re.sub('  +', ' ', comment)\n",
    "#     comment = re.sub('-', ' ', comment)\n",
    "#     text = tok.tokenize(comment)\n",
    "    text = text.split(' ')\n",
    "    text = [word for word in  text if not re.match(r'http:\\/\\/.*', word)]\n",
    "    text = [APO[word] if word in APO else word for word in text]\n",
    "#     text = [lem.lemmatize(word, 'v') for word in text]\n",
    "#     text = [lem.lemmatize(word, 'n') for word in text]\n",
    "    text = ' '.join(text).lower()\n",
    "#     text = re.sub(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])',' ', text)\n",
    "    if text == '': text = 'na'\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "print('comment text cleaning')\n",
    "train['comment_text_cleaned_retain_punctuation'] = train['comment_text'].apply(clean)\n",
    "test['comment_text_cleaned_retain_punctuation'] = test['comment_text'].apply(clean)\n",
    "\n",
    "train.to_csv(PATH + 'cleaned_train_emoji.csv', index=False)\n",
    "test.to_csv(PATH + 'cleaned_test_emoji.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation\\nwhy the edits made under my usern...\n",
       "1         d'aww! he matches this background colour i am ...\n",
       "2         hey man i am really not trying to edit war. it...\n",
       "3         \\nmore\\ni cannot make any real suggestions on ...\n",
       "4         you sir are my hero. any chance you remember w...\n",
       "5         \\n\\ncongratulations from me as well use the to...\n",
       "6              cocksucker before you piss around on my work\n",
       "7         your vandalism to the matt shirvington article...\n",
       "8         sorry if the word 'nonsense' was offensive to ...\n",
       "9         alignment on this subject and which are contra...\n",
       "10        \\nfair use rationale for image:wonju.jpg\\n\\nth...\n",
       "11        bbq \\n\\nbe a man and lets discuss it-maybe ove...\n",
       "12        hey... what is it..\\n@ | talk .\\nwhat is it......\n",
       "13        before you start throwing accusations and warn...\n",
       "14        oh and the girl above started her arguments wi...\n",
       "15        \\n\\njuelz santanas age\\n\\nin 2002 juelz santan...\n",
       "16        bye! \\n\\ndon't look come or think of comming b...\n",
       "17         redirect talk:voydan pop georgiev- chernodrinski\n",
       "18        the mitsurugi point made no sense - why not ar...\n",
       "19        do not mean to bother you \\n\\ni see that you a...\n",
       "20        \\n\\n regarding your recent edits \\n\\nonce agai...\n",
       "21        \\ngood to know. about me yeah i am studying no...\n",
       "22        \\n\\n snowflakes are not always symmetrical! \\n...\n",
       "23        \\n\\n the signpost: 24 september 2012 \\n\\n read...\n",
       "24        \\n\\nre-considering 1st paragraph edit?\\ni do n...\n",
       "25        radial symmetry \\n\\nseveral now extinct lineag...\n",
       "26        there is no need to apologize. a wikipedia art...\n",
       "27        yes because the mother of the child in the cas...\n",
       "28        \\nok. but it will take a bit of work but i can...\n",
       "29         a barnstar for you! \\n\\n the real life barnst...\n",
       "                                ...                        \n",
       "159541    your absurd edits \\n\\nyour absurd edits on gre...\n",
       "159542    maybe he is got better things to do than spend...\n",
       "159543    scrap that it does meet criteria and its gone ...\n",
       "159544                                  you could do worse.\n",
       "159545     7 march 2011 (utc)\\nare you also user:bmattso...\n",
       "159546    \\n\\nhey listen do not you ever!!!! delete my e...\n",
       "159547                         thank you very very much. ·✆\n",
       "159548                          talkback: 15 september 2012\n",
       "159549                            2005 (utc)\\n 06:35 31 mar\n",
       "159550    i agree/ on another note lil wayne is a talent...\n",
       "159551    while about half the references are from byu-i...\n",
       "159552    prague spring \\n\\ni think that prague spring d...\n",
       "159553    i see this as having been merged; undoing one ...\n",
       "159554    and i am going to keep posting the stuff you d...\n",
       "159555    \\n\\nhow come when you download that mp3 it is ...\n",
       "159556    i will be on irc too if you have a more specif...\n",
       "159557    it is my opinion that that happens to be off-t...\n",
       "159558    please stop removing content from wikipedia; i...\n",
       "159559    image:barack-obama-mother.jpg listed for delet...\n",
       "159560    editing of article without consensus & removal...\n",
       "159561    \\nno he did not read it again (i would have th...\n",
       "159562    \\n auto guides and the motoring press are not ...\n",
       "159563    \\nplease identify what part of blp applies bec...\n",
       "159564    catalan independentism is the social movement ...\n",
       "159565    the numbers in parentheses are the additional ...\n",
       "159566    :::::and for the second time of asking when yo...\n",
       "159567    you should be ashamed of yourself \\n\\nthat is ...\n",
       "159568    spitzer \\n\\numm theres no actual article for p...\n",
       "159569    and it looks like it was actually you who put ...\n",
       "159570    \\nand ... i really do not think you understand...\n",
       "Name: comment_text_cleaned_retain_punctuation, Length: 159571, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text_cleaned_retain_punctuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
