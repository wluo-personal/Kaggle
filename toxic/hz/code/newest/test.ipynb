{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7)\n",
      "(3, 7)\n",
      "1.33333333333\n",
      "0.516397779494\n"
     ]
    }
   ],
   "source": [
    "PATH = '../../data/'\n",
    "stopword = set(stopwords.words(\"english\"))\n",
    "tok = TweetTokenizer()\n",
    "\n",
    "train = pd.read_csv('a.csv')\n",
    "test = pd.read_csv('a.csv')\n",
    "\n",
    "train_sentence = train['A']\n",
    "test_sentence = test['A']\n",
    "\n",
    "def clean(comment):\n",
    "    text = tok.tokenize(comment)\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return ' '.join(text)\n",
    "\n",
    "text_length = pd.concat([train_sentence.apply(lambda x: len(x.split())),\\\n",
    "                         test_sentence.apply(lambda x: len(x.split()))])\n",
    "\n",
    "mean_length = text_length.mean()\n",
    "std_length = text_length.std()\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(mean_length)\n",
    "print(std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "MAX_FEATURES = 100000 # max num of words\n",
    "MAX_LEN = np.round(mean_length + 3*std_length).astype(int) # max sequence length\n",
    "EMBED_SIZE = 6 # embedding size\n",
    "LSTM_UNITS = 50 # LSTM hidden layer unit number\n",
    "DENSE_UNITS = 50\n",
    "DROPOUT = 0.3 # dropout rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "EMBEDDING_FILE = 'glove.6B.50d.txt'\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(pd.concat([train_sentence, test_sentence]).values)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentence.values)\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_sentence.values)\n",
    "\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=MAX_LEN)\n",
    "y = train[label_cols].values\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=MAX_LEN)\n",
    "\n",
    "print(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), (nb_words+1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting model\n",
      "[[0 2 1]\n",
      " [0 0 1]\n",
      " [0 0 1]] [[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 2s 582ms/step - loss: 0.1170 - acc: 1.0000\n",
      "[[0 2 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "[[[  7.44309902e-01   1.32505846e+00   2.24079713e-01   3.40721667e-01\n",
      "     5.98348677e-01   1.12650180e+00]\n",
      "  [  9.99879907e-04   1.00099993e+00   9.99000013e-01   9.99000013e-01\n",
      "     9.99000013e-01   9.99000013e-01]\n",
      "  [  9.99990269e-04   9.99990734e-04  -9.99995391e-04   9.99975484e-04\n",
      "    -9.99986776e-04  -9.99992248e-04]]\n",
      "\n",
      " [[  7.44309902e-01   1.32505846e+00   2.24079713e-01   3.40721667e-01\n",
      "     5.98348677e-01   1.12650180e+00]\n",
      "  [  7.44309902e-01   1.32505846e+00   2.24079713e-01   3.40721667e-01\n",
      "     5.98348677e-01   1.12650180e+00]\n",
      "  [  9.99990269e-04   9.99990734e-04  -9.99995391e-04   9.99975484e-04\n",
      "    -9.99986776e-04  -9.99992248e-04]]\n",
      "\n",
      " [[  7.44309902e-01   1.32505846e+00   2.24079713e-01   3.40721667e-01\n",
      "     5.98348677e-01   1.12650180e+00]\n",
      "  [  7.44309902e-01   1.32505846e+00   2.24079713e-01   3.40721667e-01\n",
      "     5.98348677e-01   1.12650180e+00]\n",
      "  [  9.99990269e-04   9.99990734e-04  -9.99995391e-04   9.99975484e-04\n",
      "    -9.99986776e-04  -9.99992248e-04]]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "file_path = '../../model/lstm_best.hdf5'\n",
    "sample_submission_file_path = PATH + 'sample_submission.csv'\n",
    "\n",
    "print('getting model')\n",
    "embedding_matrix = get_embedding_matrix(EMBEDDING_FILE, EMBED_SIZE, MAX_FEATURES, tokenizer)\n",
    "input = Input(shape=(MAX_LEN, ))\n",
    "x = Embedding(3, 6, weights=[embedding_matrix], trainable=False, name='ss')(input)\n",
    "x = Bidirectional(LSTM(2, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(6)(x)\n",
    "model = Model(inputs=input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(X_train, y)\n",
    "model.fit(X_train, y)\n",
    "\n",
    "dense1_layer_model = Model(inputs=model.input, outputs=model.get_layer('ss').output)  \n",
    "dense1_output = dense1_layer_model.predict(X_train)\n",
    "\n",
    "print(X_train)\n",
    "print(dense1_output)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  1.],\n",
       "       [ 0.,  0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "kvar = K.variable(X_train, dtype='float32')\n",
    "K.eval(kvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'fucking']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "b = TextBlob('i am fuckking')\n",
    "print(str(b.correct()).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "class f:\n",
    "    a = 1\n",
    "    b = 2\n",
    "    \n",
    "print(f.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c\n",
      "0  1  2  3\n",
      "1  4  5  6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "y = pd.DataFrame(x, columns=['a','b','c'])\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
