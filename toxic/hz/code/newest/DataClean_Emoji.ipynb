{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "PATH = '../../data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "test = pd.read_csv(PATH + 'test.csv')\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "APO = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"i would\",\n",
    "    \"i'd\" : \"i had\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"i'm\" : \"i am\",\n",
    "    \"im\" : \"i am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"i have\",\n",
    "    \"ive\" : \"i have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"tryin'\": \"trying\",\n",
    "    \"u\" : \"you\",\n",
    "    \"r\" : \"are\",\n",
    "    \"ur\" : \"you are\",\n",
    "    \"fuckin\" : \"fucking\",\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inplace na\n",
      "comment text cleaning\n",
      "['id' 'comment_text' 'toxic' 'severe_toxic' 'obscene' 'threat' 'insult'\n",
      " 'identity_hate' 'comment_text_cleaned']\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "tok = TweetTokenizer()\n",
    "\n",
    "def clean(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(r'\\n+', ' ', comment)\n",
    "    comment = re.sub('\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', '',comment) # remove leaky elements like ip,user\n",
    "    comment = re.sub('\\[\\[.*\\]', '',comment)    #removing usernames\n",
    "    text = tok.tokenize(comment)\n",
    "    text = [word for word in text if not re.match(r'http:\\/\\/.*', word)]\n",
    "    text = [APO[word] if word in APO else word for word in text]\n",
    "    text = tok.tokenize(' '.join(text))\n",
    "    text = [lem.lemmatize(word, 'v') for word in text]\n",
    "    text = [lem.lemmatize(word, 'n') for word in text]\n",
    "    text = ' '.join(text).lower()\n",
    "    text = re.sub('[=\",~]', '', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub(r'\\/', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    if text == '': text = 'na'\n",
    "    return text\n",
    "\n",
    "def check(comment):\n",
    "    b = TextBlob(comment)\n",
    "    return str(b.correct())\n",
    "\n",
    "# word count\n",
    "def word_count(comment): return len(comment.split())\n",
    "# unique word count\n",
    "def unique_word_count(comment): return len(set(comment.split()))\n",
    "# find the count of quesiton marks\n",
    "def question_mark_count(comment): return len(re.findall(r'\\?', comment))\n",
    "# find the count of consecutive question marks (i.e. ??)\n",
    "def multi_question_mark_count(comment): return len(re.findall(r'\\?{2,}', comment))\n",
    "# find the count of exclamation marks\n",
    "def exclamation_mark_count(comment): return len(re.findall(r'!', comment))\n",
    "# find the count of consecutive exclamation marks (i.e. !!)\n",
    "def multi_exclamation_mark_count(comment): return len(re.findall(r'!{2,}', comment))\n",
    "# find the count of uppercase letters\n",
    "def uppercase_letter_count(comment): return len(re.findall(r'[A-Z]', comment))\n",
    "# count ellipsis (3 or more . (i.e. ...))\n",
    "def ellipsis_count(comment): return len(re.findall(r'\\.{3,}', comment))\n",
    "# count period and ellipsis\n",
    "def period_count(comment): return len(re.findall(r'\\.+', comment))\n",
    "# count parentheses pairs\n",
    "def parentheses_pair_count(comment): return len(re.findall(r'\\(.*\\)', comment))\n",
    "# count special symbols\n",
    "def special_symbol_count(comment): return len(re.findall(r'[\\%\\#\\@\\*\\&\\$]', comment))\n",
    "# count period and change line\n",
    "def sentence_count(comment): return len(re.findall(r'[\\n+\\.+\\?+!+]', comment))\n",
    "\n",
    "print('inplace na')\n",
    "train['comment_text'].fillna('na', inplace=True)\n",
    "test['comment_text'].fillna('na', inplace=True)\n",
    "\n",
    "print('comment text cleaning')\n",
    "train['comment_text_cleaned'] = train['comment_text'].apply(clean)\n",
    "test['comment_text_cleaned'] = test['comment_text'].apply(clean)\n",
    "\n",
    "# print('correct train')\n",
    "# train['comment_text_correct'] = train['comment_text_cleaned'].apply(check)\n",
    "# print('correct test')\n",
    "# test['comment_text_correct'] = test['comment_text_cleaned'].apply(check)\n",
    "\n",
    "# print('word count')\n",
    "# train['word_count'] = train['comment_text'].apply(word_count)\n",
    "# test['word_count'] = test['comment_text'].apply(word_count)\n",
    "# train['cleaned_word_count'] = train['comment_text_cleaned'].apply(word_count)\n",
    "# test['cleaned_word_count'] = test['comment_text_cleaned'].apply(word_count)\n",
    "\n",
    "# print('unique word count')\n",
    "# train['unique_word_count'] = train['comment_text'].apply(unique_word_count)\n",
    "# test['unique_word_count'] = test['comment_text'].apply(unique_word_count)\n",
    "# train['cleaned_unique_word_count'] = train['comment_text_cleaned'].apply(unique_word_count)\n",
    "# test['cleaned_unique_word_count'] = test['comment_text_cleaned'].apply(unique_word_count)\n",
    "\n",
    "# print('question marks')\n",
    "# train['question_marks'] = train['comment_text'].apply(question_mark_count)\n",
    "# test['question_marks'] = test['comment_text'].apply(question_mark_count)\n",
    "\n",
    "# print('consecutive question marks')\n",
    "# train['consecutive_question_marks'] = train['comment_text'].apply(multi_question_mark_count)\n",
    "# test['consecutive_question_marks'] = test['comment_text'].apply(multi_question_mark_count)\n",
    "\n",
    "# print('exclamation marks')\n",
    "# train['exclamation_marks'] = train['comment_text'].apply(exclamation_mark_count)\n",
    "# test['exclamation_marks'] = test['comment_text'].apply(exclamation_mark_count)\n",
    "\n",
    "# print('consecutive exclamation marks')\n",
    "# train['consecutive_exclamation_marks'] = train['comment_text'].apply(multi_exclamation_mark_count)\n",
    "# test['consecutive_exclamation_marks'] = test['comment_text'].apply(multi_exclamation_mark_count)\n",
    "\n",
    "# print('uppercase letters')\n",
    "# train['uppercase_letters'] = train['comment_text'].apply(uppercase_letter_count)\n",
    "# test['uppercase_letters'] = test['comment_text'].apply(uppercase_letter_count)\n",
    "\n",
    "# print('ellipsis')\n",
    "# train['ellipsis'] = train['comment_text'].apply(ellipsis_count)\n",
    "# test['ellipsis'] = test['comment_text'].apply(ellipsis_count)\n",
    "\n",
    "# print('period and ellipsis')\n",
    "# train['period'] = train['comment_text'].apply(period_count)\n",
    "# test['period'] = test['comment_text'].apply(period_count)\n",
    "\n",
    "# print('parentheses pairs')\n",
    "# train['parentheses_pair'] = train['comment_text'].apply(parentheses_pair_count)\n",
    "# test['parentheses_pair'] = test['comment_text'].apply(parentheses_pair_count)\n",
    "\n",
    "# print('special symbols')\n",
    "# train['special_symbol'] = train['comment_text'].apply(special_symbol_count)\n",
    "# test['special_symbol'] = test['comment_text'].apply(special_symbol_count)\n",
    "\n",
    "# print('sentence count')\n",
    "# train['sentence'] = train['comment_text'].apply(sentence_count)\n",
    "# test['sentence'] = test['comment_text'].apply(sentence_count)\n",
    "\n",
    "# print('upper_word_ratio')\n",
    "# train['upper_word_ratio'] = train['uppercase_letters'] / (train['word_count'] + 1)\n",
    "# test['upper_word_ratio'] = test['uppercase_letters'] / (test['word_count'] + 1)\n",
    "\n",
    "# print('unique_word_ratio')\n",
    "# train['unique_word_ratio'] = train['unique_word_count'] / (train['word_count'] + 1)\n",
    "# test['unique_word_ratio'] = test['unique_word_count'] / (test['word_count'] + 1)\n",
    "\n",
    "# print('mark_count_ratio')\n",
    "# train['mark_count_ratio'] = (train['question_marks']+train['exclamation_marks']+train['special_symbol'])\\\n",
    "#                             /(train['word_count'] + 1)\n",
    "# test['mark_count_ratio'] = (test['question_marks']+test['exclamation_marks']+test['special_symbol'])\\\n",
    "#                             /(test['word_count'] + 1)\n",
    "\n",
    "print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train.to_csv(PATH + 'emoji_train.csv')\n",
    "test.to_csv(PATH + 'emoji_test.csv')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
