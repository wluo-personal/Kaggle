{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# #from nltk.tokenize import TweetTokenizer\n",
    "# from keras.layers import Dense, Embedding, Input, LSTM, GRU, Conv1D, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "# from keras.layers import SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, BatchNormalization\n",
    "# from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "HOME = '../../'\n",
    "DATA = HOME + 'data/'\n",
    "MODEL = HOME + 'model/'\n",
    "\n",
    "PATH = '~/data/toxic/data/'\n",
    "# PATH = '../../data/'\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from fastText import load_model\n",
    "# grid search params\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class grid_search_generator(object):\n",
    "    def __init__(self, config_file_url='grid_search_config.json',\n",
    "                 params=None, bindings=None, score_name_list=None, csv_url=None):\n",
    "        new_generator = params != None and bindings != None and score_name_list != None\n",
    "        if new_generator:\n",
    "            self.params = params\n",
    "            self.binding = bindings\n",
    "            self.score_name_list = score_name_list\n",
    "            \n",
    "            self.single_keys = [key for key in self.params.keys() if key not in self.binding.keys()]\n",
    "            self.binding_keys = list(self.binding.keys())\n",
    "            self._terminate = False\n",
    "            \n",
    "            self._save_config(config_file_url)\n",
    "        else:\n",
    "            with open(config_file_url, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.params = data['params']\n",
    "                self.binding = data['binding']\n",
    "                self.score_name_list = data['score_name_list']\n",
    "                self.single_keys = data['single_keys']\n",
    "                self.binding_keys = data['binding_keys']\n",
    "                \n",
    "        self.keys = list(self.single_keys)\n",
    "        self.keys.extend(self.binding_keys)\n",
    "        self.param_keys = list(self.single_keys)\n",
    "        for i in range(len(self.single_keys), len(self.keys)):\n",
    "            self.param_keys.extend(self.binding[self.keys[i]])\n",
    "        \n",
    "        self._terminate = False\n",
    "        self._idx = [0 for i in range(len(self.keys))]\n",
    "        self._ub = [len(self.params[key]) for key in self.keys]\n",
    "        \n",
    "        if csv_url != None: self.get_csv(csv_url)\n",
    "    \n",
    "    def _reset_config(self):\n",
    "        self._terminate = False\n",
    "        self._idx = [0 for i in range(len(self.keys))]\n",
    "        self._ub = [len(self.params[key]) for key in self.keys]\n",
    "    \n",
    "    def _save_config(self, config_file_url):\n",
    "        with open(config_file_url, 'w') as f:\n",
    "            data = {\n",
    "                'params': self.params,\n",
    "                'binding': self.binding,\n",
    "                'score_name_list': self.score_name_list,\n",
    "                'single_keys': self.single_keys,\n",
    "                'binding_keys': self.binding_keys\n",
    "            }\n",
    "            json.dump(data, f)\n",
    "    \n",
    "    def _next_idx(self):\n",
    "        i = 0\n",
    "        self._idx[i] = (self._idx[i] + 1) % self._ub[i]\n",
    "        i += 1\n",
    "        end_loop = (self._idx[i-1]!=0 or i==len(self._idx))\n",
    "        terminate = (self._idx[i-1]==0 and i==len(self._idx))\n",
    "        while(not end_loop):\n",
    "            self._idx[i] = (self._idx[i] + 1) % self._ub[i]\n",
    "            i += 1\n",
    "            end_loop = (self._idx[i-1]!=0 or i==len(self._idx))\n",
    "            terminate = (self._idx[i-1]==0 and i==len(self._idx))\n",
    "        self._terminate = terminate\n",
    "\n",
    "    def _next_param_list(self):\n",
    "        if not self._terminate:\n",
    "            input = [self.params[self.keys[i]][self._idx[i]] for i in range(len(self.single_keys))]\n",
    "            for i in range(len(self.single_keys), len(self.keys)):\n",
    "                input.extend([self.params[self.keys[i]][self._idx[i]][key] for key in self.binding[self.keys[i]]])\n",
    "            self._next_idx()\n",
    "            return input\n",
    "        else: return None\n",
    "        \n",
    "    def _get_param_frame(self):\n",
    "        param_list = []\n",
    "        self._reset_config()\n",
    "        param = self._next_param_list()\n",
    "        while param != None:\n",
    "            param_list.append(param)\n",
    "            param = self._next_param_list()\n",
    "        return pd.DataFrame(param_list, columns=self.param_keys)\n",
    "        \n",
    "    def get_csv(self, csv_url):\n",
    "        self._get_param_frame().to_csv(csv_url, index=False)\n",
    "        print('successfully generated grid search csv file\\n')\n",
    "    \n",
    "    def _get_grid_search_param(keys, values): return dict(zip(keys, values))\n",
    "    \n",
    "    def _get_next_param(csv_url, param_list):\n",
    "        df = shuffle(pd.read_csv(csv_url))\n",
    "        if df.empty: return None\n",
    "        param = grid_search_generator._get_grid_search_param(param_list, df.iloc[0].values)\n",
    "        df.iloc[1 : ].to_csv(csv_url, index=False)\n",
    "        return param\n",
    "    \n",
    "    def _recorder(values, record_csv, score_name_list, params, param_keys, label_list, confusion_matrices, initialize):\n",
    "        head = list(param_keys)\n",
    "        head.extend(score_name_list)\n",
    "        if label_list != None: head.extend(label_list)\n",
    "        \n",
    "        content = []\n",
    "        for x in param_keys: content.append(params[x])\n",
    "        content.extend(values)\n",
    "        if label_list != None:\n",
    "            for x in label_list: content.append(confusion_matrices[x])\n",
    "        if initialize:\n",
    "            with open(record_csv, 'w') as f: pd.DataFrame([content], columns=head).to_csv(f, header=True, index=False)\n",
    "        else:\n",
    "            with open(record_csv, 'a') as f: pd.DataFrame([content], columns=head).to_csv(f, header=False, index=False)\n",
    "\n",
    "        print_str = ''\n",
    "        for i, j in enumerate(values): print_str += (score_name_list[i] + ':' + str(j) + ', ')\n",
    "        print_str = print_str[ : -2] + '\\n'\n",
    "        print(print_str)\n",
    "    \n",
    "    def add_params(csv_url, param_dict, config_file_url='grid_search_config.json'):\n",
    "        for key in param_dict.keys():\n",
    "            grid_search = grid_search_generator(config_file_url=config_file_url)\n",
    "            original = list(grid_search.params[key])\n",
    "            param_values = [value for value in param_dict[key] if value not in original]\n",
    "            grid_search.params[key] = param_values\n",
    "            with open(csv_url, 'a') as f: grid_search._get_param_frame().to_csv(f, header=False, index=False)\n",
    "            grid_search.params[key].extend(original)\n",
    "            grid_search._save_config(config_file_url)\n",
    "        \n",
    "        print('successfully append the new parameters')\n",
    "        \n",
    "    def delete_params(csv_url, param_dict, config_file_url='grid_search_config.json'):\n",
    "        csv = pd.read_csv(csv_url)\n",
    "        grid_search = grid_search_generator(config_file_url)\n",
    "        \n",
    "        for key in param_dict.keys():\n",
    "            for value in param_dict[key]:\n",
    "                csv = csv[csv[key] != value]\n",
    "        with open(csv_url, 'w') as f: csv.to_csv(f, index=False)\n",
    "        \n",
    "        for key in param_dict.keys():\n",
    "            if key in grid_search.single_keys:\n",
    "                grid_search.params[key] = [value for value in grid_search.params[key] if value not in param_dict[key]]\n",
    "            else:\n",
    "                for bkey in grid_search.binding_keys:\n",
    "                    if key in grid_search.binding[bkey]:\n",
    "                        grid_search.params[bkey] = [value for value in grid_search.params[bkey] if value[key] not in param_dict[key]]\n",
    "                        break\n",
    "        grid_search._save_config(config_file_url)\n",
    "        print('successfully delete the parameters')\n",
    "    \n",
    "    def search(remain_csv, record_csv, model_run, other_model_dependency_dict, label_list=None,\n",
    "               config_file_url='grid_search_config.json', X_y_split=True, val_size=0.1, shuffle=True):\n",
    "        grid_search = grid_search_generator(config_file_url)\n",
    "        param = grid_search_generator._get_next_param(remain_csv, grid_search.param_keys)\n",
    "        header = True\n",
    "        \n",
    "        train_total = pd.read_csv(PATH+'cleaned_train.csv')\n",
    "        test = pd.read_csv(PATH+'cleaned_test.csv')\n",
    "        if X_y_split:\n",
    "            train, val = train_test_split(train_total, test_size=val_size, shuffle=shuffle)\n",
    "            X_train = train[param['train']]\n",
    "            y_train = train[label_cols]\n",
    "            X_val = val[param['train']]\n",
    "            y_val = val[label_cols]\n",
    "\n",
    "        while param != None:\n",
    "            print(param)\n",
    "            if X_y_split:\n",
    "                scores, confusion_matrices = model_run(param=param, X=X_train, X_val=X_val, y=y_train, y_val=y_val,\n",
    "                                                       other_model_dependency_dict=other_model_dependency_dict,\n",
    "                                                       test=test[param['train']])\n",
    "            else:\n",
    "                scores, confusion_matrices = model_run(param=param, X=train_total[param['train']],\n",
    "                                                       y=train_total[label_cols], test=test[param['train']],\n",
    "                                                       other_model_dependency_dict=other_model_dependency_dict)\n",
    "            grid_search_generator._recorder(scores, record_csv, grid_search.score_name_list, param,\n",
    "                                            grid_search.param_keys, label_list, confusion_matrices, header)\n",
    "            header = False\n",
    "            param = grid_search_generator._get_next_param(remain_csv, grid_search.param_keys)\n",
    "            \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): \n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    return word, np.asarray(arr[-300:], dtype='float32')\n",
    "\n",
    "def glove_get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file, encoding='utf8'))\n",
    "    embeddings_index.pop('2000000', None)  # only needed when using lexvec\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "def fasttext_get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    ft_model = load_model(embedding_file)\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = ft_model.get_word_vector(word).astype('float32')\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "def get_embedding_matrix(embed_type, file, size, max_features, tokenizer):\n",
    "    if embed_type == 'fasttext': return fasttext_get_embedding_matrix(file, size, max_features, tokenizer)\n",
    "    else: return glove_get_embedding_matrix(file, size, max_features, tokenizer)\n",
    "\n",
    "def get_rnn_model(params, tokenizer):\n",
    "    embed_type = params['embed_type']\n",
    "    embed_file = params['embed_file']\n",
    "    embed_size = params['embed_size']\n",
    "    embed_trainable = params['embed_trainable']\n",
    "    recurrent_dropout = params['recurrent_dropout']\n",
    "    max_len = params['max_len']\n",
    "    cnn = params['cnn']\n",
    "    dense = params['dense']\n",
    "    \n",
    "    loss = params['loss']\n",
    "    label_len = params['label_len']\n",
    "    max_features = params['max_features']\n",
    "    gru_lstm = params['gru_lstm']\n",
    "    lstm_units = int(params['lstm_units'])\n",
    "    \n",
    "    sequence_input = Input(shape=(max_len, ))\n",
    "    embedding_matrix, inp_len = get_embedding_matrix(embed_type, embed_file, embed_size, max_features, tokenizer)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    x = Embedding(inp_len, embed_size, weights=[embedding_matrix], trainable=embed_trainable)(sequence_input)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    if gru_lstm == 'lstm':\n",
    "        x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    else:\n",
    "        x = Bidirectional(GRU(lstm_units, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "        #x = Bidirectional(GRU(lstm_units, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    if cnn:\n",
    "        x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    \n",
    "    if dense:\n",
    "        x = Dense(100, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    preds = Dense(label_len, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss=loss,optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, params, test):\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    patience = params['patience']\n",
    "    time_stamp = str(int(time()))\n",
    "    model_file = '../../model/' + time_stamp + '.hdf5'\n",
    "    pred_file = '../../model/' + time_stamp + '.csv'\n",
    "    print(model_file)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "                                #(model_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "                                  #(monitor=\"val_acc\", mode=\"max\", patience=3)\n",
    "    ra_val = RocAucEvaluation(validation_data=(x_val, y_val), interval = 1)\n",
    "    callbacks_list = [checkpoint, earlystopping, ra_val]\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\\\n",
    "                        validation_data=(x_val,y_val), callbacks=callbacks_list)\n",
    "    \n",
    "    # predict\n",
    "    model.load_weights(model_file)\n",
    "    y_pre = model.predict(x_val, verbose=1)\n",
    "    pred = model.predict(test, verbose=1)\n",
    "    \n",
    "    submission = pd.read_csv(PATH + 'sample_submission.csv')\n",
    "    submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = pred\n",
    "    submission.to_csv(pred_file, index=False)\n",
    "    \n",
    "    # compute the scores\n",
    "    best_epoch = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "    val_loss = history.history['val_loss'][best_epoch]\n",
    "    val_auc = roc_auc_score(y_val, y_pre)\n",
    "    \n",
    "    thres = 0.5\n",
    "    def f(x):\n",
    "        return (x > thres)*1\n",
    "    \n",
    "    M = {}\n",
    "    y_pre = pd.DataFrame(y_pre, columns=label_cols)\n",
    "    for i in label_cols:\n",
    "        y_tmp = y_pre[i].apply(f)\n",
    "        M[i] = confusion_matrix(y_val[i], y_tmp)\n",
    "    \n",
    "    return val_loss, best_epoch, val_auc, model_file, pred_file, M\n",
    "\n",
    "def model_run(param, X, y, other_model_dependency_dict, test, X_val = None, y_val=None):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=param['max_features'])\n",
    "    tokenizer.fit_on_texts(X_train.values)\n",
    "    tokenized_train = tokenizer.texts_to_sequences(X_train.values)\n",
    "    tokenized_val = tokenizer.texts_to_sequences(X_val.values)\n",
    "    x_train = sequence.pad_sequences(tokenized_train, maxlen=param['max_len'])\n",
    "    x_val = sequence.pad_sequences(tokenized_val, maxlen=param['max_len'])\n",
    "    tokenized_test = tokenizer.texts_to_sequences(test.values)\n",
    "    test = sequence.pad_sequences(tokenized_test, maxlen=param['max_len'])\n",
    "    \n",
    "    model = get_rnn_model(param, tokenizer)\n",
    "    val_loss, best_epoch, val_auc, model_file, pred_file, M = train_model(model, x_train, y_train,\n",
    "                                                                          x_val, y_val, param, test)\n",
    "    return [val_loss, best_epoch, val_auc, model_file, pred_file], M\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def get_coefs(word,*arr): \n",
    "#     try:\n",
    "#         return word, np.asarray(arr, dtype='float32') \n",
    "#     except ValueError:\n",
    "#         return 'nnnnnnnaaaaaaa@@!',np.zeros(300)\n",
    "# file = '/home/kai/data/resources/FastText/wiki.en.vec'\n",
    "# embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(file, encoding='utf8')) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'lstm', 'embed_size': 300, 'label_len': 6, 'lstm_units': 128, 'embed_file': '/home/kai/data/resources/glove/glove.840B.300d.txt', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text', 'dense': True, 'batch_size': 128, 'cnn': True}\n",
      "../../model/1520863673.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9788\n",
      "Epoch 00001: val_loss improved from inf to 0.04649, saving model to ../../model/1520863673.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.973952\n",
      "129251/129251 [==============================] - 357s 3ms/step - loss: 0.0617 - acc: 0.9788 - val_loss: 0.0465 - val_acc: 0.9826\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9828\n",
      "Epoch 00002: val_loss improved from 0.04649 to 0.04346, saving model to ../../model/1520863673.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.981800\n",
      "129251/129251 [==============================] - 357s 3ms/step - loss: 0.0455 - acc: 0.9828 - val_loss: 0.0435 - val_acc: 0.9830\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9837\n",
      "Epoch 00003: val_loss improved from 0.04346 to 0.04333, saving model to ../../model/1520863673.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.979857\n",
      "129251/129251 [==============================] - 357s 3ms/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0433 - val_acc: 0.9833\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9843\n",
      "Epoch 00004: val_loss improved from 0.04333 to 0.04298, saving model to ../../model/1520863673.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.981141\n",
      "129251/129251 [==============================] - 355s 3ms/step - loss: 0.0403 - acc: 0.9843 - val_loss: 0.0430 - val_acc: 0.9833\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850\n",
      "Epoch 00005: val_loss improved from 0.04298 to 0.04284, saving model to ../../model/1520863673.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.981935\n",
      "129251/129251 [==============================] - 357s 3ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0428 - val_acc: 0.9835\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9856\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.982657\n",
      "129251/129251 [==============================] - 356s 3ms/step - loss: 0.0361 - acc: 0.9856 - val_loss: 0.0440 - val_acc: 0.9836\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9861\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.981463\n",
      "129251/129251 [==============================] - 357s 3ms/step - loss: 0.0345 - acc: 0.9861 - val_loss: 0.0436 - val_acc: 0.9834\n",
      "Epoch 8/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9868\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.982487\n",
      "129251/129251 [==============================] - 356s 3ms/step - loss: 0.0324 - acc: 0.9868 - val_loss: 0.0458 - val_acc: 0.9831\n",
      "14362/14362 [==============================] - 33s 2ms/step\n",
      "153164/153164 [==============================] - 356s 2ms/step\n",
      "val_loss:0.042838421678114795, best_epoch:4, val_auc:0.9819350111551332, model_file:../../model/1520863673.hdf5, pred_file:../../model/1520863673.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'gru', 'embed_size': 300, 'label_len': 6, 'lstm_units': 128, 'embed_file': '/home/kai/data/resources/glove/glove.840B.300d.txt', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text_cleaned', 'dense': True, 'batch_size': 128, 'cnn': False}\n",
      "../../model/1520867031.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9792\n",
      "Epoch 00001: val_loss improved from inf to 0.04350, saving model to ../../model/1520867031.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.982905\n",
      "129251/129251 [==============================] - 299s 2ms/step - loss: 0.0598 - acc: 0.9792 - val_loss: 0.0435 - val_acc: 0.9834\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9829\n",
      "Epoch 00002: val_loss improved from 0.04350 to 0.04211, saving model to ../../model/1520867031.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986482\n",
      "129251/129251 [==============================] - 299s 2ms/step - loss: 0.0451 - acc: 0.9829 - val_loss: 0.0421 - val_acc: 0.9835\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836\n",
      "Epoch 00003: val_loss improved from 0.04211 to 0.04090, saving model to ../../model/1520867031.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.987649\n",
      "129251/129251 [==============================] - 297s 2ms/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9843\n",
      "Epoch 00004: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.988766\n",
      "129251/129251 [==============================] - 298s 2ms/step - loss: 0.0404 - acc: 0.9843 - val_loss: 0.0411 - val_acc: 0.9840\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9848\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.988408\n",
      "129251/129251 [==============================] - 299s 2ms/step - loss: 0.0386 - acc: 0.9848 - val_loss: 0.0414 - val_acc: 0.9843\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9853\n",
      "Epoch 00006: val_loss improved from 0.04090 to 0.04037, saving model to ../../model/1520867031.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.988989\n",
      "129251/129251 [==============================] - 299s 2ms/step - loss: 0.0372 - acc: 0.9853 - val_loss: 0.0404 - val_acc: 0.9842\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9856\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.988938\n",
      "129251/129251 [==============================] - 298s 2ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0406 - val_acc: 0.9842\n",
      "Epoch 8/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9860\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.988427\n",
      "129251/129251 [==============================] - 299s 2ms/step - loss: 0.0346 - acc: 0.9860 - val_loss: 0.0418 - val_acc: 0.9840\n",
      "14362/14362 [==============================] - 28s 2ms/step\n",
      "153164/153164 [==============================] - 297s 2ms/step\n",
      "val_loss:0.04036597131514529, best_epoch:5, val_auc:0.9889889019556001, model_file:../../model/1520867031.hdf5, pred_file:../../model/1520867031.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'lstm', 'embed_size': 300, 'label_len': 6, 'lstm_units': 32, 'embed_file': '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text', 'dense': True, 'batch_size': 128, 'cnn': True}\n",
      "../../model/1520869857.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9772\n",
      "Epoch 00001: val_loss improved from inf to 0.04935, saving model to ../../model/1520869857.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.977621\n",
      "129251/129251 [==============================] - 362s 3ms/step - loss: 0.0699 - acc: 0.9772 - val_loss: 0.0494 - val_acc: 0.9810\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9815\n",
      "Epoch 00002: val_loss improved from 0.04935 to 0.04701, saving model to ../../model/1520869857.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.982888\n",
      "129251/129251 [==============================] - 358s 3ms/step - loss: 0.0498 - acc: 0.9815 - val_loss: 0.0470 - val_acc: 0.9817\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9822\n",
      "Epoch 00003: val_loss improved from 0.04701 to 0.04422, saving model to ../../model/1520869857.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.984798\n",
      "129251/129251 [==============================] - 361s 3ms/step - loss: 0.0468 - acc: 0.9822 - val_loss: 0.0442 - val_acc: 0.9828\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9830\n",
      "Epoch 00004: val_loss improved from 0.04422 to 0.04324, saving model to ../../model/1520869857.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.987354\n",
      "129251/129251 [==============================] - 360s 3ms/step - loss: 0.0446 - acc: 0.9830 - val_loss: 0.0432 - val_acc: 0.9831\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9835\n",
      "Epoch 00005: val_loss improved from 0.04324 to 0.04193, saving model to ../../model/1520869857.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.988180\n",
      "129251/129251 [==============================] - 357s 3ms/step - loss: 0.0425 - acc: 0.9835 - val_loss: 0.0419 - val_acc: 0.9835\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9840\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.988784\n",
      "129251/129251 [==============================] - 351s 3ms/step - loss: 0.0411 - acc: 0.9840 - val_loss: 0.0434 - val_acc: 0.9828\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9843\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.988928\n",
      "129251/129251 [==============================] - 345s 3ms/step - loss: 0.0399 - acc: 0.9843 - val_loss: 0.0431 - val_acc: 0.9832\n",
      "Epoch 8/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9846\n",
      "Epoch 00008: val_loss improved from 0.04193 to 0.04164, saving model to ../../model/1520869857.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.989108\n",
      "129251/129251 [==============================] - 345s 3ms/step - loss: 0.0392 - acc: 0.9846 - val_loss: 0.0416 - val_acc: 0.9840\n",
      "14362/14362 [==============================] - 33s 2ms/step\n",
      "153164/153164 [==============================] - 401s 3ms/step\n",
      "val_loss:0.04164193182521719, best_epoch:7, val_auc:0.989107844617478, model_file:../../model/1520869857.hdf5, pred_file:../../model/1520869857.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'gru', 'embed_size': 300, 'label_len': 6, 'lstm_units': 128, 'embed_file': '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text', 'dense': False, 'batch_size': 128, 'cnn': True}\n",
      "../../model/1520873235.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9784\n",
      "Epoch 00001: val_loss improved from inf to 0.04557, saving model to ../../model/1520873235.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.978928\n",
      "129251/129251 [==============================] - 294s 2ms/step - loss: 0.0627 - acc: 0.9784 - val_loss: 0.0456 - val_acc: 0.9832\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9823\n",
      "Epoch 00002: val_loss improved from 0.04557 to 0.04330, saving model to ../../model/1520873235.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.984689\n",
      "129251/129251 [==============================] - 292s 2ms/step - loss: 0.0465 - acc: 0.9823 - val_loss: 0.0433 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9832\n",
      "Epoch 00003: val_loss improved from 0.04330 to 0.04245, saving model to ../../model/1520873235.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.985903\n",
      "129251/129251 [==============================] - 294s 2ms/step - loss: 0.0437 - acc: 0.9832 - val_loss: 0.0425 - val_acc: 0.9839\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9839\n",
      "Epoch 00004: val_loss improved from 0.04245 to 0.04239, saving model to ../../model/1520873235.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.986883\n",
      "129251/129251 [==============================] - 294s 2ms/step - loss: 0.0415 - acc: 0.9840 - val_loss: 0.0424 - val_acc: 0.9838\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9843\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.986720\n",
      "129251/129251 [==============================] - 293s 2ms/step - loss: 0.0402 - acc: 0.9843 - val_loss: 0.0427 - val_acc: 0.9839\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9850\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.987269\n",
      "129251/129251 [==============================] - 292s 2ms/step - loss: 0.0383 - acc: 0.9850 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9857\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.986881\n",
      "129251/129251 [==============================] - 293s 2ms/step - loss: 0.0368 - acc: 0.9857 - val_loss: 0.0428 - val_acc: 0.9838\n",
      "14362/14362 [==============================] - 28s 2ms/step\n",
      "153164/153164 [==============================] - 294s 2ms/step\n",
      "val_loss:0.042390606174541955, best_epoch:3, val_auc:0.9868830943582304, model_file:../../model/1520873235.hdf5, pred_file:../../model/1520873235.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'lstm', 'embed_size': 300, 'label_len': 6, 'lstm_units': 128, 'embed_file': '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text_cleaned', 'dense': False, 'batch_size': 128, 'cnn': False}\n",
      "../../model/1520875718.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9764\n",
      "Epoch 00001: val_loss improved from inf to 0.05022, saving model to ../../model/1520875718.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.969820\n",
      "129251/129251 [==============================] - 362s 3ms/step - loss: 0.0733 - acc: 0.9764 - val_loss: 0.0502 - val_acc: 0.9814\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9818\n",
      "Epoch 00002: val_loss improved from 0.05022 to 0.04688, saving model to ../../model/1520875718.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.974702\n",
      "129251/129251 [==============================] - 361s 3ms/step - loss: 0.0485 - acc: 0.9818 - val_loss: 0.0469 - val_acc: 0.9824\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9829\n",
      "Epoch 00003: val_loss improved from 0.04688 to 0.04460, saving model to ../../model/1520875718.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.979585\n",
      "129251/129251 [==============================] - 362s 3ms/step - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0446 - val_acc: 0.9831\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9835\n",
      "Epoch 00004: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.980987\n",
      "129251/129251 [==============================] - 361s 3ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0447 - val_acc: 0.9827\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9842\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.981475\n",
      "129251/129251 [==============================] - 361s 3ms/step - loss: 0.0409 - acc: 0.9842 - val_loss: 0.0452 - val_acc: 0.9825\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9847\n",
      "Epoch 00006: val_loss improved from 0.04460 to 0.04284, saving model to ../../model/1520875718.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.981886\n",
      "129251/129251 [==============================] - 360s 3ms/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0428 - val_acc: 0.9835\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9851\n",
      "Epoch 00007: val_loss improved from 0.04284 to 0.04257, saving model to ../../model/1520875718.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.983329\n",
      "129251/129251 [==============================] - 361s 3ms/step - loss: 0.0383 - acc: 0.9851 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9856\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.983408\n",
      "129251/129251 [==============================] - 361s 3ms/step - loss: 0.0370 - acc: 0.9856 - val_loss: 0.0427 - val_acc: 0.9839\n",
      "14362/14362 [==============================] - 34s 2ms/step\n",
      "153164/153164 [==============================] - 363s 2ms/step\n",
      "val_loss:0.042568991778558746, best_epoch:6, val_auc:0.9833286617034419, model_file:../../model/1520875718.hdf5, pred_file:../../model/1520875718.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'gru', 'embed_size': 200, 'label_len': 6, 'lstm_units': 128, 'embed_file': '/home/kai/data/resources/glove/glove.twitter.27B.200d.txt', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text_cleaned', 'dense': False, 'batch_size': 128, 'cnn': True}\n",
      "../../model/1520879061.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9793\n",
      "Epoch 00001: val_loss improved from inf to 0.04622, saving model to ../../model/1520879061.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.983555\n",
      "129251/129251 [==============================] - 295s 2ms/step - loss: 0.0599 - acc: 0.9793 - val_loss: 0.0462 - val_acc: 0.9827\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9826\n",
      "Epoch 00002: val_loss improved from 0.04622 to 0.04381, saving model to ../../model/1520879061.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986656\n",
      "129251/129251 [==============================] - 299s 2ms/step - loss: 0.0462 - acc: 0.9826 - val_loss: 0.0438 - val_acc: 0.9831\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9836\n",
      "Epoch 00003: val_loss improved from 0.04381 to 0.04374, saving model to ../../model/1520879061.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.986789\n",
      "129251/129251 [==============================] - 328s 3ms/step - loss: 0.0430 - acc: 0.9836 - val_loss: 0.0437 - val_acc: 0.9835\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9842\n",
      "Epoch 00004: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.986925\n",
      "129251/129251 [==============================] - 293s 2ms/step - loss: 0.0410 - acc: 0.9842 - val_loss: 0.0438 - val_acc: 0.9831\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9850\n",
      "Epoch 00005: val_loss improved from 0.04374 to 0.04301, saving model to ../../model/1520879061.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.987711\n",
      "129251/129251 [==============================] - 291s 2ms/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9856\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.987179\n",
      "129251/129251 [==============================] - 292s 2ms/step - loss: 0.0371 - acc: 0.9856 - val_loss: 0.0442 - val_acc: 0.9830\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9862\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.987291\n",
      "129251/129251 [==============================] - 292s 2ms/step - loss: 0.0355 - acc: 0.9862 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 8/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9868\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.987343\n",
      "129251/129251 [==============================] - 290s 2ms/step - loss: 0.0337 - acc: 0.9868 - val_loss: 0.0445 - val_acc: 0.9828\n",
      "14362/14362 [==============================] - 28s 2ms/step\n",
      "153164/153164 [==============================] - 293s 2ms/step\n",
      "val_loss:0.04300946636259548, best_epoch:4, val_auc:0.9877111980957393, model_file:../../model/1520879061.hdf5, pred_file:../../model/1520879061.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'lstm', 'embed_size': 300, 'label_len': 6, 'lstm_units': 32, 'embed_file': '/home/kai/data/resources/glove/glove.840B.300d.txt', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text', 'dense': True, 'batch_size': 128, 'cnn': True}\n",
      "../../model/1520881879.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9773\n",
      "Epoch 00001: val_loss improved from inf to 0.04662, saving model to ../../model/1520881879.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.974452\n",
      "129251/129251 [==============================] - 360s 3ms/step - loss: 0.0674 - acc: 0.9773 - val_loss: 0.0466 - val_acc: 0.9826\n",
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9823\n",
      "Epoch 00002: val_loss improved from 0.04662 to 0.04406, saving model to ../../model/1520881879.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.979479\n",
      "129251/129251 [==============================] - 359s 3ms/step - loss: 0.0470 - acc: 0.9823 - val_loss: 0.0441 - val_acc: 0.9834\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9832\n",
      "Epoch 00003: val_loss improved from 0.04406 to 0.04200, saving model to ../../model/1520881879.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.981737\n",
      "129251/129251 [==============================] - 392s 3ms/step - loss: 0.0439 - acc: 0.9832 - val_loss: 0.0420 - val_acc: 0.9838\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9838\n",
      "Epoch 00004: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.983708\n",
      "129251/129251 [==============================] - 359s 3ms/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.0422 - val_acc: 0.9838\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9844\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.984218\n",
      "129251/129251 [==============================] - 360s 3ms/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.0426 - val_acc: 0.9841\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847\n",
      "Epoch 00006: val_loss improved from 0.04200 to 0.04173, saving model to ../../model/1520881879.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.984998\n",
      "129251/129251 [==============================] - 360s 3ms/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0417 - val_acc: 0.9840\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.985708\n",
      "129251/129251 [==============================] - 359s 3ms/step - loss: 0.0379 - acc: 0.9849 - val_loss: 0.0422 - val_acc: 0.9841\n",
      "Epoch 8/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.985530\n",
      "129251/129251 [==============================] - 362s 3ms/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0421 - val_acc: 0.9842\n",
      "14362/14362 [==============================] - 34s 2ms/step\n",
      "153164/153164 [==============================] - 361s 2ms/step\n",
      "val_loss:0.04172850884637234, best_epoch:5, val_auc:0.984998131356933, model_file:../../model/1520881879.hdf5, pred_file:../../model/1520881879.csv\n",
      "\n",
      "{'max_features': 100000, 'embed_type': 'glove', 'gru_lstm': 'lstm', 'embed_size': 200, 'label_len': 6, 'lstm_units': 128, 'embed_file': '/home/kai/data/resources/glove/glove.twitter.27B.200d.txt', 'recurrent_dropout': 0.1, 'epochs': 8, 'embed_trainable': False, 'loss': 'binary_crossentropy', 'patience': 3, 'max_len': 150, 'train': 'comment_text_cleaned', 'dense': True, 'batch_size': 128, 'cnn': True}\n",
      "../../model/1520885251.hdf5\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9778\n",
      "Epoch 00001: val_loss improved from inf to 0.04845, saving model to ../../model/1520885251.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.975918\n",
      "129251/129251 [==============================] - 367s 3ms/step - loss: 0.0651 - acc: 0.9778 - val_loss: 0.0485 - val_acc: 0.9818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9823\n",
      "Epoch 00002: val_loss improved from 0.04845 to 0.04359, saving model to ../../model/1520885251.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.982886\n",
      "129251/129251 [==============================] - 367s 3ms/step - loss: 0.0476 - acc: 0.9823 - val_loss: 0.0436 - val_acc: 0.9830\n",
      "Epoch 3/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832\n",
      "Epoch 00003: val_loss improved from 0.04359 to 0.04218, saving model to ../../model/1520885251.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.984940\n",
      "129251/129251 [==============================] - 364s 3ms/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0422 - val_acc: 0.9835\n",
      "Epoch 4/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9837\n",
      "Epoch 00004: val_loss improved from 0.04218 to 0.04205, saving model to ../../model/1520885251.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.986125\n",
      "129251/129251 [==============================] - 365s 3ms/step - loss: 0.0416 - acc: 0.9837 - val_loss: 0.0420 - val_acc: 0.9836\n",
      "Epoch 5/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9843\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.986549\n",
      "129251/129251 [==============================] - 367s 3ms/step - loss: 0.0401 - acc: 0.9843 - val_loss: 0.0421 - val_acc: 0.9831\n",
      "Epoch 6/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850\n",
      "Epoch 00006: val_loss improved from 0.04205 to 0.04193, saving model to ../../model/1520885251.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.985555\n",
      "129251/129251 [==============================] - 364s 3ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 7/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9855\n",
      "Epoch 00007: val_loss improved from 0.04193 to 0.04162, saving model to ../../model/1520885251.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.985982\n",
      "129251/129251 [==============================] - 365s 3ms/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0416 - val_acc: 0.9833\n",
      "Epoch 8/8\n",
      "129152/129251 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9861\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.986502\n",
      "129251/129251 [==============================] - 371s 3ms/step - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0426 - val_acc: 0.9837\n",
      "14362/14362 [==============================] - 33s 2ms/step\n",
      "133408/153164 [=========================>....] - ETA: 46s"
     ]
    }
   ],
   "source": [
    "# params = {'max_features': [100000,150000], \n",
    "#           'epochs': [8],#,5,6], \n",
    "#           'batch_size': [128],64,32],\n",
    "#           'max_len': [250,150],#int(np.round(mean_length + 2*std_length)), int(np.round(mean_length + 1*std_length))],\n",
    "#           'dropout': [0.1],\n",
    "#           'patience': [3],\n",
    "#           'loss': ['binary_crossentropy'],\n",
    "#           'label_len': [len(label_cols)],\n",
    "#           'embed_trainable': [True, False],\n",
    "#           'batch_normalization': [False],\n",
    "#           'activation': ['relu']#, 'tanh', 'sigmoid'],\n",
    "#           'lstm_activation': ['tanh'],\n",
    "#           'lstm_units': [64, 128],#GRU, LSTM\n",
    "#           'dense_units': [100],\n",
    "#           'lstm_layer_size': [1],\n",
    "#           'dense_layer_size': [1],\n",
    "#           'embedding_param': [{'embed_file': '/home/kai/data/resources/glove/glove.840B.300d.txt', 'embed_size': 300, 'embed_type': 'glove'},\n",
    "#                               {'embed_file': '/home/kai/data/resources/glove/glove.twitter.27B.200d.txt', 'embed_size': 200, 'embed_type': 'glove'},\n",
    "#                               {'embed_file': '/home/kai/data/resources/FastText/wiki.en.bin', 'embed_size': 300, 'embed_type': 'fasttext'}]\n",
    "#         }\n",
    "\n",
    "params = {'train': ['comment_text', 'comment_text_cleaned'],\n",
    "          'max_features': [100000],\n",
    "          'epochs': [6],\n",
    "          'batch_size': [128],\n",
    "          'max_len': [150],\n",
    "          'recurrent_dropout': [0.1],\n",
    "          'patience': [2],\n",
    "          'loss': ['binary_crossentropy'],\n",
    "          'label_len': [len(label_cols)],\n",
    "          'embed_trainable': [False],\n",
    "          'cnn': [True, False],\n",
    "          'lstm_units': [32, 128],\n",
    "          'gru_lstm': ['gru', 'lstm'],\n",
    "          'dense': [True, False],\n",
    "#           'embedding_param': [{'embed_file': '../../data/glove.6B.50d.txt', 'embed_size': 50, 'embed_type': 'glove'}]\n",
    "          'embedding_param': [{'embed_file': '/home/kai/data/resources/glove/glove.twitter.27B.200d.txt', 'embed_size': 200, 'embed_type': 'glove'},\n",
    "                              {'embed_file': '/home/kai/data/resources/lexvec/lexvec.commoncrawl.300d.W.pos.vectors', 'embed_size': 300, 'embed_type': 'glove'},\n",
    "                              {'embed_file': '/home/kai/data/resources/glove/glove.840B.300d.txt', 'embed_size': 300, 'embed_type': 'glove'},\n",
    "                              {'embed_file': '/home/kai/data/resources/FastText/wiki.en.bin', 'embed_size': 300, 'embed_type': 'fasttext'}]\n",
    "         }\n",
    "\n",
    "bindings = {'embedding_param': ['embed_type', 'embed_file', 'embed_size']}\n",
    "\n",
    "score_name_list = ['val_loss', 'best_epoch', 'val_auc', 'model_file', 'pred_file']\n",
    "\n",
    "grid_search_csv_url = 'grid_search.csv'\n",
    "grid_search_result_csv_url = 'grid_search_result.csv'\n",
    "# param_class = grid_search_generator(params=params, bindings=bindings,\n",
    "#                                     score_name_list=score_name_list, csv_url=grid_search_csv_url)\n",
    "\n",
    "grid_search_generator.search(remain_csv=grid_search_csv_url, record_csv=grid_search_result_csv_url,\n",
    "                             model_run=model_run, other_model_dependency_dict=None, X_y_split=True,\n",
    "                             label_list=label_cols)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
