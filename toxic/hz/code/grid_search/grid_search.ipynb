{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastText import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 10)\n",
      "74.75983104699475\n",
      "110.47788051973407\n"
     ]
    }
   ],
   "source": [
    "HOME = '/home/kai/data/kaggle/toxic/hz/'\n",
    "DATA = HOME + 'data/'\n",
    "MODEL = HOME + 'model/'\n",
    "RECORD = DATA + 'summary.csv'\n",
    "\n",
    "combine_test = False\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "tok = TweetTokenizer()\n",
    "# train = pd.read_csv(DATA + 'cleaned_train.csv')\n",
    "train = pd.read_csv('/home/kai/data/kaggle/toxic/dataset/training/emoji_train.csv')\n",
    "\n",
    "train_sentences = train['comment_text_cleaned']\n",
    "sentences = train_sentences\n",
    "\n",
    "text_length = sentences.apply(lambda x: len(tok.tokenize(x)))\n",
    "mean_length = text_length.mean()\n",
    "std_length = text_length.std()\n",
    "\n",
    "print(train.shape)\n",
    "print(mean_length)\n",
    "print(std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# grid search params\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class grid_search_generator(object):\n",
    "    # Here needs to be modified\n",
    "    def __init__(self, config_file_url=None):\n",
    "        if config_file_url == None:\n",
    "            self.params = {'max_features': [200000], \n",
    "                      'epochs': [40], \n",
    "                      'batch_size': [1024],\n",
    "                      'max_len': [int(np.round(mean_length + 3*std_length))], # max sequence length\n",
    "                      'dropout': [0.2, 0.5],\n",
    "                      'patience': [5],\n",
    "                      'model_file': [MODEL + 'lstm_best.hdf5'],\n",
    "                      'loss': ['binary_crossentropy'],\n",
    "                      'label_len': [len(label_cols)],\n",
    "                      # fixed grid search params separate line\n",
    "                      'embed_trainable': [True, False],\n",
    "                      'batch_normalization': [True, False],\n",
    "                      'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "                      'lstm_activation': ['relu', 'tanh', 'sigmoid'],\n",
    "                      'lstm_units': [5, 50, 100, 200],\n",
    "                      'dense_units': [50, 100, 200, 300],\n",
    "                      'lstm_layer_size': [1, 2],\n",
    "                      'dense_layer_size': [1, 2, 3],\n",
    "                      'embedding_param': [{'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'embed_size': 50, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/glove/glove.6B.100d.txt', 'embed_size': 100, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/glove/glove.6B.200d.txt', 'embed_size': 200, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/glove/glove.6B.300d.txt', 'embed_size': 300, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/FastText/wiki.en.bin', 'embed_size': 300, 'embed_type': 'fasttext'}]\n",
    "                    }\n",
    "\n",
    "            self.binding = {'embedding_param': ['embed_type', 'embed_file', 'embed_size']}\n",
    "            self.score_name_list = ['val_loss', 'val_auc', 'train_auc']\n",
    "            \n",
    "            self.single_keys = [key for key in self.params.keys() if key not in self.binding.keys()]\n",
    "            self.binding_keys = list(self.binding.keys())\n",
    "\n",
    "            self._terminate = False\n",
    "        else:\n",
    "            with open(config_file_url, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.params = data['params']\n",
    "                self.binding = data['binding']\n",
    "                self.score_name_list = data['score_name_list']\n",
    "                self.single_keys = data['single_keys']\n",
    "                self.binding_keys = data['binding_keys']\n",
    "                self._idx = data['_idx']\n",
    "                \n",
    "        self._terminate = False\n",
    "        self.keys = list(self.single_keys)\n",
    "        self.keys.extend(self.binding_keys)\n",
    "        self.param_keys = list(self.single_keys)\n",
    "        for i in range(len(self.single_keys), len(self.keys)):\n",
    "            self.param_keys.extend(self.binding[self.keys[i]])\n",
    "        \n",
    "        if config_file_url == None: self._idx = [int(0) for i in range(len(self.keys))]\n",
    "        self._ub = [len(self.params[key]) for key in self.keys]\n",
    "    \n",
    "    def _next_idx(self):\n",
    "        i = 0\n",
    "        self._idx[i] = (self._idx[i] + 1) % self._ub[i]\n",
    "        i += 1\n",
    "        end_loop = (self._idx[i-1]!=0 or i==len(self._idx))\n",
    "        terminate = (self._idx[i-1]==0 and i==len(self._idx))\n",
    "        while(not end_loop):\n",
    "            self._idx[i] = (self._idx[i] + 1) % self._ub[i]\n",
    "            i += 1\n",
    "            end_loop = (self._idx[i-1]!=0 or i==len(self._idx))\n",
    "            terminate = (self._idx[i-1]==0 and i==len(self._idx))\n",
    "        self._terminate = terminate\n",
    "\n",
    "    def _next_param_list(self):\n",
    "        if not self._terminate:\n",
    "            input = [self.params[self.keys[i]][self._idx[i]] for i in range(len(self.single_keys))]\n",
    "            for i in range(len(self.single_keys), len(self.keys)):\n",
    "                input.extend([self.params[self.keys[i]][self._idx[i]][key] for key in self.binding[self.keys[i]]])\n",
    "            self._next_idx()\n",
    "            return input\n",
    "        else: return None\n",
    "        \n",
    "    def get_csv(self, csv_url):\n",
    "        value_list = [float('nan') for i in range(len(self.score_name_list))]\n",
    "        column_name = list(self.param_keys)\n",
    "        column_name.extend(self.score_name_list)\n",
    "        param_list = []\n",
    "        \n",
    "        param = self._next_param_list()\n",
    "        while param != None:\n",
    "            param.extend(value_list)\n",
    "            param_list.append(param)\n",
    "            param = self._next_param_list()\n",
    "        pd.DataFrame(param_list, columns=column_name).to_csv(csv_url, index=False)\n",
    "        self._idx = [0 for i in range(len(self.keys))]\n",
    "        self._terminate = False\n",
    "        print('successfully generated grid search csv file\\n')\n",
    "        return 0\n",
    "    \n",
    "    def _get_grid_search_param(self, keys, values): return dict(zip(keys, values))\n",
    "    \n",
    "    def next_param(self, url):\n",
    "        value = self._next_param_list()\n",
    "        if value == None:\n",
    "            with open(url, 'w') as f: f.write('terminate')\n",
    "            return None\n",
    "        param_dict = self._get_grid_search_param(self.param_keys, value)\n",
    "        with open(url, 'w') as f:\n",
    "            data = {\n",
    "                'params': self.params,\n",
    "                'binding': self.binding,\n",
    "                'score_name_list': self.score_name_list,\n",
    "                'single_keys': self.single_keys,\n",
    "                'binding_keys': self.binding_keys,\n",
    "                '_idx': self._idx\n",
    "            }\n",
    "            json.dump(data, f)\n",
    "        return param_dict\n",
    "    \n",
    "    # get_model, train, recorder\n",
    "    def grid_search_on_model(self, get_model, train, recorder, record_file, x, y, tokenizer,\\\n",
    "                             val_size=0.2, shuffle=True, url='a.json'):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=val_size, shuffle=shuffle)\n",
    "        params = self.next_param(url)\n",
    "        record_start = True\n",
    "        while params != None:\n",
    "            print(params)\n",
    "            model = get_model(params, tokenizer)\n",
    "            train_auc, val_auc, val_loss = train(model, x_train, y_train, x_val, y_val, params)\n",
    "            recorder(record_file, params, record_start, train_auc, val_auc, val_loss)\n",
    "            record_start = False\n",
    "            params = self.next_param(url)\n",
    "            \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# record grid search results\n",
    "def recorder(record_file, params, initialize, train_auc, val_auc, val_loss):\n",
    "    if initialize:\n",
    "        head = ''\n",
    "        for x in params.keys():\n",
    "            head += x + ','\n",
    "        head += 'train_auc,val_auc,val_loss\\n'\n",
    "        with open(record_file, 'w') as f: f.write(head)\n",
    "    r = ''\n",
    "    for x in params.values():\n",
    "        r += str(x) + ','\n",
    "    r += '%.6f,%.6f,%.6f\\n'%(train_auc, val_auc, val_loss)\n",
    "    with open(record_file, 'a') as f: f.write(r)\n",
    "    print('train_auc: {}, val_auc: {}, val_loss: {}\\n\\n'.format(train_auc, val_auc, val_loss))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def glove_get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "#     with open(embedding_file) as ef:\n",
    "#         embeddings_index = dict(get_coefs(*o.strip().split()) for o in ef.readlines())\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file, encoding='utf8'))\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "def fasttext_get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    ft_model = load_model(embedding_file)\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = ft_model.get_word_vector(word).astype('float32')\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "def get_embedding_matrix(embed_type, file, size, max_features, tokenizer):\n",
    "    if embed_type == 'fasttext': return fasttext_get_embedding_matrix(file, size, max_features, tokenizer)\n",
    "    else: return glove_get_embedding_matrix(file, size, max_features, tokenizer)\n",
    "\n",
    "def get_rnn_model(params, tokenizer):\n",
    "    embed_type = params['embed_type']\n",
    "    embed_file = params['embed_file']\n",
    "    embed_size = params['embed_size']\n",
    "    lstm_units = params['lstm_units']\n",
    "    lstm_activation = params['lstm_activation']\n",
    "    dense_units = params['dense_units']\n",
    "    activation = params['activation']\n",
    "    embed_trainable = params['embed_trainable']\n",
    "    batch_normalization = params['batch_normalization']\n",
    "    \n",
    "    max_len = params['max_len']\n",
    "    dropout = params['dropout']\n",
    "    loss = params['loss']\n",
    "    label_len = params['label_len']\n",
    "    max_features = params['max_features']\n",
    "    \n",
    "    embedding_matrix, inp_len = get_embedding_matrix(embed_type, embed_file, embed_size, max_features, tokenizer)\n",
    "    input = Input(shape=(max_len, ))\n",
    "    x = Embedding(inp_len, embed_size, weights=[embedding_matrix], trainable=embed_trainable)(input)\n",
    "    for i in range(params['lstm_layer_size']):\n",
    "        x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=dropout,\\\n",
    "                               recurrent_dropout=dropout, activation=lstm_activation))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    if batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    for i in range(params['dense_layer_size']):\n",
    "        x = Dense(dense_units, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(label_len, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, x, y, x_val, y_val, params):\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    patience = params['patience']\n",
    "    model_file = params['model_file']\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "    history = model.fit(x, y, batch_size=batch_size, epochs=epochs,\\\n",
    "                        validation_data=(x_val,y_val), callbacks=callbacks_list)\n",
    "    \n",
    "    # predict\n",
    "    model.load_weights(model_file)\n",
    "    y_train = model.predict(x, verbose=1)\n",
    "    y_pre = model.predict(x_val, verbose=1)\n",
    "    \n",
    "    # compute the scores\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    val_auc = roc_auc_score(y_val, y_pre)\n",
    "    train_auc = roc_auc_score(y, y_train)\n",
    "    \n",
    "    return val_loss, val_auc, train_auc\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'binary_crossentropy', 'patience': 5, 'max_len': 406, 'max_features': 200000, 'embed_size': 50, 'lstm_activation': 'relu', 'label_len': 6, 'dense_layer_size': 1, 'embed_trainable': True, 'epochs': 40, 'batch_size': 1024, 'activation': 'relu', 'batch_normalization': True, 'dropout': 0.2, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'dense_units': 50, 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'lstm_units': 5, 'lstm_layer_size': 1, 'embed_type': 'glove'}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.9597\n",
      "Epoch 00001: val_loss improved from inf to 0.63772, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 981us/step - loss: 0.6657 - acc: 0.9597 - val_loss: 0.6377 - val_acc: 0.9637\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.9633\n",
      "Epoch 00002: val_loss improved from 0.63772 to 0.58772, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 967us/step - loss: 0.6126 - acc: 0.9632 - val_loss: 0.5877 - val_acc: 0.9637\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.9632\n",
      "Epoch 00003: val_loss improved from 0.58772 to 0.54284, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 975us/step - loss: 0.5653 - acc: 0.9632 - val_loss: 0.5428 - val_acc: 0.9637\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.9633\n",
      "Epoch 00004: val_loss improved from 0.54284 to 0.50257, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 977us/step - loss: 0.5228 - acc: 0.9632 - val_loss: 0.5026 - val_acc: 0.9637\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.9633\n",
      "Epoch 00005: val_loss improved from 0.50257 to 0.46651, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 972us/step - loss: 0.4847 - acc: 0.9632 - val_loss: 0.4665 - val_acc: 0.9637\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.9632\n",
      "Epoch 00006: val_loss improved from 0.46651 to 0.43421, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.4505 - acc: 0.9632 - val_loss: 0.4342 - val_acc: 0.9637\n",
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.9633\n",
      "Epoch 00007: val_loss improved from 0.43421 to 0.40528, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 968us/step - loss: 0.4200 - acc: 0.9632 - val_loss: 0.4053 - val_acc: 0.9637\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.9633\n",
      "Epoch 00008: val_loss improved from 0.40528 to 0.37939, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.3926 - acc: 0.9632 - val_loss: 0.3794 - val_acc: 0.9637\n",
      "Epoch 9/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.9632\n",
      "Epoch 00009: val_loss improved from 0.37939 to 0.35621, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 975us/step - loss: 0.3681 - acc: 0.9632 - val_loss: 0.3562 - val_acc: 0.9637\n",
      "Epoch 10/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.9633\n",
      "Epoch 00010: val_loss improved from 0.35621 to 0.33543, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.3462 - acc: 0.9632 - val_loss: 0.3354 - val_acc: 0.9637\n",
      "Epoch 11/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.9632\n",
      "Epoch 00011: val_loss improved from 0.33543 to 0.31680, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.3266 - acc: 0.9632 - val_loss: 0.3168 - val_acc: 0.9637\n",
      "Epoch 12/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9633\n",
      "Epoch 00012: val_loss improved from 0.31680 to 0.30007, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.3089 - acc: 0.9632 - val_loss: 0.3001 - val_acc: 0.9637\n",
      "Epoch 13/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9632\n",
      "Epoch 00013: val_loss improved from 0.30007 to 0.28506, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.2931 - acc: 0.9632 - val_loss: 0.2851 - val_acc: 0.9637\n",
      "Epoch 14/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9632\n",
      "Epoch 00014: val_loss improved from 0.28506 to 0.27155, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.2789 - acc: 0.9632 - val_loss: 0.2715 - val_acc: 0.9637\n",
      "Epoch 15/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9632\n",
      "Epoch 00015: val_loss improved from 0.27155 to 0.25940, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 968us/step - loss: 0.2661 - acc: 0.9632 - val_loss: 0.2594 - val_acc: 0.9637\n",
      "Epoch 16/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9633\n",
      "Epoch 00016: val_loss improved from 0.25940 to 0.24844, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.2546 - acc: 0.9632 - val_loss: 0.2484 - val_acc: 0.9637\n",
      "Epoch 17/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9632\n",
      "Epoch 00017: val_loss improved from 0.24844 to 0.23857, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.2442 - acc: 0.9632 - val_loss: 0.2386 - val_acc: 0.9637\n",
      "Epoch 18/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9632\n",
      "Epoch 00018: val_loss improved from 0.23857 to 0.22967, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.2348 - acc: 0.9632 - val_loss: 0.2297 - val_acc: 0.9637\n",
      "Epoch 19/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9633\n",
      "Epoch 00019: val_loss improved from 0.22967 to 0.22161, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 967us/step - loss: 0.2264 - acc: 0.9632 - val_loss: 0.2216 - val_acc: 0.9637\n",
      "Epoch 20/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9633\n",
      "Epoch 00020: val_loss improved from 0.22161 to 0.21434, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 974us/step - loss: 0.2187 - acc: 0.9632 - val_loss: 0.2143 - val_acc: 0.9637\n",
      "Epoch 21/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9632\n",
      "Epoch 00021: val_loss improved from 0.21434 to 0.20775, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.2118 - acc: 0.9632 - val_loss: 0.2078 - val_acc: 0.9637\n",
      "Epoch 22/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9632\n",
      "Epoch 00022: val_loss improved from 0.20775 to 0.20178, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 972us/step - loss: 0.2056 - acc: 0.9632 - val_loss: 0.2018 - val_acc: 0.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9633\n",
      "Epoch 00023: val_loss improved from 0.20178 to 0.19637, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.1999 - acc: 0.9632 - val_loss: 0.1964 - val_acc: 0.9637\n",
      "Epoch 24/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9632\n",
      "Epoch 00024: val_loss improved from 0.19637 to 0.19146, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.1948 - acc: 0.9632 - val_loss: 0.1915 - val_acc: 0.9637\n",
      "Epoch 25/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9632\n",
      "Epoch 00025: val_loss improved from 0.19146 to 0.18700, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 967us/step - loss: 0.1901 - acc: 0.9632 - val_loss: 0.1870 - val_acc: 0.9637\n",
      "Epoch 26/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9633\n",
      "Epoch 00026: val_loss improved from 0.18700 to 0.18296, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.1859 - acc: 0.9632 - val_loss: 0.1830 - val_acc: 0.9637\n",
      "Epoch 27/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9633\n",
      "Epoch 00027: val_loss improved from 0.18296 to 0.17927, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.1820 - acc: 0.9632 - val_loss: 0.1793 - val_acc: 0.9637\n",
      "Epoch 28/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9633\n",
      "Epoch 00028: val_loss improved from 0.17927 to 0.17593, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 991us/step - loss: 0.1785 - acc: 0.9632 - val_loss: 0.1759 - val_acc: 0.9637\n",
      "Epoch 29/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9632\n",
      "Epoch 00029: val_loss improved from 0.17593 to 0.17288, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.1754 - acc: 0.9632 - val_loss: 0.1729 - val_acc: 0.9637\n",
      "Epoch 30/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9632\n",
      "Epoch 00030: val_loss improved from 0.17288 to 0.17011, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.1725 - acc: 0.9632 - val_loss: 0.1701 - val_acc: 0.9637\n",
      "Epoch 31/40\n",
      " 89088/127656 [===================>..........] - ETA: 35s - loss: 0.1699 - acc: 0.9634"
     ]
    }
   ],
   "source": [
    "param_class = grid_search_generator()\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=param_class.params['max_features'][0])\n",
    "tokenizer.fit_on_texts(sentences.values)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences.values)\n",
    "\n",
    "x = sequence.pad_sequences(tokenized_train, maxlen=param_class.params['max_len'][0])\n",
    "y = train[label_cols].values\n",
    "\n",
    "param_class.grid_search_on_model(get_rnn_model, train_model, recorder, RECORD, x, y, tokenizer)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_trainable': True, 'patience': 5, 'embed_type': 'glove', 'batch_size': 1024, 'dense_units': 50, 'epochs': 40, 'activation': 'sigmoid', 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'max_len': 406, 'loss': 'binary_crossentropy', 'max_features': 200000, 'lstm_layer_size': 1, 'lstm_activation': 'relu', 'dropout': 0.2, 'label_len': 6, 'lstm_units': 5, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'batch_normalization': True, 'dense_layer_size': 1, 'embed_size': 50}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3743 - acc: 0.8708\n",
      "Epoch 00001: val_loss improved from inf to 15.34928, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 139s 1ms/step - loss: 0.3732 - acc: 0.8713 - val_loss: 15.3493 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 7.7553 - acc: 0.4824\n",
      "Epoch 00002: val_loss did not improve\n",
      "127656/127656 [==============================] - 141s 1ms/step - loss: 7.7961 - acc: 0.4799 - val_loss: 15.3493 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 15.3594 - acc: 0.0000e+00\n",
      "Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 138s 1ms/step - loss: 15.3601 - acc: 0.0000e+00 - val_loss: 15.3493 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 15.3598 - acc: 0.0000e+00\n",
      "Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 148s 1ms/step - loss: 15.3601 - acc: 0.0000e+00 - val_loss: 15.3493 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 15.3596 - acc: 0.0000e+00\n",
      "Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 146s 1ms/step - loss: 15.3601 - acc: 0.0000e+00 - val_loss: 15.3493 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 15.3604 - acc: 0.0000e+00\n",
      "Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 148s 1ms/step - loss: 15.3601 - acc: 0.0000e+00 - val_loss: 15.3493 - val_acc: 0.0000e+00\n",
      "127656/127656 [==============================] - 874s 7ms/step\n",
      "31915/31915 [==============================] - 223s 7ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e9c2758582db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mparam_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search_on_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_rnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRECORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a692c34c575d>\u001b[0m in \u001b[0;36mgrid_search_on_model\u001b[0;34m(self, get_model, train, recorder, record_file, x, y, tokenizer, val_size, shuffle, url)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mtrain_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mrecorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mrecord_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2ea6f5aa68b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, x_val, y_val, params)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# compute the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mval_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mnot_average_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "#continue case\n",
    "\n",
    "param_class = grid_search_generator('a.json')\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=param_class.params['max_features'][0])\n",
    "tokenizer.fit_on_texts(sentences.values)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences.values)\n",
    "\n",
    "x = sequence.pad_sequences(tokenized_train, maxlen=param_class.params['max_len'][0])\n",
    "y = train[label_cols].values\n",
    "\n",
    "param_class.grid_search_on_model(get_rnn_model, train_model, recorder, RECORD, x, y, tokenizer)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
