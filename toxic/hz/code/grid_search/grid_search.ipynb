{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.layers import Dense, Embedding, Input, LSTM, Bidirectional, GlobalMaxPool1D, Dropout, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fastText import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 10)\n",
      "74.75983104699475\n",
      "110.47788051973407\n"
     ]
    }
   ],
   "source": [
    "HOME = '/home/kai/data/kaggle/toxic/hz/'\n",
    "DATA = HOME + 'data/'\n",
    "MODEL = HOME + 'model/'\n",
    "RECORD = DATA + 'summary.csv'\n",
    "\n",
    "combine_test = False\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "tok = TweetTokenizer()\n",
    "# train = pd.read_csv(DATA + 'cleaned_train.csv')\n",
    "train = pd.read_csv('/home/kai/data/kaggle/toxic/dataset/training/emoji_train.csv')\n",
    "\n",
    "train_sentences = train['comment_text_cleaned']\n",
    "sentences = train_sentences\n",
    "\n",
    "text_length = sentences.apply(lambda x: len(tok.tokenize(x)))\n",
    "mean_length = text_length.mean()\n",
    "std_length = text_length.std()\n",
    "\n",
    "print(train.shape)\n",
    "print(mean_length)\n",
    "print(std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# grid search params\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class grid_search_generator(object):\n",
    "    # Here needs to be modified\n",
    "    def __init__(self, config_file_url=None):\n",
    "        if config_file_url == None:\n",
    "            self.params = {'max_features': [200000], \n",
    "                      'epochs': [40], \n",
    "                      'batch_size': [1024],\n",
    "                      'max_len': [int(np.round(mean_length + 3*std_length))], # max sequence length\n",
    "                      'dropout': [0.2, 0.5],\n",
    "                      'patience': [5],\n",
    "                      'model_file': [MODEL + 'lstm_best.hdf5'],\n",
    "                      'loss': ['binary_crossentropy'],\n",
    "                      'label_len': [len(label_cols)],\n",
    "                      # fixed grid search params separate line\n",
    "                      'embed_trainable': [True, False],\n",
    "                      'batch_normalization': [True, False],\n",
    "                      'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "                      'lstm_activation': ['relu', 'tanh', 'sigmoid'],\n",
    "                      'lstm_units': [5, 50, 100, 200],\n",
    "                      'dense_units': [50, 100, 200, 300],\n",
    "                      'lstm_layer_size': [1, 2],\n",
    "                      'dense_layer_size': [1, 2, 3],\n",
    "                      'embedding_param': [{'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'embed_size': 50, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/glove/glove.6B.100d.txt', 'embed_size': 100, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/glove/glove.6B.200d.txt', 'embed_size': 200, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/glove/glove.6B.300d.txt', 'embed_size': 300, 'embed_type': 'glove'},\n",
    "                                          {'embed_file': '/home/kai/data/resources/FastText/wiki.en.bin', 'embed_size': 300, 'embed_type': 'fasttext'}]\n",
    "                    }\n",
    "\n",
    "            self.binding = {'embedding_param': ['embed_type', 'embed_file', 'embed_size']}\n",
    "            self.score_name_list = ['val_loss', 'val_auc', 'train_auc']\n",
    "            \n",
    "            self.single_keys = [key for key in self.params.keys() if key not in self.binding.keys()]\n",
    "            self.binding_keys = list(self.binding.keys())\n",
    "\n",
    "            self._terminate = False\n",
    "        else:\n",
    "            with open(config_file_url, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.params = data['params']\n",
    "                self.binding = data['binding']\n",
    "                self.score_name_list = data['score_name_list']\n",
    "                self.single_keys = data['single_keys']\n",
    "                self.binding_keys = data['binding_keys']\n",
    "                self._idx = data['_idx']\n",
    "                \n",
    "        self._terminate = False\n",
    "        self.keys = list(self.single_keys)\n",
    "        self.keys.extend(self.binding_keys)\n",
    "        self.param_keys = list(self.single_keys)\n",
    "        for i in range(len(self.single_keys), len(self.keys)):\n",
    "            self.param_keys.extend(self.binding[self.keys[i]])\n",
    "        \n",
    "        if config_file_url == None: self._idx = [int(0) for i in range(len(self.keys))]\n",
    "        self._ub = [len(self.params[key]) for key in self.keys]\n",
    "    \n",
    "    def _next_idx(self):\n",
    "        i = 0\n",
    "        self._idx[i] = (self._idx[i] + 1) % self._ub[i]\n",
    "        i += 1\n",
    "        end_loop = (self._idx[i-1]!=0 or i==len(self._idx))\n",
    "        terminate = (self._idx[i-1]==0 and i==len(self._idx))\n",
    "        while(not end_loop):\n",
    "            self._idx[i] = (self._idx[i] + 1) % self._ub[i]\n",
    "            i += 1\n",
    "            end_loop = (self._idx[i-1]!=0 or i==len(self._idx))\n",
    "            terminate = (self._idx[i-1]==0 and i==len(self._idx))\n",
    "        self._terminate = terminate\n",
    "\n",
    "    def _next_param_list(self):\n",
    "        if not self._terminate:\n",
    "            input = [self.params[self.keys[i]][self._idx[i]] for i in range(len(self.single_keys))]\n",
    "            for i in range(len(self.single_keys), len(self.keys)):\n",
    "                input.extend([self.params[self.keys[i]][self._idx[i]][key] for key in self.binding[self.keys[i]]])\n",
    "            self._next_idx()\n",
    "            return input\n",
    "        else: return None\n",
    "        \n",
    "    def get_csv(self, csv_url):\n",
    "        value_list = [float('nan') for i in range(len(self.score_name_list))]\n",
    "        column_name = list(self.param_keys)\n",
    "        column_name.extend(self.score_name_list)\n",
    "        param_list = []\n",
    "        \n",
    "        param = self._next_param_list()\n",
    "        while param != None:\n",
    "            param.extend(value_list)\n",
    "            param_list.append(param)\n",
    "            param = self._next_param_list()\n",
    "        pd.DataFrame(param_list, columns=column_name).to_csv(csv_url, index=False)\n",
    "        self._idx = [0 for i in range(len(self.keys))]\n",
    "        self._terminate = False\n",
    "        print('successfully generated grid search csv file\\n')\n",
    "        return 0\n",
    "    \n",
    "    def _get_grid_search_param(self, keys, values): return dict(zip(keys, values))\n",
    "    \n",
    "    def next_param(self, url):\n",
    "        value = self._next_param_list()\n",
    "        if value == None:\n",
    "            with open(url, 'w') as f: f.write('terminate')\n",
    "            return None\n",
    "        param_dict = self._get_grid_search_param(self.param_keys, value)\n",
    "        with open(url, 'w') as f:\n",
    "            data = {\n",
    "                'params': self.params,\n",
    "                'binding': self.binding,\n",
    "                'score_name_list': self.score_name_list,\n",
    "                'single_keys': self.single_keys,\n",
    "                'binding_keys': self.binding_keys,\n",
    "                '_idx': self._idx\n",
    "            }\n",
    "            json.dump(data, f)\n",
    "        return param_dict\n",
    "    \n",
    "    # get_model, train, recorder\n",
    "    def grid_search_on_model(self, get_model, train, recorder, record_file, x, y, tokenizer,\\\n",
    "                             val_size=0.2, shuffle=True, url='a.json'):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=val_size, shuffle=shuffle)\n",
    "        params = self.next_param(url)\n",
    "        record_start = True\n",
    "        while params != None:\n",
    "            print(params)\n",
    "            model = get_model(params, tokenizer)\n",
    "            train_auc, val_auc, val_loss = train(model, x_train, y_train, x_val, y_val, params)\n",
    "            recorder(record_file, params, record_start, train_auc, val_auc, val_loss)\n",
    "            record_start = False\n",
    "            params = self.next_param(url)\n",
    "            \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# record grid search results\n",
    "def recorder(record_file, params, initialize, train_auc, val_auc, val_loss):\n",
    "    if initialize:\n",
    "        head = ''\n",
    "        for x in params.keys():\n",
    "            head += x + ','\n",
    "        head += 'train_auc,val_auc,val_loss\\n'\n",
    "        with open(record_file, 'w') as f: f.write(head)\n",
    "    r = ''\n",
    "    for x in params.values():\n",
    "        r += str(x) + ','\n",
    "    r += '%.6f,%.6f,%.6f\\n'%(train_auc, val_auc, val_loss)\n",
    "    with open(record_file, 'a') as f: f.write(r)\n",
    "    print('train_auc: {}, val_auc: {}, val_loss: {}\\n\\n'.format(train_auc, val_auc, val_loss))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def glove_get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "#     with open(embedding_file) as ef:\n",
    "#         embeddings_index = dict(get_coefs(*o.strip().split()) for o in ef.readlines())\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file, encoding='utf8'))\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "def fasttext_get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    ft_model = load_model(embedding_file)\n",
    "    nb_words = min(max_features, len(word_index) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = ft_model.get_word_vector(word).astype('float32')\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, nb_words\n",
    "\n",
    "def get_embedding_matrix(embed_type, file, size, max_features, tokenizer):\n",
    "    if embed_type == 'fasttext': return fasttext_get_embedding_matrix(file, size, max_features, tokenizer)\n",
    "    else: return glove_get_embedding_matrix(file, size, max_features, tokenizer)\n",
    "\n",
    "def get_rnn_model(params, tokenizer):\n",
    "    embed_type = params['embed_type']\n",
    "    embed_file = params['embed_file']\n",
    "    embed_size = params['embed_size']\n",
    "    lstm_units = params['lstm_units']\n",
    "    lstm_activation = params['lstm_activation']\n",
    "    dense_units = params['dense_units']\n",
    "    activation = params['activation']\n",
    "    embed_trainable = params['embed_trainable']\n",
    "    batch_normalization = params['batch_normalization']\n",
    "    \n",
    "    max_len = params['max_len']\n",
    "    dropout = params['dropout']\n",
    "    loss = params['loss']\n",
    "    label_len = params['label_len']\n",
    "    max_features = params['max_features']\n",
    "    \n",
    "    embedding_matrix, inp_len = get_embedding_matrix(embed_type, embed_file, embed_size, max_features, tokenizer)\n",
    "    input = Input(shape=(max_len, ))\n",
    "    x = Embedding(inp_len, embed_size, weights=[embedding_matrix], trainable=embed_trainable)(input)\n",
    "    for i in range(params['lstm_layer_size']):\n",
    "        x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=dropout,\\\n",
    "                               recurrent_dropout=dropout, activation=lstm_activation))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    if batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    for i in range(params['dense_layer_size']):\n",
    "        x = Dense(dense_units, activation=activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(label_len, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(model, x, y, x_val, y_val, params):\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "    patience = params['patience']\n",
    "    model_file = params['model_file']\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(model_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=patience)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "    history = model.fit(x, y, batch_size=batch_size, epochs=epochs,\\\n",
    "                        validation_data=(x_val,y_val), callbacks=callbacks_list)\n",
    "    \n",
    "    # predict\n",
    "    model.load_weights(model_file)\n",
    "    y_train = model.predict(x, verbose=1)\n",
    "    y_pre = model.predict(x_val, verbose=1)\n",
    "    \n",
    "    # compute the scores\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    if np.isnan(y_val).any():\n",
    "        print('y_val contains Nan')\n",
    "        y_val = np.nan_to_num(y_val)\n",
    "    if np.isnan(y_pre).any():\n",
    "        print('y_pre contains Nan')\n",
    "        y_pre = np.nan_to_num(y_pre)\n",
    "    if np.isnan(y_train).any():\n",
    "        print('y_train contains Nan')\n",
    "        y_train = np.nan_to_num(y_train)\n",
    "    val_auc = roc_auc_score(y_val, y_pre)\n",
    "    train_auc = roc_auc_score(y, y_train)\n",
    "    \n",
    "    return val_loss, val_auc, train_auc\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'binary_crossentropy', 'patience': 5, 'max_len': 406, 'max_features': 200000, 'embed_size': 50, 'lstm_activation': 'relu', 'label_len': 6, 'dense_layer_size': 1, 'embed_trainable': True, 'epochs': 40, 'batch_size': 1024, 'activation': 'relu', 'batch_normalization': True, 'dropout': 0.2, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'dense_units': 50, 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'lstm_units': 5, 'lstm_layer_size': 1, 'embed_type': 'glove'}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.9597\n",
      "Epoch 00001: val_loss improved from inf to 0.63772, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 981us/step - loss: 0.6657 - acc: 0.9597 - val_loss: 0.6377 - val_acc: 0.9637\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.9633\n",
      "Epoch 00002: val_loss improved from 0.63772 to 0.58772, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 967us/step - loss: 0.6126 - acc: 0.9632 - val_loss: 0.5877 - val_acc: 0.9637\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.5654 - acc: 0.9632\n",
      "Epoch 00003: val_loss improved from 0.58772 to 0.54284, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 975us/step - loss: 0.5653 - acc: 0.9632 - val_loss: 0.5428 - val_acc: 0.9637\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.5228 - acc: 0.9633\n",
      "Epoch 00004: val_loss improved from 0.54284 to 0.50257, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 977us/step - loss: 0.5228 - acc: 0.9632 - val_loss: 0.5026 - val_acc: 0.9637\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.4847 - acc: 0.9633\n",
      "Epoch 00005: val_loss improved from 0.50257 to 0.46651, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 972us/step - loss: 0.4847 - acc: 0.9632 - val_loss: 0.4665 - val_acc: 0.9637\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.9632\n",
      "Epoch 00006: val_loss improved from 0.46651 to 0.43421, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.4505 - acc: 0.9632 - val_loss: 0.4342 - val_acc: 0.9637\n",
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.4200 - acc: 0.9633\n",
      "Epoch 00007: val_loss improved from 0.43421 to 0.40528, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 968us/step - loss: 0.4200 - acc: 0.9632 - val_loss: 0.4053 - val_acc: 0.9637\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.9633\n",
      "Epoch 00008: val_loss improved from 0.40528 to 0.37939, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.3926 - acc: 0.9632 - val_loss: 0.3794 - val_acc: 0.9637\n",
      "Epoch 9/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3682 - acc: 0.9632\n",
      "Epoch 00009: val_loss improved from 0.37939 to 0.35621, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 975us/step - loss: 0.3681 - acc: 0.9632 - val_loss: 0.3562 - val_acc: 0.9637\n",
      "Epoch 10/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3463 - acc: 0.9633\n",
      "Epoch 00010: val_loss improved from 0.35621 to 0.33543, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.3462 - acc: 0.9632 - val_loss: 0.3354 - val_acc: 0.9637\n",
      "Epoch 11/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.9632\n",
      "Epoch 00011: val_loss improved from 0.33543 to 0.31680, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.3266 - acc: 0.9632 - val_loss: 0.3168 - val_acc: 0.9637\n",
      "Epoch 12/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.3090 - acc: 0.9633\n",
      "Epoch 00012: val_loss improved from 0.31680 to 0.30007, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.3089 - acc: 0.9632 - val_loss: 0.3001 - val_acc: 0.9637\n",
      "Epoch 13/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2932 - acc: 0.9632\n",
      "Epoch 00013: val_loss improved from 0.30007 to 0.28506, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.2931 - acc: 0.9632 - val_loss: 0.2851 - val_acc: 0.9637\n",
      "Epoch 14/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2789 - acc: 0.9632\n",
      "Epoch 00014: val_loss improved from 0.28506 to 0.27155, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.2789 - acc: 0.9632 - val_loss: 0.2715 - val_acc: 0.9637\n",
      "Epoch 15/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2662 - acc: 0.9632\n",
      "Epoch 00015: val_loss improved from 0.27155 to 0.25940, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 968us/step - loss: 0.2661 - acc: 0.9632 - val_loss: 0.2594 - val_acc: 0.9637\n",
      "Epoch 16/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2546 - acc: 0.9633\n",
      "Epoch 00016: val_loss improved from 0.25940 to 0.24844, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.2546 - acc: 0.9632 - val_loss: 0.2484 - val_acc: 0.9637\n",
      "Epoch 17/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9632\n",
      "Epoch 00017: val_loss improved from 0.24844 to 0.23857, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.2442 - acc: 0.9632 - val_loss: 0.2386 - val_acc: 0.9637\n",
      "Epoch 18/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9632\n",
      "Epoch 00018: val_loss improved from 0.23857 to 0.22967, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.2348 - acc: 0.9632 - val_loss: 0.2297 - val_acc: 0.9637\n",
      "Epoch 19/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9633\n",
      "Epoch 00019: val_loss improved from 0.22967 to 0.22161, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 967us/step - loss: 0.2264 - acc: 0.9632 - val_loss: 0.2216 - val_acc: 0.9637\n",
      "Epoch 20/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2187 - acc: 0.9633\n",
      "Epoch 00020: val_loss improved from 0.22161 to 0.21434, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 974us/step - loss: 0.2187 - acc: 0.9632 - val_loss: 0.2143 - val_acc: 0.9637\n",
      "Epoch 21/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2119 - acc: 0.9632\n",
      "Epoch 00021: val_loss improved from 0.21434 to 0.20775, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 979us/step - loss: 0.2118 - acc: 0.9632 - val_loss: 0.2078 - val_acc: 0.9637\n",
      "Epoch 22/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9632\n",
      "Epoch 00022: val_loss improved from 0.20775 to 0.20178, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 972us/step - loss: 0.2056 - acc: 0.9632 - val_loss: 0.2018 - val_acc: 0.9637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1999 - acc: 0.9633\n",
      "Epoch 00023: val_loss improved from 0.20178 to 0.19637, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.1999 - acc: 0.9632 - val_loss: 0.1964 - val_acc: 0.9637\n",
      "Epoch 24/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1948 - acc: 0.9632\n",
      "Epoch 00024: val_loss improved from 0.19637 to 0.19146, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.1948 - acc: 0.9632 - val_loss: 0.1915 - val_acc: 0.9637\n",
      "Epoch 25/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1902 - acc: 0.9632\n",
      "Epoch 00025: val_loss improved from 0.19146 to 0.18700, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 967us/step - loss: 0.1901 - acc: 0.9632 - val_loss: 0.1870 - val_acc: 0.9637\n",
      "Epoch 26/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9633\n",
      "Epoch 00026: val_loss improved from 0.18700 to 0.18296, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 980us/step - loss: 0.1859 - acc: 0.9632 - val_loss: 0.1830 - val_acc: 0.9637\n",
      "Epoch 27/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.9633\n",
      "Epoch 00027: val_loss improved from 0.18296 to 0.17927, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.1820 - acc: 0.9632 - val_loss: 0.1793 - val_acc: 0.9637\n",
      "Epoch 28/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9633\n",
      "Epoch 00028: val_loss improved from 0.17927 to 0.17593, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 991us/step - loss: 0.1785 - acc: 0.9632 - val_loss: 0.1759 - val_acc: 0.9637\n",
      "Epoch 29/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9632\n",
      "Epoch 00029: val_loss improved from 0.17593 to 0.17288, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.1754 - acc: 0.9632 - val_loss: 0.1729 - val_acc: 0.9637\n",
      "Epoch 30/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.9632\n",
      "Epoch 00030: val_loss improved from 0.17288 to 0.17011, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 978us/step - loss: 0.1725 - acc: 0.9632 - val_loss: 0.1701 - val_acc: 0.9637\n",
      "Epoch 31/40\n",
      " 89088/127656 [===================>..........] - ETA: 35s - loss: 0.1699 - acc: 0.9634"
     ]
    }
   ],
   "source": [
    "param_class = grid_search_generator()\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=param_class.params['max_features'][0])\n",
    "tokenizer.fit_on_texts(sentences.values)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences.values)\n",
    "\n",
    "x = sequence.pad_sequences(tokenized_train, maxlen=param_class.params['max_len'][0])\n",
    "y = train[label_cols].values\n",
    "\n",
    "param_class.grid_search_on_model(get_rnn_model, train_model, recorder, RECORD, x, y, tokenizer)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_trainable': True, 'patience': 5, 'embed_type': 'glove', 'batch_size': 1024, 'dense_units': 50, 'epochs': 40, 'activation': 'relu', 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'max_len': 406, 'loss': 'binary_crossentropy', 'max_features': 200000, 'lstm_layer_size': 1, 'lstm_activation': 'tanh', 'dropout': 0.2, 'label_len': 6, 'lstm_units': 5, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'batch_normalization': True, 'dense_layer_size': 3, 'embed_size': 50}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9321\n",
      "Epoch 00001: val_loss improved from inf to 0.10372, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 142s 1ms/step - loss: 0.2300 - acc: 0.9322 - val_loss: 0.1037 - val_acc: 0.9627\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9673\n",
      "Epoch 00002: val_loss improved from 0.10372 to 0.06889, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 999us/step - loss: 0.0897 - acc: 0.9674 - val_loss: 0.0689 - val_acc: 0.9763\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9775\n",
      "Epoch 00003: val_loss improved from 0.06889 to 0.05970, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0670 - acc: 0.9775 - val_loss: 0.0597 - val_acc: 0.9786\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9791\n",
      "Epoch 00004: val_loss improved from 0.05970 to 0.05667, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 997us/step - loss: 0.0607 - acc: 0.9791 - val_loss: 0.0567 - val_acc: 0.9792\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9799\n",
      "Epoch 00005: val_loss improved from 0.05667 to 0.05443, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 133s 1ms/step - loss: 0.0569 - acc: 0.9798 - val_loss: 0.0544 - val_acc: 0.9800\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9805\n",
      "Epoch 00006: val_loss improved from 0.05443 to 0.05333, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 133s 1ms/step - loss: 0.0540 - acc: 0.9805 - val_loss: 0.0533 - val_acc: 0.9804\n",
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9811\n",
      "Epoch 00007: val_loss improved from 0.05333 to 0.05225, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 133s 1ms/step - loss: 0.0517 - acc: 0.9811 - val_loss: 0.0523 - val_acc: 0.9805\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9815\n",
      "Epoch 00008: val_loss improved from 0.05225 to 0.05125, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.0498 - acc: 0.9815 - val_loss: 0.0513 - val_acc: 0.9810\n",
      "Epoch 9/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9821\n",
      "Epoch 00009: val_loss improved from 0.05125 to 0.05081, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.0480 - acc: 0.9821 - val_loss: 0.0508 - val_acc: 0.9812\n",
      "Epoch 10/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9823\n",
      "Epoch 00010: val_loss improved from 0.05081 to 0.05015, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 998us/step - loss: 0.0463 - acc: 0.9823 - val_loss: 0.0502 - val_acc: 0.9813\n",
      "Epoch 11/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9829\n",
      "Epoch 00011: val_loss improved from 0.05015 to 0.05009, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0449 - acc: 0.9829 - val_loss: 0.0501 - val_acc: 0.9816\n",
      "Epoch 12/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9832\n",
      "Epoch 00012: val_loss did not improve\n",
      "127656/127656 [==============================] - 127s 991us/step - loss: 0.0437 - acc: 0.9832 - val_loss: 0.0501 - val_acc: 0.9814\n",
      "Epoch 13/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836\n",
      "Epoch 00013: val_loss improved from 0.05009 to 0.04992, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 994us/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0499 - val_acc: 0.9816\n",
      "Epoch 14/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9840\n",
      "Epoch 00014: val_loss did not improve\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0413 - acc: 0.9841 - val_loss: 0.0502 - val_acc: 0.9815\n",
      "Epoch 15/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9842\n",
      "Epoch 00015: val_loss improved from 0.04992 to 0.04989, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0405 - acc: 0.9842 - val_loss: 0.0499 - val_acc: 0.9817\n",
      "Epoch 16/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9846\n",
      "Epoch 00016: val_loss improved from 0.04989 to 0.04988, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 982us/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0499 - val_acc: 0.9816\n",
      "Epoch 17/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9849\n",
      "Epoch 00017: val_loss did not improve\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0508 - val_acc: 0.9819\n",
      "Epoch 18/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9853\n",
      "Epoch 00018: val_loss did not improve\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0376 - acc: 0.9853 - val_loss: 0.0512 - val_acc: 0.9815\n",
      "Epoch 19/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855\n",
      "Epoch 00019: val_loss did not improve\n",
      "127656/127656 [==============================] - 126s 988us/step - loss: 0.0367 - acc: 0.9855 - val_loss: 0.0514 - val_acc: 0.9813\n",
      "Epoch 20/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9859\n",
      "Epoch 00020: val_loss did not improve\n",
      "127656/127656 [==============================] - 126s 991us/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0521 - val_acc: 0.9818\n",
      "Epoch 21/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9860\n",
      "Epoch 00021: val_loss did not improve\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.0354 - acc: 0.9860 - val_loss: 0.0524 - val_acc: 0.9814\n",
      "127656/127656 [==============================] - 739s 6ms/step\n",
      "31915/31915 [==============================] - 180s 6ms/step\n",
      "train_auc: 0.05235921151932552, val_auc: 0.9776133007952384, val_loss: 0.9882716893359523\n",
      "\n",
      "\n",
      "{'embed_trainable': True, 'patience': 5, 'embed_type': 'glove', 'batch_size': 1024, 'dense_units': 50, 'epochs': 40, 'activation': 'tanh', 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'max_len': 406, 'loss': 'binary_crossentropy', 'max_features': 200000, 'lstm_layer_size': 1, 'lstm_activation': 'tanh', 'dropout': 0.2, 'label_len': 6, 'lstm_units': 5, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'batch_normalization': True, 'dense_layer_size': 3, 'embed_size': 50}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2634 - acc: 0.8951\n",
      "Epoch 00001: val_loss improved from inf to 0.07845, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 141s 1ms/step - loss: 0.2624 - acc: 0.8955 - val_loss: 0.0784 - val_acc: 0.9736\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9764\n",
      "Epoch 00002: val_loss improved from 0.07845 to 0.06584, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 995us/step - loss: 0.0723 - acc: 0.9764 - val_loss: 0.0658 - val_acc: 0.9766\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9783\n",
      "Epoch 00003: val_loss improved from 0.06584 to 0.05806, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 995us/step - loss: 0.0638 - acc: 0.9783 - val_loss: 0.0581 - val_acc: 0.9793\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9794\n",
      "Epoch 00004: val_loss improved from 0.05806 to 0.05532, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0585 - acc: 0.9794 - val_loss: 0.0553 - val_acc: 0.9798\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9803\n",
      "Epoch 00005: val_loss improved from 0.05532 to 0.05308, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 132s 1ms/step - loss: 0.0552 - acc: 0.9803 - val_loss: 0.0531 - val_acc: 0.9805\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9809\n",
      "Epoch 00006: val_loss improved from 0.05308 to 0.05129, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 132s 1ms/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0513 - val_acc: 0.9807\n",
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9814\n",
      "Epoch 00007: val_loss improved from 0.05129 to 0.05015, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0501 - acc: 0.9814 - val_loss: 0.0502 - val_acc: 0.9813\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9819\n",
      "Epoch 00008: val_loss improved from 0.05015 to 0.04942, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0484 - acc: 0.9819 - val_loss: 0.0494 - val_acc: 0.9813\n",
      "Epoch 9/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9824\n",
      "Epoch 00009: val_loss improved from 0.04942 to 0.04843, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 982us/step - loss: 0.0465 - acc: 0.9824 - val_loss: 0.0484 - val_acc: 0.9817\n",
      "Epoch 10/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9827\n",
      "Epoch 00010: val_loss improved from 0.04843 to 0.04796, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0452 - acc: 0.9827 - val_loss: 0.0480 - val_acc: 0.9817\n",
      "Epoch 11/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9831\n",
      "Epoch 00011: val_loss did not improve\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0436 - acc: 0.9831 - val_loss: 0.0485 - val_acc: 0.9814\n",
      "Epoch 12/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9838\n",
      "Epoch 00012: val_loss improved from 0.04796 to 0.04767, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0422 - acc: 0.9838 - val_loss: 0.0477 - val_acc: 0.9819\n",
      "Epoch 13/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9841\n",
      "Epoch 00013: val_loss did not improve\n",
      "127656/127656 [==============================] - 127s 994us/step - loss: 0.0408 - acc: 0.9842 - val_loss: 0.0483 - val_acc: 0.9820\n",
      "Epoch 14/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9845\n",
      "Epoch 00014: val_loss did not improve\n",
      "127656/127656 [==============================] - 125s 977us/step - loss: 0.0399 - acc: 0.9845 - val_loss: 0.0479 - val_acc: 0.9819\n",
      "Epoch 15/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849\n",
      "Epoch 00015: val_loss did not improve\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0482 - val_acc: 0.9818\n",
      "Epoch 16/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9853\n",
      "Epoch 00016: val_loss did not improve\n",
      "127656/127656 [==============================] - 126s 987us/step - loss: 0.0375 - acc: 0.9853 - val_loss: 0.0490 - val_acc: 0.9822\n",
      "Epoch 17/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9856\n",
      "Epoch 00017: val_loss did not improve\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0367 - acc: 0.9856 - val_loss: 0.0486 - val_acc: 0.9817\n",
      "127656/127656 [==============================] - 730s 6ms/step\n",
      "31915/31915 [==============================] - 183s 6ms/step\n",
      "train_auc: 0.04859265080980983, val_auc: 0.9821048140057672, val_loss: 0.9884154320087406\n",
      "\n",
      "\n",
      "{'embed_trainable': True, 'patience': 5, 'embed_type': 'glove', 'batch_size': 1024, 'dense_units': 50, 'epochs': 40, 'activation': 'sigmoid', 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'max_len': 406, 'loss': 'binary_crossentropy', 'max_features': 200000, 'lstm_layer_size': 1, 'lstm_activation': 'tanh', 'dropout': 0.2, 'label_len': 6, 'lstm_units': 5, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'batch_normalization': True, 'dense_layer_size': 3, 'embed_size': 50}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.9185\n",
      "Epoch 00001: val_loss improved from inf to 0.13868, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 143s 1ms/step - loss: 0.2461 - acc: 0.9187 - val_loss: 0.1387 - val_acc: 0.9627\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9635\n",
      "Epoch 00002: val_loss improved from 0.13868 to 0.11363, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.1311 - acc: 0.9635 - val_loss: 0.1136 - val_acc: 0.9627\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9658\n",
      "Epoch 00003: val_loss improved from 0.11363 to 0.08157, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.1008 - acc: 0.9659 - val_loss: 0.0816 - val_acc: 0.9720\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9727\n",
      "Epoch 00004: val_loss improved from 0.08157 to 0.06501, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 962us/step - loss: 0.0739 - acc: 0.9728 - val_loss: 0.0650 - val_acc: 0.9723\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9774\n",
      "Epoch 00005: val_loss improved from 0.06501 to 0.05850, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 125s 976us/step - loss: 0.0620 - acc: 0.9774 - val_loss: 0.0585 - val_acc: 0.9810\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9809\n",
      "Epoch 00006: val_loss improved from 0.05850 to 0.05525, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0561 - acc: 0.9809 - val_loss: 0.0553 - val_acc: 0.9809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9819\n",
      "Epoch 00007: val_loss improved from 0.05525 to 0.05322, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 123s 962us/step - loss: 0.0523 - acc: 0.9819 - val_loss: 0.0532 - val_acc: 0.9811\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9825\n",
      "Epoch 00008: val_loss improved from 0.05322 to 0.05218, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 996us/step - loss: 0.0498 - acc: 0.9825 - val_loss: 0.0522 - val_acc: 0.9814\n",
      "Epoch 9/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9831\n",
      "Epoch 00009: val_loss improved from 0.05218 to 0.05118, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 124s 973us/step - loss: 0.0476 - acc: 0.9831 - val_loss: 0.0512 - val_acc: 0.9813\n",
      "Epoch 10/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9835\n",
      "Epoch 00010: val_loss improved from 0.05118 to 0.05082, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0458 - acc: 0.9835 - val_loss: 0.0508 - val_acc: 0.9812\n",
      "Epoch 11/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9838\n",
      "Epoch 00011: val_loss did not improve\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0444 - acc: 0.9839 - val_loss: 0.0510 - val_acc: 0.9810\n",
      "Epoch 12/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9843\n",
      "Epoch 00012: val_loss did not improve\n",
      "127656/127656 [==============================] - 124s 970us/step - loss: 0.0429 - acc: 0.9843 - val_loss: 0.0509 - val_acc: 0.9814\n",
      "Epoch 13/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9845\n",
      "Epoch 00013: val_loss improved from 0.05082 to 0.05041, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 126s 987us/step - loss: 0.0419 - acc: 0.9845 - val_loss: 0.0504 - val_acc: 0.9811\n",
      "Epoch 14/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9847\n",
      "Epoch 00014: val_loss did not improve\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0409 - acc: 0.9847 - val_loss: 0.0510 - val_acc: 0.9813\n",
      "Epoch 15/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9853\n",
      "Epoch 00015: val_loss did not improve\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0398 - acc: 0.9852 - val_loss: 0.0510 - val_acc: 0.9812\n",
      "Epoch 16/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9855\n",
      "Epoch 00016: val_loss did not improve\n",
      "127656/127656 [==============================] - 127s 994us/step - loss: 0.0389 - acc: 0.9855 - val_loss: 0.0514 - val_acc: 0.9814\n",
      "Epoch 17/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9856\n",
      "Epoch 00017: val_loss did not improve\n",
      "127656/127656 [==============================] - 133s 1ms/step - loss: 0.0381 - acc: 0.9856 - val_loss: 0.0521 - val_acc: 0.9814\n",
      "Epoch 18/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9860\n",
      "Epoch 00018: val_loss did not improve\n",
      "127656/127656 [==============================] - 123s 966us/step - loss: 0.0374 - acc: 0.9860 - val_loss: 0.0517 - val_acc: 0.9811\n",
      "127656/127656 [==============================] - 723s 6ms/step\n",
      "31915/31915 [==============================] - 183s 6ms/step\n",
      "train_auc: 0.05172046475554827, val_auc: 0.9756266056200817, val_loss: 0.9822382777021539\n",
      "\n",
      "\n",
      "{'embed_trainable': True, 'patience': 5, 'embed_type': 'glove', 'batch_size': 1024, 'dense_units': 50, 'epochs': 40, 'activation': 'relu', 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'max_len': 406, 'loss': 'binary_crossentropy', 'max_features': 200000, 'lstm_layer_size': 1, 'lstm_activation': 'sigmoid', 'dropout': 0.2, 'label_len': 6, 'lstm_units': 5, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'batch_normalization': True, 'dense_layer_size': 3, 'embed_size': 50}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9505\n",
      "Epoch 00001: val_loss improved from inf to 0.09345, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 145s 1ms/step - loss: 0.2010 - acc: 0.9506 - val_loss: 0.0934 - val_acc: 0.9627\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9676\n",
      "Epoch 00002: val_loss improved from 0.09345 to 0.07173, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0871 - acc: 0.9676 - val_loss: 0.0717 - val_acc: 0.9751\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9771\n",
      "Epoch 00003: val_loss improved from 0.07173 to 0.06298, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 998us/step - loss: 0.0688 - acc: 0.9771 - val_loss: 0.0630 - val_acc: 0.9787\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9790\n",
      "Epoch 00004: val_loss improved from 0.06298 to 0.05899, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 997us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.0590 - val_acc: 0.9795\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9797\n",
      "Epoch 00005: val_loss improved from 0.05899 to 0.05624, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.0578 - acc: 0.9797 - val_loss: 0.0562 - val_acc: 0.9800\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9806\n",
      "Epoch 00006: val_loss improved from 0.05624 to 0.05388, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0542 - acc: 0.9806 - val_loss: 0.0539 - val_acc: 0.9805\n",
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9812\n",
      "Epoch 00007: val_loss improved from 0.05388 to 0.05279, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0520 - acc: 0.9812 - val_loss: 0.0528 - val_acc: 0.9806\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9814\n",
      "Epoch 00008: val_loss improved from 0.05279 to 0.05192, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 996us/step - loss: 0.0503 - acc: 0.9814 - val_loss: 0.0519 - val_acc: 0.9806\n",
      "Epoch 9/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9821\n",
      "Epoch 00009: val_loss improved from 0.05192 to 0.05130, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0481 - acc: 0.9821 - val_loss: 0.0513 - val_acc: 0.9811\n",
      "Epoch 10/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9823\n",
      "Epoch 00010: val_loss improved from 0.05130 to 0.05094, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 133s 1ms/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0509 - val_acc: 0.9808\n",
      "Epoch 11/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9828\n",
      "Epoch 00011: val_loss improved from 0.05094 to 0.05049, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 134s 1ms/step - loss: 0.0454 - acc: 0.9827 - val_loss: 0.0505 - val_acc: 0.9811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9829\n",
      "Epoch 00012: val_loss did not improve\n",
      "127656/127656 [==============================] - 132s 1ms/step - loss: 0.0441 - acc: 0.9829 - val_loss: 0.0508 - val_acc: 0.9810\n",
      "Epoch 13/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9833\n",
      "Epoch 00013: val_loss improved from 0.05049 to 0.05033, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 127s 996us/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0503 - val_acc: 0.9811\n",
      "Epoch 14/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9838\n",
      "Epoch 00014: val_loss did not improve\n",
      "127656/127656 [==============================] - 126s 991us/step - loss: 0.0415 - acc: 0.9837 - val_loss: 0.0506 - val_acc: 0.9812\n",
      "Epoch 15/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9840\n",
      "Epoch 00015: val_loss did not improve\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0405 - acc: 0.9840 - val_loss: 0.0504 - val_acc: 0.9814\n",
      "Epoch 16/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845\n",
      "Epoch 00016: val_loss did not improve\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0392 - acc: 0.9845 - val_loss: 0.0513 - val_acc: 0.9811\n",
      "Epoch 17/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9847\n",
      "Epoch 00017: val_loss did not improve\n",
      "127656/127656 [==============================] - 129s 1ms/step - loss: 0.0385 - acc: 0.9846 - val_loss: 0.0514 - val_acc: 0.9815\n",
      "Epoch 18/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9850\n",
      "Epoch 00018: val_loss did not improve\n",
      "127656/127656 [==============================] - 127s 998us/step - loss: 0.0374 - acc: 0.9850 - val_loss: 0.0521 - val_acc: 0.9815\n",
      "127656/127656 [==============================] - 734s 6ms/step\n",
      "31915/31915 [==============================] - 183s 6ms/step\n",
      "train_auc: 0.05212143960743139, val_auc: 0.9760183759068148, val_loss: 0.9850226663420835\n",
      "\n",
      "\n",
      "{'embed_trainable': True, 'patience': 5, 'embed_type': 'glove', 'batch_size': 1024, 'dense_units': 50, 'epochs': 40, 'activation': 'tanh', 'model_file': '/home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5', 'max_len': 406, 'loss': 'binary_crossentropy', 'max_features': 200000, 'lstm_layer_size': 1, 'lstm_activation': 'sigmoid', 'dropout': 0.2, 'label_len': 6, 'lstm_units': 5, 'embed_file': '/home/kai/data/resources/glove/glove.6B.50d.txt', 'batch_normalization': True, 'dense_layer_size': 3, 'embed_size': 50}\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.2619 - acc: 0.8982\n",
      "Epoch 00001: val_loss improved from inf to 0.07468, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 142s 1ms/step - loss: 0.2609 - acc: 0.8986 - val_loss: 0.0747 - val_acc: 0.9759\n",
      "Epoch 2/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9764\n",
      "Epoch 00002: val_loss improved from 0.07468 to 0.06464, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 130s 1ms/step - loss: 0.0723 - acc: 0.9764 - val_loss: 0.0646 - val_acc: 0.9783\n",
      "Epoch 3/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9780\n",
      "Epoch 00003: val_loss improved from 0.06464 to 0.06047, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0652 - acc: 0.9780 - val_loss: 0.0605 - val_acc: 0.9791\n",
      "Epoch 4/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9789\n",
      "Epoch 00004: val_loss improved from 0.06047 to 0.05771, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0607 - acc: 0.9789 - val_loss: 0.0577 - val_acc: 0.9796\n",
      "Epoch 5/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9796\n",
      "Epoch 00005: val_loss improved from 0.05771 to 0.05534, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 128s 1ms/step - loss: 0.0572 - acc: 0.9796 - val_loss: 0.0553 - val_acc: 0.9799\n",
      "Epoch 6/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9802\n",
      "Epoch 00006: val_loss improved from 0.05534 to 0.05282, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 132s 1ms/step - loss: 0.0546 - acc: 0.9802 - val_loss: 0.0528 - val_acc: 0.9807\n",
      "Epoch 7/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9808\n",
      "Epoch 00007: val_loss improved from 0.05282 to 0.05212, saving model to /home/kai/data/kaggle/toxic/hz/model/lstm_best.hdf5\n",
      "127656/127656 [==============================] - 132s 1ms/step - loss: 0.0515 - acc: 0.9808 - val_loss: 0.0521 - val_acc: 0.9810\n",
      "Epoch 8/40\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9815"
     ]
    }
   ],
   "source": [
    "#continue case\n",
    "\n",
    "param_class = grid_search_generator('a.json')\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=param_class.params['max_features'][0])\n",
    "tokenizer.fit_on_texts(sentences.values)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences.values)\n",
    "\n",
    "x = sequence.pad_sequences(tokenized_train, maxlen=param_class.params['max_len'][0])\n",
    "y = train[label_cols].values\n",
    "\n",
    "param_class.grid_search_on_model(get_rnn_model, train_model, recorder, RECORD, x, y, tokenizer)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, shuffle=True)\n",
    "np.isnan(x_train).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(y).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-977511115cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    269\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "y1 = np.array([0,0,0,1])\n",
    "y2 = np.array([0,0,0,0])\n",
    "roc_auc_score(y2, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
