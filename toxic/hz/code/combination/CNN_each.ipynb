{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 27)\n",
      "(153164, 21)\n",
      "72.2062896702\n",
      "114.017305915\n"
     ]
    }
   ],
   "source": [
    "PATH = '../../data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'cleaned_train.csv')\n",
    "test = pd.read_csv(PATH + 'cleaned_test.csv')\n",
    "\n",
    "train_sentence = train['comment_text_cleaned']\n",
    "test_sentence = test['comment_text_cleaned']\n",
    "\n",
    "text_length = pd.concat([train_sentence.apply(lambda x: len(x.split())),\\\n",
    "                         test_sentence.apply(lambda x: len(x.split()))])\n",
    "\n",
    "mean_length = text_length.mean()\n",
    "std_length = text_length.std()\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(mean_length)\n",
    "print(std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "MAX_FEATURES = 100000 # max num of words\n",
    "MAX_LEN = np.round(mean_length + 3*std_length).astype(int) # max sequence length\n",
    "EMBED_SIZE = 50 # embedding size\n",
    "FILTERS = 128 # cnn config\n",
    "KERNEL_SIZE = 7 #cnn config\n",
    "DENSE_UNITS = 50\n",
    "DROPOUT = 0.3 # dropout rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "EMBEDDING_FILE = 'glove.6B.50d.txt'\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[634, 79, 2, 33, 46, 198, 26, 708, 3771, 8376, 805, 1, 140, 45, 1, 12, 226, 50, 5098, 17, 64, 2056, 159, 5, 492, 31, 127, 1177, 8377, 2249, 7, 51, 14, 12, 98, 2, 280, 28, 2, 43, 22, 150, 5, 1, 2550, 90]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(pd.concat([train_sentence, test_sentence]).values)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentence.values)\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_sentence.values)\n",
    "\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=MAX_LEN)\n",
    "# y = train[label_cols].values\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=MAX_LEN)\n",
    "\n",
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer):\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_features:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_cnn_model(embedding_file, embed_size, max_features, tokenizer,\\\n",
    "                  kernel_size, filters, dropout, dense_units, max_len, label_cols, output_size):\n",
    "    embedding_matrix = get_embedding_matrix(embedding_file, embed_size, max_features, tokenizer)\n",
    "    input = Input(shape=(max_len, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(input)\n",
    "    x = Conv1D(FILTERS, kernel_size, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Dense(output_size, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, file_path, batch_size, epochs, X_train, y):\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "    model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "    return model\n",
    "\n",
    "def predict(model, file_path, X_test):\n",
    "    model.load_weights(file_path)\n",
    "    return model.predict(X_test, verbose=1)\n",
    "    \n",
    "def save(model_name, y_test, label_cols, path, is_train=False):\n",
    "    if is_train:\n",
    "        submission = pd.read_csv(path + 'sample_train.csv')\n",
    "        file_name = 'train_' + model_name\n",
    "    else:\n",
    "        submission = pd.read_csv(path + 'sample_submission.csv')\n",
    "        file_name = model_name\n",
    "    submission[label_cols] = y_test\n",
    "    submission.to_csv(path + model_name + '/' + file_name + '.csv', index=False)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9534Epoch 00001: val_loss improved from inf to 0.10374, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1631s 11ms/step - loss: 0.1491 - acc: 0.9534 - val_loss: 0.1037 - val_acc: 0.9625\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0905 - acc: 0.9668Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 2097s 15ms/step - loss: 0.0905 - acc: 0.9668 - val_loss: 0.1043 - val_acc: 0.9619\n",
      "153164/153164 [==============================] - 5977s 39ms/step\n",
      "159571/159571 [==============================] - 416s 3ms/step\n",
      "accuracy: 0.9863304827520918\n",
      "\n",
      "\n",
      "\n",
      "fit severe_toxic\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9899Epoch 00001: val_loss improved from inf to 0.02582, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1561s 11ms/step - loss: 0.0464 - acc: 0.9899 - val_loss: 0.0258 - val_acc: 0.9906\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9899Epoch 00002: val_loss improved from 0.02582 to 0.02517, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1606s 11ms/step - loss: 0.0240 - acc: 0.9899 - val_loss: 0.0252 - val_acc: 0.9906\n",
      "153164/153164 [==============================] - 392s 3ms/step\n",
      "159571/159571 [==============================] - 409s 3ms/step\n",
      "accuracy: 0.9916186407744487\n",
      "\n",
      "\n",
      "\n",
      "fit obscene\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9750Epoch 00001: val_loss improved from inf to 0.07617, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1570s 11ms/step - loss: 0.0969 - acc: 0.9750 - val_loss: 0.0762 - val_acc: 0.9741\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9821Epoch 00002: val_loss improved from 0.07617 to 0.06013, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1548s 11ms/step - loss: 0.0503 - acc: 0.9821 - val_loss: 0.0601 - val_acc: 0.9794\n",
      "153164/153164 [==============================] - 397s 3ms/step\n",
      "159571/159571 [==============================] - 428s 3ms/step\n",
      "accuracy: 0.9950292348209943\n",
      "\n",
      "\n",
      "\n",
      "fit threat\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9969Epoch 00001: val_loss improved from inf to 0.01235, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1962s 14ms/step - loss: 0.0322 - acc: 0.9969 - val_loss: 0.0124 - val_acc: 0.9969\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9970Epoch 00002: val_loss improved from 0.01235 to 0.01107, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1524s 11ms/step - loss: 0.0096 - acc: 0.9970 - val_loss: 0.0111 - val_acc: 0.9969\n",
      "153164/153164 [==============================] - 376s 2ms/step\n",
      "159571/159571 [==============================] - 472s 3ms/step\n",
      "accuracy: 0.996130148290675\n",
      "\n",
      "\n",
      "\n",
      "fit insult\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.1017 - acc: 0.9685Epoch 00001: val_loss improved from inf to 0.07685, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 2075s 14ms/step - loss: 0.1017 - acc: 0.9685 - val_loss: 0.0768 - val_acc: 0.9702\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9753Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 2127s 15ms/step - loss: 0.0632 - acc: 0.9753 - val_loss: 0.0788 - val_acc: 0.9727\n",
      "153164/153164 [==============================] - 541s 4ms/step\n",
      "159571/159571 [==============================] - 570s 4ms/step\n",
      "accuracy: 0.9876764395326038\n",
      "\n",
      "\n",
      "\n",
      "fit identity_hate\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9911Epoch 00001: val_loss improved from inf to 0.02622, saving model to ../../model/cnn_best.hdf5\n",
      "143613/143613 [==============================] - 1849s 13ms/step - loss: 0.0523 - acc: 0.9911 - val_loss: 0.0262 - val_acc: 0.9910\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9912Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 2210s 15ms/step - loss: 0.0233 - acc: 0.9912 - val_loss: 0.0317 - val_acc: 0.9910\n",
      "153164/153164 [==============================] - 528s 3ms/step\n",
      "159571/159571 [==============================] - 462s 3ms/step\n",
      "accuracy: 0.988006469890659\n",
      "\n",
      "\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../model/cnn_best.hdf5'\n",
    "sample_submission_file_path = PATH + 'sample_submission.csv'\n",
    "\n",
    "preds = np.zeros((test.shape[0], len(label_cols)))\n",
    "preds_train = np.zeros((train.shape[0], len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    y = train[j].to_frame()\n",
    "    y['2'] = 1 - y\n",
    "    y = y.values\n",
    "    model = get_cnn_model(PATH + EMBEDDING_FILE, EMBED_SIZE, MAX_FEATURES, tokenizer,\\\n",
    "                          KERNEL_SIZE, FILTERS, DROPOUT, DENSE_UNITS, MAX_LEN, label_cols, 2)\n",
    "    model = train_model(model, file_path, BATCH_SIZE, EPOCHS, X_train, y)\n",
    "    preds[:, i] = predict(model, file_path, X_test)[:, 0]\n",
    "    preds_train[:, i] = predict(model, file_path, X_train)[:, 0]\n",
    "    print('accuracy: {}'.format(roc_auc_score(train[j], preds_train[:, i])))\n",
    "    print('\\n\\n')\n",
    "\n",
    "save('cnn', preds, label_cols, PATH)\n",
    "save('cnn', preds_train, label_cols, PATH, True)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
